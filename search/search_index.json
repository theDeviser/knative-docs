{"config":{"lang":["en"],"separator":"[\\/\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about-analytics-cookies/","title":"Learn about Google Analytics cookies","text":"<p>By using this website, you are confirming that you accept our use of cookies for the purposes of collecting anonymous usage data to improve the user experience and content on the website.</p> <p>Continue to sections below for details about knative.dev, or use the following resources to learn about cookies in general:</p> <ul> <li> <p>Learn about basic site analytics usage at: https://www.cookiechoices.org/</p> </li> <li> <p>You can also watch a video about how Google uses cookies.</p> </li> </ul>"},{"location":"about-analytics-cookies/#what-are-cookies","title":"What are cookies?","text":"<p>Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity.</p> <p>For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page.</p> <p>Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice).</p> <p>However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior.</p>"},{"location":"about-analytics-cookies/#what-cookies-are-used-on-knativedev","title":"What cookies are used on knative.dev","text":"<p>We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used.</p> <p>These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy.</p> <p>If you prefer to view the Knative docs from within the knative/docs  GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement.</p>"},{"location":"about-analytics-cookies/#options-for-opting-out","title":"Options for opting out","text":"<p>Use the following options to prevent your data from being shared with websites.</p>"},{"location":"about-analytics-cookies/#opt-out-browser-add-on","title":"Opt-out Browser Add-on","text":"<p>You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on.</p>"},{"location":"about-analytics-cookies/#disabling-cookies","title":"Disabling cookies","text":"<p>You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at:</p> <p>Apple Safari</p> <p>Google Chrome</p> <p>Microsoft Internet Explorer</p> <p>Mozilla Firefox</p>"},{"location":"about/testimonials/","title":"Testimonials","text":"Enterprise-grade Serverless on your own terms. Understanding Knative \"If Kubernetes is an electrical grid, then Knative is its light switch.\"        \u2014Kelsey Hightower, Google Cloud Platform      <p>     Knative is an automated system that helps development teams manage and maintain processes in Kubernetes. Its purpose is to simplify, automate, and monitor deployments of Kubernetes so teams spend less time on maintenance and more time on app development and projects. Knative takes over repetitive and time-intensive tasks while removing obstacles and delays.   </p> <p>     Knative does this through two features. The first is Knative Eventing. Eventing allows developers to set up detailed actions triggered by specific events within a broader environment. The second is Knative Serving, which automatically manages the creation and scaling of services through Kubernetes, including scaling down to zero. Each of these features aims to free up resources that teams would otherwise spend managing systems. They also save businesses money by reacting to conditions in real time. Meaning, companies only pay for the resources they are using, not the ones they might use.   </p> Scale to Zero is a feature of Knative Serving that automatically turns off services running in containers when there is no demand for them. Instead of running programs on standby, they can be turned off and turned back on when needed again. Scale to zero reduces costs over time and helps manage technical resources.      <p>     The core idea behind Knative is to allow teams to harness the power of serverless application deployment. Serverless refers to managing cloud-based servers and virtual machines, often hosted on platforms like AWS, Google Cloud, Microsoft Azure, and more. Serverless is a great option for companies looking to move away from the costly endeavor of managing their own servers and infrastructure.   </p> \"I often think of Knative as part of 'Serverless 2.0.' It combines the good things about serverless with a loosening of constraints around execution time and availability of resources.\"       -Michael Behrendt, Distinguished Engineer and Chief Architect of Serverless and Cloud Functions for IBM.       IBM is a committed sponsor of Knative      Knative in the broader ecosystem <p>     To understand Knative more fully, it is important to know that it exists in a larger ecosystem of services that work together. For example, Knative acts as a framework on top of Kubernetes that helps build a serverless platform. Kubernetes itself is a system that orchestrates the creation and running of containers used in app deployment, scaling, and more. Those containers can run anything, from simple tools written in python to complex Al systems. Containers were developed to help tackle the problem of complexity. As development teams build software products, they create massive codebases. Left unorganized, those codebases can become gigantic and confusing-even for those who make them. Containers solve this problem by breaking codebases into small, self-contained processes that can interact to do work. They also help developers manage complex webs of dependencies like APIs and databases. These containers are easier to maintain for teams looking to work fast while maintaining best practices.   </p> Knative's value in DevOps <p>     DevOps promises effective application development processes with faster deployments and fewer bugs. While Kubernetes helps facilitate this, it can produce significant complexity. Achieving value at scale with Kubernetes traditionally involves teams developing specialized knowledge. Knative cuts down on that by providing a serverless experience that removes the need for all development team members to know or understand the ins and outs of Kubernetes.   </p> \"What we are doing with Knative is to provide a developer experience that makes it easier to focus on code. Cloud developers focus on the business problems they are solving without having to coordinate or wait for approvals from platform teams to scale their apps. Knative is a framework that helps automate platform capabilities so your apps can scale as if they were running on Serverless compute.\"       -Aparna Sinha, Director of Product Management, Google      Tangible benefits of Knative for teams <p>     It has always been true that organizations need to develop and innovate faster than their competition while deploying products with fewer flaws. However, being bogged down by configuring networks and operating systems harms developer productivity and morale. Developers want to create things, and Knative helps them do that.   </p> \"The amount of internal work needed to use Knative is minuscule.\"       -Tilen Kav\u010di\u010d, Backend Developer for Outfit7, which uses Knative for key backend system      The advantage of Open Source <p>     Open source has been a powerful resource for creating business solutions for decades. Kubernetes and Knative are now paving the way for that relationship to become stronger. Each project has significant support from some of the biggest names in tech including IBM, Google, Red Hat, and VMware. The Kubernetes and Knative ecosystem consists of widely adopted projects that are proven across many installations for a multitude of uses. The open-source foundation of Knative means that anyone using the platform can participate in the community to get help, solve problems, and influence the direction of deployment for future versions.   </p> Find out more &gt; Case Studies: Read about organizations using Knative, from platform developers to proven companies to innovative startups &gt; Check the getting started guide to get up and running with Knative in an afternoon &gt; Join the Knative Slack to talk to the community"},{"location":"about/case-studies/","title":"Knative Case Studies","text":"AI Startup deepc Connects Researchers to Radiologists with Knative Eventing Game maker Outfit7 automates high performance ad bidding with Knative Serving PNC Bank automated software supply chain compliance Relay by Puppet Brings Workflows to Everything using Knative SVA uses Knative to kickstart cloud native adoption and patterns"},{"location":"about/case-studies/deepc/","title":"deepc","text":"deepc Case Study \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d <p>-- Andrew Webber, Senior Software Engineer for deepc</p> AI Startup deepc Connects Researchers to Radiologists with Knative Eventing  deepc is a startup at the cutting edge, bringing advanced data techniques and artificial intelligence to healthcare. The German company helps radiologists  access better diagnostics and resources through cloud-based technology. Their goal is to elevate treatment  quality across the board while creating more efficiency and improvement in medical settings. This revolution in technology comes with hefty challenges. Care systems are highly regulated, making patient privacy and safety a top priority for deepc. Doctors and medical staff also demand that new technologies be reliable and stable when lives are on the line.    Rising to the challenge  deepc has risen to meet these challenges through carefully architected solutions, using tools like Knative Eventing to their full potential. Their product helps radiologists access a wide selection of artificial intelligence (AI) programs that analyze imaging, like x-rays and MRIs. The data generated from these AI programs help radiologists make more accurate diagnoses.  The deepc workflow  The radiologist uploads the image into deepcOS, initially to a virtual machine within the hospital IT infrastructure containing the deepcOS client application. After making a series of selections, deepcOS identifies the proper AI to use. It then removes the patient information from the scans before encrypting the data. deepcOS sends that data to the cloud-based deepc AI platform. This platform does the heavy lifting in providing the computing power the AI algorithms need to do their work. After the program finishes, the results are sent back. Finally, the data is reassociated with the patient, and the radiologist can take action based on the results. Critically, patient information always remains on-premises in the hospital and is not transmitted to deepc servers.  A Knative-powered process  The deepcOS workflow builds on a sophisticated implementation of Knative Eventing. Knative Eventing allows teams to deploy event-driven architecture with serverless applications quickly. In conjunction with Knative Serving, deepc resources and programs scale up and down automatically based on specific event triggers laid out by the developers. Knative takes care of the management, so the process does not need to wait for a person to take action. When data is sent to deepc's cloud-based platform, Knative emits an event that triggers a specific AI. After one is selected, Knative starts a container environment for the program to run. Some AI programs may only need one container. Others may require multiple, running parallel or in sequence. In the case of multiple containers, the deepc team created workflows using Knative Eventing to coordinate the complex processes. After the process finishes and provides the output for the radiologist, Knative triggers stop the active containers.  \"Knative gives us a foundation of consistency,\" said Andrew Webber, Senior Software Engineer. Bridging between legacy and advanced systems  The platform makes available AIs developed by leading global companies and researchers. Knative has also allowed integration with the work of independent researchers through an SDK implementation for radiologists. They don\u2019t need to be Kubernetes experts or take days to bring their work to patients through deepc\u2019s platform.  \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d said Webber.  Some implementations are more complex. They use legacy technology that does not fit into a standard container or they have unique architectures that require OS-level configuration. deepc has built out APIs and virtual machines that connect those technologies to their own cloud-based platform and still integrate with the Knative Eventing workflow. This approach ensures those programs work flawlessly within the system.  The case for business  The choice to develop their platform around Knative has had several business benefits for the startup. One of the most complicated aspects of growing a company is scaling. Many technology companies find their developers start scrambling when more customers are onboarded, uncovering new bugs and other issues. However, because of the nature of Knative, this is less of a problem for deepc. Knative's combination of automation and serverless methods means as more customers are onboarded, deepc will not need to build out more resources - it will all happen automatically. Knative has also allowed the startup to add real value to customers using their technology. For example, because many applications used by radiologists are built by different companies, medical professionals have had to interact with disparate systems and procedures. deepc provides access to the work of many researchers on one platform, ending complicated processes for professionals on the ground. Healthcare systems get simple, unified billing. Knative has helped deepc create a robust case for customers to use their platform.  Looking forward  deepc has already done amazing things as a company, with many more features planned. The company is a model for how Knative can help any organization build an impressive technical architecture capable of addressing some of today's most complex problems. Using features provided by Knative has enabled them to pioneer what is possible.  Find out more <ul> <li>Getting started with Knative</li> <li>Knative Serving</li> <li>Knative Eventing</li> </ul>"},{"location":"about/case-studies/outfit7/","title":"Outfit7","text":"Outfit7 Case Study \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" <p>-- Tilen Kav\u010di\u010d, Software Engineer for Outfit7</p> Game maker Outfit7 automates high performance ad bidding with Knative Serving   Since its founding in 2009, the mobile gaming company Outfit7 has seen astronomical growth\u2014garnering over 17 billion downloads and over 85 billion video views last year.  Outfit7 was in the Top 5 game publishers list on iOS and Google Play worldwide by the number of game downloads for 6 consecutive years (2015 - 2020). With their latest launch, My Talking Angela 2, they were number 1 by global games downloads in July, August, and September (more than 120 million downloads).  The success of the well-known game developer behind massive hits like the Talking Tom &amp; Friends franchise and stand-alone titles like Mythic Legends created large-scale challenges.  With up to 470 million monthly active users, 20 thousand server requests per second, and terabytes of data generated daily, they needed a stable, high performance solution. They turned to a Knative and Kubernetes solution to optimize real-time bidding ad sales in a way that can automatically scale up and down as needed.  They were able to develop a system so easy to maintain that it freed up two software engineers who are now able to work on more important tasks, like optimizing backend costs and adding new game features.  High performance in-app bidding  Ad sales are an important revenue stream of Outfit7. The team needed to walk a careful balance: sell the space for the highest bid, use technical resources efficiently, and make sure ads are served to players quickly. To achieve this they decided to adopt an in-app bidding approach.  The Outfit7 user base generates around 8,000 ad-related requests per second. With so many users spread worldwide, the amount of these requests can drop or surge depending on all sorts of factors. Not just predictable things like the time of day, but current events can suddenly create traffic. The pandemic saw their usage soar, for instance.  To manage the process in-house, the team needed to be able to test and deploy very efficiently. \"There were two specific use cases we wanted to cover,\" explained Luka Draksler, backend engineer in the ad technology department at Outfit7. \"One was to have the ability to do zero downtime canary deployments using an automatic gradual rollout process. This works in a way that the new version of the software is deployed using a continuous deployment pipeline with a small amount of traffic first. If everything checks out, all production traffic is migrated to the new version. In the worst-case scenario (if requests start failing) traffic can be quickly migrated to the old version. The second use-case was the ability to have developers deploy versions to specific groups of users for instances of A/B testing and other use cases.\"  The team decided to adopt Knative Serving as the backbone of their solution. Knative allowed Outfit7 to streamline deployments and cut down on development time. After being surprised at how easily they generated an internal proof of concept, the team saw that it could craft custom solutions tuned for their internal workflows\u2014without consuming valuable developer time. In addition, they could quickly configure A/B testing and deploy multiple versions of code simultaneously.  Serverless solution  Knative Serving gave Outfit7 access to a robust set of tools and features that allows their team to automate and monitor the deployment of applications to handle ad requests. When more requests are coming in, their system automatically spins up more containers that house the workers and tools. When these requests drop, unneeded containers shut down. Outfit7 only pays for the resources they require for the current load.  Knative works as a layer installed on top of Kubernetes. It brings the power of serverless workloads to the scalable capabilities of Kubernetes. Teams quickly spin up container-based applications without needing to consider the details of Kubernetes.  Knative also simplifies project deployments to Kubernetes. Mitja Bezen\u0161ek, the Lead Developer on Outfit7's backend team, estimated that the traditional development that Knative replaced would have required three full-time engineers to maintain. Their new platform operates with minimal work and allows the developers to deploy updates at will.  The open source community  Outfit7's team was blown away by the supportive and helpful community around Knative. After discovering a problem with network scaling, the team was surprised by how easy it was to find answers and solutions.  \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Sharing their story  The great experience with Knative encouraged their team to share their experience with fellow companies and engineers at a local meetup. The presentation which included several live demos was a success, helping spawn another meet-up focused on the technology. \"Tilen showed them the demo and what it's all about,\" said Bezen\u0161ek. \"I hope we got them engaged going forward.\"  Looking forward  Outfit7 shows no signs of slowing down. \u201cAs we want to support our vision in expanding our games portfolio, we are always looking for new strategic partners who can accompany us on this path,\u201d added Helder Lopes, Head of R&amp;D in Cyprus headquarters. The company plans to incorporate and adopt Knative into other back-end systems \u2013 taking advantage of the easier workflows that Knative offers.  Find out more <ul> <li>Getting started with Knative</li> <li>Knative Serving</li> <li>Knative Eventing</li> </ul>"},{"location":"about/case-studies/pnc/","title":"PNC Bank","text":"PNC Bank Case Study \u201cOur real success was our ability to say if your code change is fully compliant and does not affect complex authentication or funds transfer, you can go from new code to production in just the time it takes to run your pipelines. There is no longer 120 hours over 37 days of non-code compliance work standing in the way of production. Thousands of PNC software developers have seen their deployment window shrink to near real time. Our teams achieve true continuous delivery. That is our big win.\u201d  <p>-- Director, PNC Bank</p> PNC Bank automated software supply chain compliance  As one of the largest banks within the United States with $367 billion of assets under administration, PNC has a massive IT footprint and a dev team that needs to not only deliver innovative code but also consistently meet regulatory compliance requirements. PNC sought to develop a way to ensure new code would meet security standards and audit compliance requirements automatically\u2014replacing the cumbersome 30-day manual process they had in place.  Please read the full case study at CNCF site <li>How PNC Bank automated software supply chain compliance</li> Find out more <ul> <li>Getting started with Knative</li> <li>Knative Serving</li> <li>Knative Eventing</li> </ul>"},{"location":"about/case-studies/puppet/","title":"Puppet","text":"Puppet Case Study \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional\" <p>-- Noah Fontes, Senior Principal Software Engineer for Puppet</p> Relay by Puppet Brings Workflows to Everything using Knative Puppet is a software company specializing in infrastructure automation. The company was founded in 2009 to solve some of the most complex problems for those who work in operations. Around 2019, the team noticed that cloud operations teams weren\u2019t able to effectively manage modern cloud-native applications because they were relying on manual workflow processes. The group saw an opportunity here to build out a platform to connect events triggered by modern architectures to ensure cloud environments remained secure, compliant, and cost-effective.  This is the story of how Relay, a cloud-native workflow automation platform, was created, and how Knative and Kubernetes modernize and super-charge business process automation.   The glue for DevOps  When Puppet first began exploring the power and flexibility of Knative to trigger their Tekton-based workflow execution engine, they weren't quite sure where their journey was going to take them. Knative offered an attractive feature set, so they began building and experimenting. They wanted to build an event-driven DevOps tool; they weren't interested in building just another continuous integration product. In addition, as they continued to explore, they realized that they wanted something flexible and not tied to just one vertical. Whatever they were building, it was not going to focus on just one market.  As their target came into focus, they realized that the serverless applications and functions enabled by Knative Serving would be perfect for a cloud-based business process automation service. Out of this realization, they built Relay, a cloud workflow automation product that helps Cloud Ops teams solve the integration and eventing issues that arise as organizations adopt multiple clouds and SaaS products alongside legacy solutions.   Containers and webhooks  Containers and webhooks are key elements in the Relay architecture. Containers allow Puppet to offer a cloud-based solution where businesses can configure and deploy workflows as discrete business units. Since the containers provide self-contained environments, even legacy services and packages can be included. This proved to be an essential feature for business customers. Anything that can be contained in a Docker image, for example, can be part of a Relay workflow.  \"We focused on containers because they provide isolation,\" explains Noah Fontes, Senior Principal Software Engineer for Puppet, \"Containers provide discrete units of execution, where users can decrease the maintenance burden of complex systems.\"   Allowing fully-configurable webhooks gives users the flexibility needed to incorporate business processes of all kinds. With webhooks, Relay can interact with nearly any web-based API to trigger rich, fully featured workflows across third party SaaS products, cloud services, web applications, and even system utilities.  Knative Serving provides important infrastructure for Relay. It allows webhooks and services to scale automatically, even down to zero. This allows Relay to support pretty much any integration, including those used by only a small number of users. With autoscaling, those services don't consume resources while they are not being used.   What is Knative Serving?  Modern cloud-based applications deal with massive scaling challenges through several approaches. At the core of most of these is the use of containers: discrete computing units that run single applications, single services, or even just single functions. This approach is incredibly powerful, allowing services to scale the number of resources they consume as demand dictates.  However, while all of this sounds amazing, it can be difficult to manage and configure. One of the most successful solutions for delivering this advanced architecture is Knative Serving. This framework builds on top of Kubernetes to support the deployment and management of serverless applications, services, and functions. In particular, Knative Services focuses on being easy to configure, deploy, and manage.   Workflow integrations  The open architecture allows Relay to integrate dozens of different services and platforms into workflows. A look at the Relay integrations GitHub page provides a list of these integrations and demonstrates their commitment to the open source community.  \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional,\" says Fontes.   Results: automated infrastructure management  While Relay's infrastructure runs on the Google Cloud Platform, it is a library of workflows, integrations, steps, and triggers that includes services across all major cloud service providers. Relay customers can integrate across Microsoft Azure, AWS, and Oracle Cloud Infrastructure among others. By combining these integrations with SaaS offerings, it truly is becoming the Zapier of infrastructure management.  \u201cOur customers have diverse needs for managing their workloads that are often best implemented as web APIs. Our product provides a serverless microservice environment powered by Knative that allows them to build this complex tooling without the management and maintenance overhead of traditional deployment architectures. We pass the cost savings on to them, and everyone is happier,\" said Fontes.  Building and deploying Relay would not have been possible without the existing infrastructure offered by systems such as Knative and Tekton. Remarkably, Fontes' team never grew above eight engineers. Once they solidified their plan for Relay, they were able to bring it to production in just three months, says Fontes.  \"Thanks to Knative, getting Relay out the door was easier than we thought it would be.\" said Noah Fontes, Senior Principal Software Engineer.  Knative aims to make scalable, secure, stateless architectures available quickly by abstracting away the complex details of a Kubernetes installation and enabling developers to focus on what matters.  Find out more <ul> <li>Getting started with Knative</li> <li>Knative Serving</li> <li>Knative Eventing</li> <li>A Basic Introduction to Webhooks</li> </ul>"},{"location":"about/case-studies/sva/","title":"SVA","text":"SVA System Vertrieb Alexander GmbH Case Study \u201cSVA built an HTTP-based, event-driven platform that was highly available and adhered to cloud native best practices. The solution was opt-in and provided a quality developer experience so that the developers would want to use it.\u201d  SVA uses Knative to kickstart cloud native adoption and patterns  SVA System Vertrieb Alexander GmbH is a leading German company that specializes in Professional Services, server migrations, and managed services. Public sectors pose unique challenges due to their complex structure and regulated requirements. For example, one particular customer, a German government organisation, is divided into departments, groups, units, and application teams, each with specific functional responsibilities. Some applications have to adhere to specific laws, which limits innovation and makes the lifecycle of applications stale. With the gradual introduction of microservices and service-oriented architecture, the number of applications connecting to each other grew into a complicated mesh of applications which was quickly becoming unmanageable.  Please read the full case study at CNCF site <li>How SVA used Knative to kickstart cloud native adoption and patterns</li> Find out more <ul> <li>Getting started with Knative</li> <li>Knative Serving</li> <li>Knative Eventing</li> </ul>"},{"location":"client/","title":"CLI tools","text":"<p>The following CLI tools are supported for use with Knative.</p>"},{"location":"client/#kubectl","title":"kubectl","text":"<p>You can use <code>kubectl</code> to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML.</p> <p>See Install and Set Up <code>kubectl</code>.</p>"},{"location":"client/#kn","title":"kn","text":"<p><code>kn</code> provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. <code>kn</code> also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting.</p> <p>Note</p> <p><code>kn</code> cannot be used to install Knative components such as Serving or Eventing.</p>"},{"location":"client/#additional-resources","title":"Additional resources","text":"<ul> <li>See Installing <code>kn</code>.</li> <li>See the <code>kn</code> documentation in Github.</li> </ul>"},{"location":"client/#func","title":"func","text":"<p>The <code>func</code> CLI enables you to create, build, and deploy Knative Functions without the need to create or modify YAML files directly.</p>"},{"location":"client/#additional-resources_1","title":"Additional resources","text":"<ul> <li>See Installing Knative Functions.</li> <li>See the <code>func</code> documentation in Github.</li> </ul>"},{"location":"client/#connecting-cli-tools-to-your-cluster","title":"Connecting CLI tools to your cluster","text":"<p>After you have installed <code>kubectl</code> or <code>kn</code>, these tools will search for the <code>kubeconfig</code> file of your cluster in the default location of <code>$HOME/.kube/config</code>, and will use this file to connect to the cluster. A <code>kubeconfig</code> file is usually automatically created when you create a Kubernetes cluster.</p> <p>You can also set the environment variable <code>$KUBECONFIG</code>, and point it to the kubeconfig file.</p> <p>Using the <code>kn</code> CLI, you can specify the following options to connect to the cluster:</p> <ul> <li><code>--kubeconfig</code>: use this option to point to the <code>kubeconfig</code> file. This is equivalent to setting the <code>$KUBECONFIG</code> environment variable.</li> <li><code>--context</code>: use this option to specify the name of a context from the existing <code>kubeconfig</code> file. Use one of the contexts from the output of <code>kubectl config get-contexts</code>.</li> </ul> <p>You can also specify a config file in the following ways:</p> <ul> <li> <p>Setting the environment variable <code>$KUBECONFIG</code>, and point it to the kubeconfig file.</p> </li> <li> <p>Using the <code>kn</code> CLI <code>--config</code> option, for example, <code>kn service list --config path/to/config.yaml</code>. The default config is at <code>~/.config/kn/config.yaml</code>.</p> </li> </ul> <p>For more information about <code>kubeconfig</code> files, see Organizing Cluster Access Using kubeconfig Files.</p>"},{"location":"client/#using-kubeconfig-files-with-your-platform","title":"Using kubeconfig files with your platform","text":"<p>Instructions for using <code>kubeconfig</code> files are available for the following platforms:</p> <ul> <li>Amazon EKS</li> <li>Google GKE</li> <li>IBM IKS</li> <li>Red Hat OpenShift Cloud Platform</li> <li>Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.</li> </ul>"},{"location":"client/configure-kn/","title":"Customizing kn","text":"<p>You can customize your <code>kn</code> CLI setup by creating a <code>config.yaml</code> configuration file. You can provide this configuration by using the <code>--config</code> flag, otherwise the configuration is picked up from a default location. The default configuration location conforms to the XDG Base Directory Specification, and is different for Unix systems and Windows systems.</p> <ul> <li>If the <code>XDG_CONFIG_HOME</code> environment variable is set, the default configuration location that <code>kn</code> looks for is <code>$XDG_CONFIG_HOME/kn</code>.</li> <li>If the <code>XDG_CONFIG_HOME</code> environment variable is not set, <code>kn</code> looks for the configuration in the home directory of the user at <code>$HOME/.config/kn/config.yaml</code>.</li> <li>For Windows systems, the default <code>kn</code> configuration location is <code>%APPDATA%\\kn</code>.</li> </ul>"},{"location":"client/configure-kn/#example-configuration-file","title":"Example configuration file","text":"<pre><code># Plugins related configuration\nplugins:\n  # Whether to lookup configuration in the execution path (default: true). This option is deprecated and will be removed in a future version where path lookup will be enabled unconditionally\n  path-lookup: true\n  # Directory from where plugins with the prefix \"kn-\" are looked up. (default: \"$base_dir/plugins\"\n  # where \"$base_dir\" is the directory where this configuration file is stored)\n  directory: ~/.config/kn/plugins\n# Eventing related configuration\neventing:\n  # List of sink mappings that allow custom prefixes wherever a sink\n  # specification is used (like for the --sink option of a broker)\n  sink-mappings:\n    # Prefix as used in the command (e.g. \"--sink svc:myservice\")\n  - prefix: svc\n    # Api group of the mapped resource\n    group: core\n    # Api version of the mapped resource\n    version: v1\n    # Resource name (lowercased plural form of the 'kind')\n    resource: services\n  # Channel mappings that you can use in --channel options\n  channel-type-mappings:\n    # Alias that can be used as a type for a channel option (e.g. \"kn create channel mychannel --type Kafka\")\n  - alias: Kafka\n    # Api group of the mapped resource\n    group: messaging.knative.dev\n    # Api version of the mapped resource\n    version: v1beta1\n    # Kind of the resource\n    kind: KafkaChannel\n</code></pre> <p>Where</p> <ul> <li><code>path-lookup</code> specifies whether <code>kn</code> should look for plugins in the <code>PATH</code> environment variable. This is a boolean configuration option (default: <code>true</code>). Note: the <code>path-lookup</code> option has been deprecated and will be removed in a future version where path lookup will be enabled unconditionally.</li> <li><code>directory</code> specifies the directory where <code>kn</code> will look for plugins. The default path depends on the operating system, as described earlier. This can be any directory that is visible to the user (default: <code>$base_dir/plugins</code>, where <code>$base_dir</code> is the directory where this configuration file is stored).</li> <li><code>sink-mappings</code> defines the Kubernetes Addressable resource that is used when you use the <code>--sink</code> flag with a <code>kn</code> CLI command.<ul> <li><code>prefix</code>: The prefix you want to use to describe your sink. Service, <code>svc</code>, <code>channel</code>, and <code>broker</code> are predefined prefixes in <code>kn</code>. </li> <li><code>group</code>: The API group of the Kubernetes resource.</li> <li><code>version</code>: The version of the Kubernetes resource.</li> <li><code>resource</code>: The lowercased, plural name of the Kubernetes resource type. For example, <code>services</code> or <code>brokers</code>.</li> </ul> </li> <li><code>channel-type-mappings</code> can be used to define aliases for custom channel types that can be used wherever a channel type is required (as in <code>kn channel create --type</code>). This configuration section defines an array of entries with the following fields:</li> <li><code>alias</code>: The name that can be used as the type</li> <li><code>group</code>: The APIGroup of the channel CRD.</li> <li><code>version</code>: The version of the channel CRD.</li> <li><code>kind</code>: Kind of the channel CRD (e.g. <code>KafkaChannel</code>)</li> </ul>"},{"location":"client/install-kn/","title":"Installing the Knative CLI","text":"<p>This guide provides details about how you can install the Knative <code>kn</code> CLI.</p>"},{"location":"client/install-kn/#verifying-cli-binaries","title":"Verifying CLI binaries","text":"<p>Knative <code>kn</code> CLI releases from 1.9 onwards are signed with cosign. You can use the following steps to verify the CLI binaries:</p> <ol> <li> <p>Download the files you want, and the <code>checksums.txt</code>, <code>checksum.txt.pem</code>, and <code>checksums.txt.sig</code> files from the releases page, by running the commands:</p> <pre><code>wget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/checksums.txt\nwget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/kn-darwin-amd64\nwget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/checksums.txt.sig\nwget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/checksums.txt.pem\n</code></pre> <p>Where <code>&lt;kn-version&gt;</code> is the version of the CLI that you want to verify. For example, <code>knative-v1.8.0</code>.</p> </li> <li> <p>Verify the signature by running the command:</p> <pre><code>cosign verify-blob \\\n--cert checksums.txt.pem \\\n--signature checksums.txt.sig \\\n--certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n--certificate-oidc-issuer=https://accounts.google.com \\\nchecksums.txt\n</code></pre> </li> <li> <p>If the signature is valid, you can then verify the <code>SHA256</code> sums match the downloaded binary, by running the command:</p> <pre><code>sha256sum --ignore-missing -c checksums.txt\n</code></pre> </li> </ol> <p>Note</p> <p>Knative images are signed in <code>KEYLESS</code> mode. To learn more about keyless signing, please refer to Keyless Signatures. The signing identity for Knative releases is <code>signer@knative-releases.iam.gserviceaccount.com</code>, and the issuer is <code>https://accounts.google.com</code>.</p>"},{"location":"client/install-kn/#install-the-knative-cli","title":"Install the Knative CLI","text":"<p>The Knative CLI (<code>kn</code>) provides a quick and easy interface for creating Knative resources, such as Knative Services and Event Sources, without the need to create or modify YAML files directly.</p> <p>The <code>kn</code> CLI also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting.</p> Using HomebrewUsing a binaryUsing GoUsing a container image <p>Do one of the following:</p> <ul> <li> <p>To install <code>kn</code> by using Homebrew, run the command (Use <code>brew upgrade</code> instead if you are upgrading from a previous version):</p> <pre><code>brew install knative/client/kn\n</code></pre> Having issues upgrading <code>kn</code> using Homebrew? <p>If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the <code>master</code> branch was renamed to <code>main</code>. Resolve this issue by running the command:</p> <pre><code>brew uninstall kn\nbrew untap knative/client --force\nbrew install knative/client/kn\n</code></pre> </li> </ul> <p>You can install <code>kn</code> by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>kn</code> release page.</p> </li> <li> <p>Rename the binary to <code>kn</code> and make it executable by running the commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; kn\nchmod +x kn\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>kn-darwin-amd64</code> or <code>kn-linux-amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn /usr/local/bin\n</code></pre> </li> <li> <p>Verify that <code>kn</code> commands are working properly. For example:</p> <pre><code>kn version\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>kn</code> client repository:</p> <pre><code>git clone https://github.com/knative/client.git\ncd client/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>hack/build.sh -f\n</code></pre> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn /usr/local/bin\n</code></pre> </li> <li> <p>Verify that <code>kn</code> commands are working properly. For example:</p> <pre><code>kn version\n</code></pre> </li> </ol> <p>Links to images are available here:</p> <ul> <li>Latest release</li> </ul> <p>You can run <code>kn</code> from a container image. For example:</p> <pre><code>docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list\n</code></pre> <p>Note</p> <p>Running <code>kn</code> from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use <code>kn</code>.</p>"},{"location":"client/install-kn/#install-kn-using-the-nightly-built-binary","title":"Install kn using the nightly-built binary","text":"<p>Warning</p> <p>Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable.</p> <p>Nightly-built executable binaries are available for users who want to install the latest pre-release build of <code>kn</code>.</p> <p>Links to the latest nightly-built executable binaries are available here:</p> <ul> <li>macOS</li> <li>Linux</li> <li>Windows</li> </ul>"},{"location":"client/install-kn/#using-kn-with-tekton","title":"Using kn with Tekton","text":"<p>See the Tekton documentation.</p>"},{"location":"client/kn-plugins/","title":"kn plugins","text":"<p>The <code>kn</code> CLI supports the use of plugins. Plugins enable you to extend the functionality of your <code>kn</code> installation by adding custom commands and other shared commands that are not part of the core distribution of <code>kn</code>.</p> <p>Warning</p> <p>The plugins must be named with the prefix <code>kn-</code> to be detected by <code>kn</code>. For example, <code>kn-func</code> will be detected but <code>func</code> won't be detected.</p>"},{"location":"client/kn-plugins/#kn-source-plugins","title":"kn source plugins","text":"<p>An event source plugin has the following characteristics:</p> <ul> <li>It has a name that is part of the <code>kn source</code> group.</li> <li>It provides CRUD sub-commands; <code>create</code>, <code>update</code>, <code>delete</code>, <code>describe</code>, and sometimes <code>apply</code>.</li> <li>It requires a mandatory <code>--sink</code> flag to be passed when using the <code>create</code> command.</li> </ul>"},{"location":"client/kn-plugins/#list-of-knative-plugins","title":"List of Knative plugins","text":"<p>You can view all available <code>kn</code> plugins in the Knative Extensions repository.</p> Plugin Description Available via Homebrew? kn-plugin-admin <code>kn</code> plugin for managing a Kubernetes based Knative installation Y kn-plugin-diag <code>kn</code> plugin for diagnosing issues by exposing detailed information for different layers of Knative objects N kn-plugin-event <code>kn</code> plugin for sending events to Knative sinks Y kn-plugin-func <code>kn</code> plugin for functions Y kn-plugin-migration <code>kn</code> plugin for migrating Knative Services from one cluster to another N kn-plugin-operator <code>kn</code> plugin for managing Knative with Knative Operator N kn-plugin-quickstart <code>kn</code> plugin for developers to install a quickstart Knative cluster for experimentation purposes Y kn-plugin-service-log <code>kn</code> plugin for showing the standard output of Knative Services N kn-plugin-source-kafka <code>kn</code> plugin for managing Kafka event sources Y kn-plugin-source-kamelet <code>kn</code> plugin for managing Kamelets and KameletBindings Y"},{"location":"client/kn-plugins/#manually-install-a-plugin","title":"Manually install a plugin","text":"<p>You can manually install all plugins. To manually install a plugin:</p> <ol> <li>Download the current release of the plugin from GitHub. See the list of Knative plugins you can download.</li> <li>Rename the file to remove the OS and architecture information. For example, rename <code>kn-admin-darwin-amd64</code> to <code>kn-admin</code>.</li> <li>Make the plugin executable. For example, <code>chmod +x kn-admin</code>.</li> <li>Move the file to a directory on your <code>PATH</code>. For example, <code>/usr/local/bin</code>.</li> </ol>"},{"location":"client/kn-plugins/#install-a-plugin-by-using-homebrew","title":"Install a plugin by using Homebrew","text":"<p>You can install some plugins can be installed using the Knative plugins Homebrew Tap. For example, you can install the <code>kn-admin</code> plugin by running <code>brew install knative-extensions/kn-plugins/admin</code>.</p>"},{"location":"client/kn-plugins/#list-available-plugins","title":"List available plugins","text":"<p>You can list all available (installed) plugins by entering the command:</p> <pre><code>kn plugin list\n</code></pre>"},{"location":"community/","title":"How to Get Involved","text":"<p>The Knative community consists of three different audiences:</p> <p></p> <ul> <li> <p>Developers write serverless and event-driven applications using the   constructs surfaced by Knative.  They build containers and applications which   leverage the concepts exposed by Knative to deliver services to end users and   systems.</p> </li> <li> <p>Operators build and maintain Kubernetes platforms where Knative is   installed.  They also manage the default settings for Knative on those   clusters.</p> </li> <li> <p>Contributors are the authors of the Knative   components; they contribute and review source code, run the community   meetings, document and publicize the project, and otherwise keep the project   running.</p> </li> </ul> <p>Knative components are intended to operate well stand-alone, or integrated into a larger platform offering by either an internal systems team or a cloud provider.  You can see a list of organizations that have adopted Knative in our community repo.</p>"},{"location":"community/#questions","title":"Questions","text":"<p>For the fastest response, you can ask questions on the <code>#knative</code>, <code>#knative-serving</code>, <code>#knative-eventing</code>, or <code>#knative-functions</code> channels of the CNCF Slack.</p> <p>We also have a Stack Overflow topic, <code>knative</code> (as well as <code>knative-serving</code>, <code>knative-eventing</code>, and <code>knative-functions</code>), and the knative-users mailing list if you prefer those formats.</p>"},{"location":"community/#bug-reports-and-feature-requests","title":"Bug Reports and Feature Requests","text":"<p>Knative is composed of many different components. We use GitHub Issues to track bug reports and feature requests. While it's most helpful if you know the specific component that the bug is happening in, we understand that it can be difficult to tell sometimes. A clear bug report in the wrong component is much better than a partial report in the right component, so feel free to file your issue in one of these three main repositories if you're not sure:</p> <ul> <li>Serving</li> <li>Eventing</li> <li>Functions</li> </ul> <p>A good bug report should include:</p> <ul> <li>What you were trying to do, and what happened</li> <li>What version of Knative and Kubernetes you are using (if using a cloud provider, indicate which one)</li> <li>Relevant resource yaml, HTTP requests, or log lines</li> </ul>"},{"location":"community/#community-meetups","title":"Community Meetups","text":"<p>This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative.</p> <p>Catch up with past community meetups on our YouTube channel.</p> <p>Stay tuned for new events by subscribing to the calendar (iCal export file) and following us on Twitter.</p>"},{"location":"community/#communication-channels","title":"Communication Channels","text":"<p>Much of the community meets on the CNCF Slack, using the following channels:</p> <ul> <li> <p>#knative: General discussion about Knative usage</p> </li> <li> <p>#knative-client: Discussion about the Knative CLI client <code>kn</code></p> </li> <li> <p>#knative-conformance: Knative API standardization</p> </li> <li> <p>#knative-contributors: General discussion channel for folks contributing to the Knative project in any capacity</p> </li> <li> <p>#knative-documentation: Knative documentation and website user experience</p> </li> <li> <p>#knative-eventing: Discussion for Knative Eventing</p> </li> <li> <p>#knative-functions: Discussion for Knative Functions</p> </li> <li> <p>#knative-productivity: Discussion around Knative integration with test infrastructure, Prow and other tooling on GitHub</p> </li> <li> <p>#knative-release: Coordinating and managing Knative releases</p> </li> <li> <p>#knative-security: Discussion area for security issues affecting Knative components</p> </li> <li> <p>#knative-serving: Discussion area for Knative Serving and related areas (networking / autoscaling)</p> </li> </ul> <p>We also have user (<code>knative-users@googlegroups.com</code>) and developer (<code>knative-dev@googlegroups.com</code>) mailing lists for discussions, and for access to documents in the shared Google Drive. Access to <code>knative-users@</code> is automatically approved; access to <code>knative-dev@</code> is handled via a lightweight approval process -- it helps if you provide a one-sentence description when requesting access.</p> <p>We also use GitHub Issues and GitHub projects for tracking longer-term efforts, including each working group's roadmap, and the backlog for oversight committees like the Technical Oversight Committee and the Steering Committee.</p> <p>Feature design is generally done via Google Docs stored in a shared Google Drive. Due to limitations of shared drives, the default access control is that all documents are readable by <code>knative-users@</code> and commentable and editable by <code>knative-dev@</code>. Documents generally cannot be deleted without special intervention.</p>"},{"location":"community/#meetings","title":"Meetings","text":"<p>Knative schedules all meetings on a shared Google calendar. Working group leads and TOC/Steering members should have permissions to add events, including one-off events for coordination if needed.</p> <p>If you're using Google Calendar, the above should work. If you're using some other system (Apple Calendar or Outlook, for example), here is an iCal export of the community calendar.</p> <ul> <li>Follow these directions to import into Outlook Web</li> <li>Follow these directions for desktop Outlook</li> <li>Follow the import directions to import into Apple Calendar</li> </ul>"},{"location":"community/#getting-more-involved-as-a-contributor","title":"Getting More Involved as a Contributor","text":"<p>If you're interested in becoming a Knative contributor, you'll want to check out our contributor page.  Contributing is not an expectation -- everyone who uses Knative, talks about it, lurks on a mailing list, or engages with the ideas is part of our community!  If you do want to get more deeply involved, you'll find that contributing to open source can be a great career booster.</p>"},{"location":"community/#governance","title":"Governance","text":"<p>Knative is part of the CNCF, and is governed by those rules, including the CNCF Code of Conduct. Beyond that, we have a page about our governance rules.</p>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>Knative follows the CNCF Code of Conduct. Reports of code of conduct violations may be sent to code-of-conduct@knative.team or conduct@cncf.io.</p>"},{"location":"community/contributing/","title":"Contribute to Knative","text":"<p>This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved.</p>"},{"location":"community/contributing/#prerequisites","title":"Prerequisites","text":"<p>If you want to contribute to Knative, you must do the following:</p> <ul> <li> <p>Before you can make a contribution, you must sign the CNCF EasyCLA using the same email address you use for <code>git commit</code>s. For more information, see Contributor license agreements.</p> </li> <li> <p>Read the Knative contributor guide.</p> </li> <li> <p>Read the Code of conduct.</p> </li> </ul> <p>For more information about how the Knative community is run, see About the Knative community.</p>"},{"location":"community/contributing/#contribute-to-the-code","title":"Contribute to the code","text":"<p>Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub.</p> <p>Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label.</p> <ul> <li> <p>Knative Serving:</p> <ul> <li>To get started with contributing, see the Serving development workflow.</li> <li>For good starter issues, see Serving issues.</li> </ul> </li> <li> <p>Knative Eventing:</p> <ul> <li>To get started with contributing, see the Eventing development workflow.</li> <li>For good starter issues, see Eventing issues.</li> </ul> </li> <li> <p>Knative Client (kn):</p> <ul> <li>To get started with contributing, see the Client development workflow.</li> <li>For good starter issues, see Client issues.</li> </ul> </li> <li> <p>Functions:</p> <ul> <li>To get started with contributing, see the Functions development workflow.</li> <li>For good starter issues, see Functions issues.</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>To get started with contributing, see the Docs contributor guide.</li> <li>For good starter issues, see Documentation issues.</li> </ul> </li> </ul>"},{"location":"community/contributing/#contribute-code-samples-to-the-community","title":"Contribute code samples to the community","text":"<p>Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative?</p> <p>Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems.</p> <p>Community tutorials are stored in Markdown files under the <code>code-samples/community</code>. These documents are contributed, reviewed, and maintained by the community.</p> <p>Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial.</p> <ul> <li> <p>Contribute code samples: Share your samples with the community.</p> </li> <li> <p>Link existing code samples: Link to your Knative samples that live on another site.</p> </li> </ul>"},{"location":"community/contributing/#learn-and-connect","title":"Learn and connect","text":"<p>Using or want to use Knative? Have any questions? Find out more here:</p> <ul> <li> <p>Knative users group: Discussion and help from your fellow users.</p> </li> <li> <p>Knative developers group Discussion and help from Knative developers.</p> </li> <li> <p>Knative Slack: Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines.</p> </li> <li> <p>Twitter: Follow us on Twitter to get the latest news!</p> </li> <li> <p>Stack Overflow questions: Knative tagged questions and curated answers.</p> </li> </ul>"},{"location":"community/contributing/#community-meetups","title":"Community Meetups","text":"<p>New to Knative or built something cool?  Share your impressions by presenting at the Knative Community Meetup!  Use this form to sign up to present.</p> <p>Catch up with past community meetups on our YouTube channel.</p> <p>Stay tuned for new events by subscribing to the calendar (iCal export file) and following us on Twitter.</p>"},{"location":"community/governance/","title":"Community Rules and Practices","text":"<p>This page provides links to documents for common Knative community practices and a description of Knative's audience.</p> <p>While the project was started by Google, it has received contributions from over 200 companies and is now governed by the CNCF.</p>"},{"location":"community/governance/#community-values","title":"Community values","text":"<p>This section links to documents about our values.</p> <ul> <li> <p>Knative project values: shared goals and values for the community.</p> </li> <li> <p>Knative team values: the goals and values we hold as a team.</p> </li> </ul>"},{"location":"community/governance/#governance","title":"Governance","text":"<p>This section links to documents about how the Knative community is governed.</p> <p>Knative has public and recorded monthly community meetings.  Each component has one or more working groups driving the effort, and Knative has a single technical oversight committee monitoring the overall project.</p> <ul> <li> <p>Governance: the Knative governance framework.</p> </li> <li> <p>Community roles: describes the roles individuals can assume within the Knative community such as member, approver, or working group lead.</p> </li> <li> <p>Working groups: provides information about our various working groups.</p> </li> <li> <p>Steering Committee (SC): describes our steering committee.</p> </li> <li> <p>Technical Oversight Committee (TOC): describes our technical oversight committee.</p> </li> <li> <p>Annual reports: lists previous annual reports.</p> </li> </ul>"},{"location":"community/governance/#processes","title":"Processes","text":"<p>This section links to documents for common Knative community processes.</p> <p>At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs.</p> <ul> <li> <p>Reviewing and Merging Pull Requests: how we manage pull requests.</p> </li> <li> <p>Working group processes: how working groups operate.</p> </li> <li> <p>SC election process: elcection process for our steering committee.</p> </li> <li> <p>TOC election process: election process for our technical oversight committee.</p> </li> <li> <p>Repository Guidelines: how we create and remove core repositories.</p> </li> <li> <p>Extensions repo process: how to create a repo in the <code>knative-extensions</code> GitHub org.</p> </li> <li> <p>Feature tracks: outlines the process for adding non-trivial features.</p> </li> <li> <p>Golang policy: principles regarding the Golang version Knative tests and releases with.</p> </li> <li> <p>Release principles: release principles including information about support and feature phases.</p> </li> <li> <p>Release schedule: Knative past and future release dates.</p> </li> <li> <p>Sunsetting features: process to sunset features that are getting no apparent usage, but are time consuming to maintain.</p> </li> </ul>"},{"location":"community/governance/#community-calendar","title":"Community calendar","text":"<p>The Knative community calendar (iCal export file) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings.</p> <p>Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the <code>#knative</code> channel in the CNCF Slack workspace.</p>"},{"location":"community/governance/#knatives-audience","title":"Knative's audience","text":"<p>Knative is designed for different personas:</p> <p></p>"},{"location":"community/governance/#developers","title":"Developers","text":"<p>Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime.</p> <p>To join the conversation, head over to the Knative users Google group.</p>"},{"location":"community/governance/#operators","title":"Operators","text":"<p>Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate.</p> <p>Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers.</p>"},{"location":"community/governance/#contributors","title":"Contributors","text":"<p>With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow.</p> <p>Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components:</p>"},{"location":"community/governance/#knative-authors","title":"Knative authors","text":"<p>Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>The documentation in this section explains commonly referenced Knative concepts and abstractions, and helps to provide you with a better understanding of how Knative works.</p>"},{"location":"concepts/#what-is-knative","title":"What is Knative?","text":"<p>Knative is a platform-agnostic solution for running serverless deployments.</p>"},{"location":"concepts/#knative-serving","title":"Knative Serving","text":"<p>Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These resources are used to define and control how your serverless workload behaves on the cluster.</p> <p></p> <p>The primary Knative Serving resources are Services, Routes, Configurations, and Revisions:</p> <ul> <li> <p>Services:   The <code>service.serving.knative.dev</code> resource automatically manages the whole   lifecycle of your workload. It controls the creation of other objects to   ensure that your app has a route, a configuration, and a new revision for each   update of the service. Service can be defined to always route traffic to the   latest revision or to a pinned revision.</p> </li> <li> <p>Routes:   The <code>route.serving.knative.dev</code> resource maps a network endpoint to one or   more revisions. You can manage the traffic in several ways, including   fractional traffic and named routes.</p> </li> <li> <p>Configurations:   The <code>configuration.serving.knative.dev</code> resource maintains the desired state   for your deployment. It provides a clean separation between code and   configuration and follows the Twelve-Factor App methodology. Modifying a   configuration creates a new revision.</p> </li> <li> <p>Revisions:   The <code>revision.serving.knative.dev</code> resource is a point-in-time snapshot of the   code and configuration for each modification made to the workload. Revisions   are immutable objects and can be retained for as long as useful. Knative   Serving Revisions can be automatically scaled up and down according to   incoming traffic.</p> </li> </ul> <p>For more information on the resources and their interactions, see the Resource Types Overview in the <code>serving</code> Github repository.</p>"},{"location":"concepts/#knative-eventing-the-event-driven-application-platform-for-kubernetes","title":"Knative Eventing - The Event-driven application platform for Kubernetes","text":"<p>Knative Eventing is a collection of APIs that enable you to use an event-driven architecture with your applications. You can use these APIs to create components that route events from event producers (known as sources) to event consumers (known as sinks) that receive events. Sinks can also be configured to respond to HTTP requests by sending a response event.</p> <p>Knative Eventing is a standalone platform that provides support for various types of workloads, including standard Kubernetes Services and Knative Serving Services.</p> <p>Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks. These events conform to the CloudEvents specifications, which enables creating, parsing, sending, and receiving events in any programming language.</p> <p>Knative Eventing components are loosely coupled, and can be developed and deployed independently of each other. Any producer can generate events before there are active event consumers that are listening for those events. Any event consumer can express interest in a class of events before there are producers that are creating those events.</p>"},{"location":"concepts/duck-typing/","title":"Duck typing","text":"<p>Knative enables loose coupling of its components by using duck typing.</p> <p>Duck typing means that the compatibility of a resource for use in a Knative system is determined by certain properties that are used to identify the resource control plane shape and behaviors. These properties are based on a set of common definitions for different types of resources, called duck types.</p> <p>Knative can use a resource as if it is the generic duck type, without specific knowledge about the resource type, if:</p> <ul> <li>The resource has the same fields in the same schema locations as the common definition specifies</li> <li>The same control or data plane behaviors as the common definition specifies</li> </ul> <p>Some resources can opt in to multiple duck types.</p> <p>A fundamental use of duck typing in Knative is using object references in resource specs to point to other resources. The definition of the object containing the reference prescribes the expected duck type of the resource being referenced.</p>"},{"location":"concepts/duck-typing/#example","title":"Example","text":"<p>In the following example, a Knative <code>Example</code> resource named <code>pointer</code> references a <code>Dog</code> resource named <code>pointee</code> in its spec:</p> <pre><code>apiVersion: sample.knative.dev/v1\nkind: Example\nmetadata:\n  name: pointer\nspec:\n  size:\n    apiVersion: extension.example.com/v1\n    kind: Dog\n    name: pointee\n</code></pre> <p>If the expected shape of a Sizable duck type is that, in the <code>status</code>, the schema shape is the following:</p> <pre><code>status:\n  height: &lt;in centimetres&gt;\n  weight: &lt;in kilograms&gt;\n</code></pre> <p>Now the instance of <code>pointee</code> could look like this:</p> <pre><code>apiVersion: extension.example.com/v1\nkind: Dog\nmetadata:\n  name: pointee\nspec:\n  owner: Smith Family\n  etc: more here\nstatus:\n  lastFeeding: 2 hours ago\n  hungry: true\n  age: 2\n  height: 60\n  weight: 20\n</code></pre> <p>When the <code>Example</code> resource functions, it only acts on the information in the Sizable duck type shape, and the <code>Dog</code> implementation is free to have the information that makes the most sense for that resource. The power of duck typing is apparent when we extend the system with a new type, for example, <code>Human</code>, if the new resource adheres to the contract set by Sizable.</p> <pre><code>apiVersion: sample.knative.dev/v1\nkind: Example\nmetadata:\n  name: pointer\nspec:\n  size:\n    apiVersion: people.example.com/v1\n    kind: human\n    name: pointee\n---\napiVersion: people.example.com/v1\nkind: Human\nmetadata:\n  name: pointee\nspec:\n  etc: even more here\nstatus:\n  college: true\n  hungry: true\n  age: 22\n  height: 170\n  weight: 50\n</code></pre> <p>The <code>Example</code> resource is able to apply the logic configured for it, without explicit knowledge of <code>Human</code> or <code>Dog</code>.</p>"},{"location":"concepts/duck-typing/#knative-duck-types","title":"Knative Duck Types","text":"<p>Knative defines several duck type contracts that are in use across the project:</p> <ul> <li>Addressable</li> <li>Binding</li> <li>Channelable </li> <li>Podspecable </li> <li>Source</li> </ul>"},{"location":"concepts/duck-typing/#addressable","title":"Addressable","text":"<p>Addressable is expected to be the following shape:</p> <pre><code>apiVersion: group/version\nkind: Kind\nstatus:\n  address:\n    url: http://host/path?query\n</code></pre>"},{"location":"concepts/duck-typing/#binding","title":"Binding","text":"<p>With a direct <code>subject</code>, Binding is expected to be in the following shape:</p> <pre><code>apiVersion: group/version\nkind: Kind\nspec:\n  subject:\n    apiVersion: group/version\n    kind: SomeKind\n    namespace: the-namespace\n    name: a-name\n</code></pre> <p>With an indirect <code>subject</code>, Binding is expected to be in the following shape:</p> <pre><code>apiVersion: group/version\nkind: Kind\nspec:\n  subject:\n    apiVersion: group/version\n    kind: SomeKind\n    namespace: the-namespace\n    selector:\n      matchLabels:\n        key: value\n</code></pre>"},{"location":"concepts/duck-typing/#source","title":"Source","text":"<p>With a <code>ref</code> Sink, Source is expected to be in the following shape:</p> <pre><code>apiVersion: group/version\nkind: Kind\nspec:\n  sink:\n    ref:\n      apiVersion: group/version\n      kind: AnAddressableKind\n      name: a-name\n  ceOverrides:\n    extensions:\n      key: value\nstatus:\n  observedGeneration: 1\n  conditions:\n    - type: Ready\n      status: \"True\"\n  sinkUri: http://host\n</code></pre> <p>With a <code>uri</code> Sink, Source is expected to be in the following shape:</p> <pre><code>apiVersion: group/version\nkind: Kind\nspec:\n  sink:\n    uri: http://host/path?query\n  ceOverrides:\n    extensions:\n      key: value\nstatus:\n  observedGeneration: 1\n  conditions:\n    - type: Ready\n      status: \"True\"\n  sinkUri: http://host/path?query\n</code></pre> <p>With <code>ref</code> and <code>uri</code> Sinks, Source is expected to be in the following shape:</p> <pre><code>apiVersion: group/version\nkind: Kind\nspec:\n  sink:\n    ref:\n      apiVersion: group/version\n      kind: AnAddressableKind\n      name: a-name\n    uri: /path?query\n  ceOverrides:\n    extensions:\n      key: value\nstatus:\n  observedGeneration: 1\n  conditions:\n    - type: Ready\n      status: \"True\"\n  sinkUri: http://host/path?query\n</code></pre>"},{"location":"concepts/eventing-resources/brokers/","title":"Brokers","text":"<p>Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of events. Brokers provide a discoverable endpoint for event ingress, and use Triggers for event delivery. Event producers can send events to a broker by POSTing the event.</p> <p></p>"},{"location":"concepts/eventing-resources/brokers/#related-concepts","title":"Related concepts","text":""},{"location":"concepts/eventing-resources/brokers/#triggers","title":"Triggers","text":"<p>After an event has entered a broker, it can be forwarded to subscribers by using Triggers. Triggers allow events to be filtered by attributes, so that events with particular attributes can be sent to Subscribers that have registered interest in events with those attributes.</p>"},{"location":"concepts/eventing-resources/brokers/#subscribers","title":"Subscribers","text":"<p>A Subscriber can be any URL or Addressable resource. Subscribers can also reply to an active request from the Broker, and can respond with a new event that is sent back into the Broker.</p>"},{"location":"concepts/serving-resources/revisions/","title":"Revisions","text":"<p>Revisions are Knative Serving resources that contain point-in-time snapshots of the application code and configuration for each change made to a Knative Service.</p> <p>You cannot create Revisions or update a Revision spec directly; Revisions are always created in response to updates to a Configuration spec. However, you can force the deletion of Revisions, to handle leaked resources as well as for removal of known bad Revisions to avoid future errors when managing a Knative Service.</p> <p>Revisions are generally immutable, except where they may reference mutable core Kubernetes resources such as ConfigMaps and Secrets. Revisions can also be mutated by changes in Revision defaults. Changes to defaults that mutate Revisions are generally syntactic and not semantic.</p>"},{"location":"concepts/serving-resources/revisions/#related-concepts","title":"Related concepts","text":""},{"location":"concepts/serving-resources/revisions/#autoscaling","title":"Autoscaling","text":"<p>Revisions can be automatically scaled up and down according to incoming traffic. For more information, see Autoscaling.</p>"},{"location":"concepts/serving-resources/revisions/#gradual-rollout-of-traffic-to-revisions","title":"Gradual rollout of traffic to Revisions","text":"<p>Revisions enable progressive roll-out and roll-back of application changes. For more information, see Configuring gradual rollout of traffic to Revisions.</p>"},{"location":"concepts/serving-resources/revisions/#garbage-collection","title":"Garbage collection","text":"<p>When Revisions of a Knative Service are inactive, they are automatically cleaned up and cluster resources are reclaimed after a set time period. This is known as garbage collection.</p> <p>You can configure garbage collection parameters a specific Revision if you are a developer. You can also configure default, cluster-wide garbage collection parameters for all the Revisions of all the Services on a cluster if you have cluster administrator permissions.</p> <p>For more information, see Configuration options for Revisions.</p>"},{"location":"concepts/serving-resources/revisions/#configuration-options-for-revisions","title":"Configuration options for Revisions","text":"<ul> <li>Cluster administrator (global, cluster-wide) configuration options</li> <li>Developer (per Revision) configuration options</li> </ul>"},{"location":"concepts/serving-resources/revisions/#additional-resources","title":"Additional resources","text":"<ul> <li>Revision API specification</li> </ul>"},{"location":"concepts/serving-resources/revisions/#next-steps","title":"Next steps","text":"<ul> <li>Delete, describe, and list Revisions by using the Knative (<code>kn</code>) CLI</li> <li>Check the status of a Revision</li> <li>Routing traffic between Revisions of a Service</li> </ul>"},{"location":"eventing/","title":"Knative Eventing - The Event-driven application platform for Kubernetes","text":"<p>Knative Eventing is a collection of APIs that enable you to use an event-driven architecture with your applications. You can use these APIs to create components that route events from event producers (known as sources) to event consumers (known as sinks) that receive events. Sinks can also be configured to respond to HTTP requests by sending a response event.</p> <p>Knative Eventing is a standalone platform that provides support for various types of workloads, including standard Kubernetes Services and Knative Serving Services.</p> <p>Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks. These events conform to the CloudEvents specifications, which enables creating, parsing, sending, and receiving events in any programming language.</p> <p>Knative Eventing components are loosely coupled, and can be developed and deployed independently of each other. Any producer can generate events before there are active event consumers that are listening for those events. Any event consumer can express interest in a class of events before there are producers that are creating those events.</p> <p>Examples of supported Knative Eventing use cases:</p> <ul> <li> <p>Publish an event without creating a consumer. You can send events to a broker as an HTTP POST, and use binding to decouple the destination configuration from your application that produces events.</p> </li> <li> <p>Consume an event without creating a publisher. You can use a trigger to consume events from a broker based on event attributes. The application receives events as an HTTP POST.</p> </li> </ul> <p>Tip</p> <p>Multiple event producers and sinks can be used together to create more advanced Knative Eventing flows to solve complex use cases.</p>"},{"location":"eventing/#eventing-examples","title":"Eventing examples","text":"<p> Creating and responding to Kubernetes API events</p> <p>image/svg+xml Creating an image processing pipeline</p> <p>image/svg+xml Facilitating AI workloads at the edge in large-scale, drone-powered sustainable agriculture projects</p>"},{"location":"eventing/#next-steps","title":"Next steps","text":"<ul> <li>You can install Knative Eventing by using the methods listed on the installation page.</li> </ul>"},{"location":"eventing/accessing-traces/","title":"Accessing CloudEvent traces","text":"<p>Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests.</p>"},{"location":"eventing/accessing-traces/#before-you-begin","title":"Before you begin","text":"<p>You must have a Knative cluster running with the Eventing component installed. Learn more.</p>"},{"location":"eventing/accessing-traces/#configuring-tracing","title":"Configuring tracing","text":"<p>With the exception of importers, the Knative Eventing tracing is configured through the <code>config-tracing</code> ConfigMap in the <code>knative-eventing</code> namespace.</p> <p>Most importers do not use the ConfigMap and instead, use a static 1% sampling rate.</p> <p>You can use the <code>config-tracing</code> ConfigMap to configure the following Eventing components:</p> <ul> <li>Brokers</li> <li>Triggers</li> <li>InMemoryChannel</li> <li>ApiServerSource</li> <li>PingSource</li> <li>GitlabSource</li> <li>KafkaSource</li> <li>PrometheusSource</li> </ul> <p>Example:</p> <p>The following example <code>config-tracing</code> ConfigMap samples 10% of all CloudEvents:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-tracing\n  namespace: knative-eventing\ndata:\n  backend: \"zipkin\"\n  zipkin-endpoint: \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\"\n  sample-rate: \"0.1\"\n</code></pre>"},{"location":"eventing/accessing-traces/#configuration-options","title":"Configuration options","text":"<p>You can configure your <code>config-tracing</code> with following options:</p> <ul> <li> <p><code>backend</code>: Valid values are <code>zipkin</code> or <code>none</code>. The default is <code>none</code>.</p> </li> <li> <p><code>zipkin-endpoint</code>: Specifies the URL to the zipkin collector where you want to send the traces.    Must be set if backend is set to <code>zipkin</code>.</p> </li> <li> <p><code>sample-rate</code>: Specifies the sampling rate. Valid values are decimals from <code>0</code> to <code>1</code>    (interpreted as a float64), which indicate the probability that any given request is sampled.    An example value is <code>0.5</code>, which gives each request a 50% sampling probablity.</p> </li> <li> <p><code>debug</code>: Enables debugging. Valid values are <code>true</code> or <code>false</code>. Defaults to <code>false</code> when not specified.    Set to <code>true</code> to enable debug mode, which forces the <code>sample-rate</code> to <code>1.0</code> and sends all spans to    the server.</p> </li> </ul>"},{"location":"eventing/accessing-traces/#viewing-your-config-tracing-configmap","title":"Viewing your <code>config-tracing</code> ConfigMap","text":"<p>To view your current configuration:</p> <pre><code>kubectl -n knative-eventing get configmap config-tracing -oyaml\n</code></pre>"},{"location":"eventing/accessing-traces/#editing-and-deploying-your-config-tracing-configmap","title":"Editing and deploying your <code>config-tracing</code> ConfigMap","text":"<p>To edit and then immediately deploy changes to your ConfigMap, run the following command:</p> <pre><code>kubectl -n knative-eventing edit configmap config-tracing\n</code></pre>"},{"location":"eventing/accessing-traces/#accessing-traces-in-eventing","title":"Accessing traces in Eventing","text":"<p>To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section:</p> <ul> <li>Zipkin</li> <li>Jaeger</li> </ul>"},{"location":"eventing/accessing-traces/#example","title":"Example","text":"<p>The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the <code>TestBrokerTracing</code> End-to-End test.</p> <p>For this example, assume the following details:</p> <ul> <li>Everything happens in the <code>includes-incoming-trace-id-2qszn</code> namespace.</li> <li>The Broker is named <code>br</code>.</li> <li>There are two Triggers that are associated with the Broker:<ul> <li><code>transformer</code> - Filters to only allow events whose type is <code>transformer</code>.   Sends the event to the Kubernetes Service <code>transformer</code>, which will reply with an   identical event, except the replied event's type will be <code>logger</code>.</li> <li><code>logger</code> - Filters to only allow events whose type is <code>logger</code>. Sends the event to   the Kubernetes Service <code>logger</code>.</li> </ul> </li> <li>An event is sent to the Broker with the type <code>transformer</code>, by the Pod named <code>sender</code>.</li> </ul> <p>Given this scenario, the expected path and behavior of an event is as follows:</p> <ol> <li><code>sender</code> Pod sends the request to the Broker.</li> <li>Go to the Broker's ingress Pod.</li> <li>Go to the <code>imc-dispatcher</code> Channel (imc stands for InMemoryChannel).</li> <li>Go to both Triggers.<ol> <li>Go to the Broker's filter Pod for the Trigger <code>logger</code>. The Trigger's filter ignores this event.</li> <li>Go to the Broker's filter Pod for the Trigger <code>transformer</code>. The filter does pass, so it goes to the Kubernetes Service pointed at, also named <code>transformer</code>.<ol> <li><code>transformer</code> Pod replies with the modified event.</li> <li>Go to an InMemory dispatcher.</li> <li>Go to the Broker's ingress Pod.</li> <li>Go to the InMemory dispatcher.</li> <li>Go to both Triggers.<ol> <li>Go to the Broker's filter Pod for the Trigger <code>transformer</code>. The Trigger's filter ignores the event.</li> <li>Go to the Broker's filter Pod for the Trigger <code>logger</code>. The filter passes.<ol> <li>Go to the <code>logger</code> Pod. There is no reply.</li> </ol> </li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section:</p> <p></p> <p>This is the same screenshot without the annotations.</p> <p></p> <p>If you are interested, here is the raw JSON of the trace.</p>"},{"location":"eventing/event-delivery/","title":"Handling Delivery Failure","text":"<p>You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered</p>"},{"location":"eventing/event-delivery/#configuring-subscription-event-delivery","title":"Configuring Subscription event delivery","text":"<p>You can configure how events are delivered for each Subscription by adding a <code>delivery</code> spec to the <code>Subscription</code> object, as shown in the following example:</p> <pre><code>apiVersion: messaging.knative.dev/v1\nkind: Subscription\nmetadata:\n  name: example-subscription\n  namespace: example-namespace\nspec:\n  delivery:\n    deadLetterSink:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: example-sink\n    backoffDelay: &lt;duration&gt;\n    backoffPolicy: &lt;policy-type&gt;\n    retry: &lt;integer&gt;\n</code></pre> <p>Where</p> <ul> <li>The <code>deadLetterSink</code> spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative Service or a URI. In the example, the destination is a <code>Service</code> object, or Knative Service, named <code>example-sink</code>.</li> <li>The <code>backoffDelay</code> delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the <code>backoffDelay</code> parameter is specified using the ISO 8601 format. For example, <code>PT1S</code> specifies a 1 second delay.</li> <li>The <code>backoffPolicy</code> delivery parameter can be used to specify the retry back off policy. The policy can be specified as either <code>linear</code> or <code>exponential</code>. When using the <code>linear</code> back off policy, the back off delay is the time interval specified between retries. When using the <code>exponential</code> back off policy, the back off delay is equal to <code>backoffDelay*2^&lt;numberOfRetries&gt;</code>.</li> <li><code>retry</code> specifies the number of times that event delivery is retried before the event is sent to the dead letter sink.</li> </ul>"},{"location":"eventing/event-delivery/#configuring-broker-event-delivery","title":"Configuring Broker event delivery","text":"<p>You can configure how events are delivered for each Broker by adding a <code>delivery</code> spec, as shown in the following example:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: with-dead-letter-sink\nspec:\n  delivery:\n    deadLetterSink:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: example-sink\n    backoffDelay: &lt;duration&gt;\n    backoffPolicy: &lt;policy-type&gt;\n    retry: &lt;integer&gt;\n</code></pre> <p>Where</p> <ul> <li>The <code>deadLetterSink</code> spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. In the example, the destination is a <code>Service</code> object, or Knative Service, named <code>example-sink</code>.</li> <li>The <code>backoffDelay</code> delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the <code>backoffDelay</code> parameter is specified using the ISO 8601 format. For example, <code>PT1S</code> specifies a 1 second delay.</li> <li>The <code>backoffPolicy</code> delivery parameter can be used to specify the retry back off policy. The policy can be specified as either <code>linear</code> or <code>exponential</code>. When using the <code>linear</code> back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the <code>exponential</code> back off policy, the back off delay increases by a multiplier of the given interval for each retry.</li> <li><code>retry</code> specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the <code>retry</code> value +1.</li> </ul>"},{"location":"eventing/event-delivery/#broker-support","title":"Broker support","text":"<p>The following table summarizes which delivery parameters are supported for each Broker implementation type:</p> Broker Class Supported Delivery Parameters googlecloud <code>deadLetterSink</code>, <code>retry</code>, <code>backoffPolicy</code>, <code>backoffDelay</code> Kafka <code>deadLetterSink</code>, <code>retry</code>, <code>backoffPolicy</code>, <code>backoffDelay</code> MTChannelBasedBroker depends on the underlying Channel RabbitMQBroker <code>deadLetterSink</code>, <code>retry</code>, <code>backoffPolicy</code>, <code>backoffDelay</code> <p>Note</p> <p><code>deadLetterSink</code> must be a GCP Pub/Sub topic URI. <code>googlecloud</code> Broker only supports the <code>exponential</code> back off policy.</p>"},{"location":"eventing/event-delivery/#configuring-channel-event-delivery","title":"Configuring Channel event delivery","text":"<p>Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the<code>deadLetterSink</code>. These extension attributes are as follows:</p> <ul> <li> <p>knativeerrordest</p> <ul> <li>Type: String</li> <li>Description: The original destination URL to which the failed event   was sent.  This could be either a <code>delivery</code> or <code>reply</code> URL based on   which operation encountered the failed event.</li> <li>Constraints: Always present because every HTTP Request has a   destination URL.</li> <li>Examples:<ul> <li>\"http://myservice.mynamespace.svc.cluster.local:3000/mypath\"</li> <li>...any <code>deadLetterSink</code> URL...</li> </ul> </li> </ul> </li> <li> <p>knativeerrorcode</p> <ul> <li>Type: Int</li> <li>Description: The HTTP Response StatusCode from the final event   dispatch attempt.</li> <li>Constraints: Always present because every HTTP Response contains   a StatusCode.</li> <li>Examples:<ul> <li>\"500\"</li> <li>...any HTTP StatusCode...</li> </ul> </li> </ul> </li> <li> <p>knativeerrordata</p> <ul> <li>Type: String</li> <li>Description: The HTTP Response Body from the final event dispatch   attempt.</li> <li>Constraints: Empty if the HTTP Response Body is empty,   and may be truncated if the length is excessive.</li> <li>Examples:<ul> <li>'Internal Server Error: Failed to process event.'</li> <li>'{\"key\": \"value\"}'</li> <li>...any HTTP Response Body...</li> </ul> </li> </ul> </li> </ul>"},{"location":"eventing/event-delivery/#channel-support","title":"Channel support","text":"<p>The following table summarizes which delivery parameters are supported for each Channel implementation.</p> Channel Type Supported Delivery Parameters GCP PubSub none In-Memory <code>deadLetterSink</code>, <code>retry</code>, <code>backoffPolicy</code>, <code>backoffDelay</code> Kafka <code>deadLetterSink</code>, <code>retry</code>, <code>backoffPolicy</code>, <code>backoffDelay</code> Natss none"},{"location":"eventing/event-mesh/","title":"Event Mesh","text":"<p>An Event Mesh is dynamic infrastructure which is designed to simplify distributing events from senders to recipients.  Similar to traditional message-channel architectures like Apache Kafka or RabbitMQ, an Event Mesh provides asynchronous (store-and-forward) delivery of messages which allows decoupling senders and recipients in time.  Unlike traditional message-channel based integration patterns, Event Meshes also simplify the routing concerns of senders and recipients by decoupling them from the underlying event transport infrastructure (which may be a federated set of solutions like Kafka, RabbitMQ, or cloud provider infrastructure).  The mesh transports events from producers to consumers via a network of interconnected event brokers across any environment, and even between clouds in a seamless and loosely coupled way.</p> <p>In an Event Mesh, both producing and consuming applications do not need to implement event routing or subscription management.  Event producers can publish all events to the mesh, which can route events to interested subscribers without needing the application to subdivide events to channels.  Event consumers can use mesh configuration to receive events of interest using fine-grained filter expressions rather than needing to implement multiple subscriptions and application-level event filtering to select the events of interest.  Event serialization and de-serialization can be handled by language-native libraries without needing to implement heavier-weight routing and filtering.</p>"},{"location":"eventing/event-mesh/#knative-event-mesh","title":"Knative Event Mesh","text":"<p>The above mentioned event brokers map directly to a core API in Knative Eventing: the <code>Broker</code> API offers a discoverable endpoint for event ingress and the <code>Trigger</code> API completes the offering with its event filtering and delivery capabilities.  With these APIs Knative Eventing offers an Event Mesh as defined above:</p> <p></p> <p>As visible in the above diagram, the Event Mesh is defined with the <code>Broker</code> and <code>Trigger</code> APIs for the ingress and the egress of events.  Knative Eventing enables multiple resources to participate in the Event Mesh with a partial schema pattern called \"duck typing\".  Duck typing allows multiple resource types to advertise common capabilities, such as \"can receive events at a URL\" or \"can deliver events to a destination\".  Knative Eventing uses these capabilities to offer a pool of interoperable sources for sending events to the <code>Broker</code> and as destinations for <code>Trigger</code>-routed events.  The Knative Eventing APIs contain three categories of APIs:</p> <ul> <li>Events Ingress: Support for connecting event senders: Source duck type and SinkBinding to support easily configuring applications to deliver events to a <code>Broker</code>.  Applications can submit events and use Eventing even without any sources installed.</li> <li>Event routing: <code>Broker</code> and <code>Trigger</code> objects support defining the mesh and event routing.  Note that <code>Broker</code> matches the definition of an Addressable event destination, so it is possible to relay events from a Broker in one cluster to a Broker in another cluster.  Similarly, <code>Trigger</code> uses the same Deliverable duck type as many sources, so it is easy to substitute an event mesh for direct delivery of events.</li> <li>Event egress : The Deliverable contract supports specifying either a bare URL or referencing a Kubernetes object which implements the Addressable interface (has a <code>status.address.url</code>) as a destination.  All event destinations (\"sinks\") must implement the CloudEvents delivery specification, but do not necessarily need to implement any Kubernetes behavior -- a bare VM referenced by URL is an acceptable event egress.</li> </ul> <p>It is important to note that event sources and sinks are supporting components of the eventing ecosystem but are not directly part of the Event Mesh.  While not part of the Event Mesh, these ecosystem components complement the mesh and benefit from the duck type APIs (<code>Callable</code>/<code>Addressable</code>) for a smooth integration or connection with the \"Event Mesh\".</p>"},{"location":"eventing/event-registry/","title":"Event registry","text":"<p>Knative Eventing defines an <code>EventType</code> object to make it easier for consumers to discover the types of events they can consume from Brokers or Channels.</p> <p>The event registry maintains a catalog of event types that each Broker or Channel can consume. The event types stored in the registry contain all required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism.</p> <p>This topic provides information about how you can populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest.</p> <p>Note</p> <p>Before using the event registry, it is recommended that you have a basic understanding of Brokers, Triggers, Event Sources, and the CloudEvents spec (particularly the Context Attributes section).</p>"},{"location":"eventing/event-registry/#about-eventtype-objects","title":"About EventType objects","text":"<p>EventType objects represent a type of event that can be consumed from a Broker or Channel, such as Apache Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore.</p> <p>The following is an example EventType YAML that omits irrelevant fields:</p> <pre><code>apiVersion: eventing.knative.dev/v1beta2\nkind: EventType\nmetadata:\n  name: dev.knative.source.github.push-34cnb\n  namespace: default\n  labels:\n    eventing.knative.dev/sourceName: github-sample\nspec:\n  type: dev.knative.source.github.push\n  source: https://github.com/knative/eventing\n  schema:\n  description:\n  reference:\n    apiVersion: eventing.knative.dev/v1\n    kind: Broker\n    name: default\nstatus:\n  conditions:\n    - status: \"True\"\n      type: ReferenceExists\n    - status: \"True\"\n      type: Ready\n</code></pre> <p>For the full specification for an EventType object, see the EventType API reference.</p> <p>The <code>metadata.name</code> field is advisory, that is, non-authoritative. It is typically generated using <code>generateName</code> to avoid naming collisions. <code>metadata.name</code> is not needed when you create Triggers.</p> <p>For consumers, the fields that matter the most are <code>spec</code> and <code>status</code>. This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Reference is present to receive events.</p> <p>The following table has more information about the <code>spec</code> and <code>status</code> fields of EventType objects:</p> Field Description Required or optional <code>spec.type</code> Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required <code>spec.source</code> Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required <code>spec.schema</code> A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional <code>spec.description</code> A string describing what the EventType is about. Optional <code>spec.reference</code> Refers to the KResource that can provide the EventType. Required <code>status</code> Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the KReference being present. Optional"},{"location":"eventing/event-registry/#populate-the-registry-with-events","title":"Populate the registry with events","text":"<p>You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources.</p>"},{"location":"eventing/event-registry/#manual-registration","title":"Manual registration","text":"<p>For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource.</p> <p>To apply EventTypes YAML files manually:</p> <ol> <li> <p>Create an EventType YAML file. For information about the required fields, see About EventType objects.</p> </li> <li> <p>Apply the YAML by running the command:</p> <pre><code>kubectl apply -f &lt;event-type.yaml&gt;\n</code></pre> </li> </ol>"},{"location":"eventing/event-registry/#automatic-registration","title":"Automatic registration","text":"<p>Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated.</p>"},{"location":"eventing/event-registry/#support-for-automatic-registration","title":"Support for automatic registration","text":"<p>Knative supports automatic registration of EventTypes for the following event sources:</p> <ul> <li>CronJobSource</li> <li>ApiServerSource</li> <li>GithubSource</li> <li>GcpPubSubSource</li> <li>KafkaSource</li> <li>AwsSqsSource</li> </ul> <p>Knative supports automatic creation of EventTypes for sources, that are compliant to the Sources Duck type.</p>"},{"location":"eventing/event-registry/#procedure-for-automatic-registration","title":"Procedure for automatic registration","text":"<ul> <li> <p>To register EventTypes automatically, apply your event source YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;event-source.yaml&gt;\n</code></pre> </li> </ul> <p>After your event source is instantiated, EventTypes are added to the registry.</p>"},{"location":"eventing/event-registry/#example-automatic-registration-using-kafkasource","title":"Example: Automatic registration using KafkaSource","text":"<p>Given the following KafkaSource sample to populate the registry:</p> <pre><code>apiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\n  name: kafka-sample\n  namespace: default\nspec:\n  bootstrapServers:\n   - my-cluster-kafka-bootstrap.kafka:9092\n  topics:\n   - knative-demo\n   - news\n  sink:\n    apiVersion: eventing.knative.dev/v1\n    kind: Broker\n    name: default\n</code></pre> <p>The <code>topics</code> field in the above example is used to generate the EventType <code>source</code> field.</p> <p>After running <code>kubectl apply</code> using the above YAML, the KafkaSource <code>kafka-source-sample</code> is instantiated, and two EventTypes are added to the registry because there are two topics.</p>"},{"location":"eventing/event-registry/#discover-events-using-the-registry","title":"Discover events using the registry","text":"<p>Using the registry, you can discover the different types of events that Broker event meshes can consume.</p> <p>Note</p> <p>With the feature for EventType auto creation you can see more event types in the registry. Learn here how to enable this feature.</p>"},{"location":"eventing/event-registry/#view-all-event-types-you-can-subscribe-to","title":"View all event types you can subscribe to","text":"<ul> <li> <p>To see a list of event types in the registry that are available to subscribe to, run the command:</p> <pre><code>kubectl get eventtypes -n &lt;namespace&gt;\n</code></pre> <p>Example output using the <code>default</code> namespace in a testing cluster:</p> <pre><code>NAME                                         TYPE                                    SOURCE                                                               SCHEMA        BROKER     DESCRIPTION     READY     REASON\ndev.knative.source.github.push-34cnb         dev.knative.source.github.push          https://github.com/knative/eventing                                                default                    True\ndev.knative.source.github.push-44svn         dev.knative.source.github.push          https://github.com/knative/serving                                                 default                    True\ndev.knative.source.github.pullrequest-86jhv  dev.knative.source.github.pull_request  https://github.com/knative/eventing                                                default                    True\ndev.knative.source.github.pullrequest-97shf  dev.knative.source.github.pull_request  https://github.com/knative/serving                                                 default                    True\ndev.knative.kafka.event-cjvcr                dev.knative.kafka.event                 /apis/v1/namespaces/default/kafkasources/kafka-sample#news                         default                    True\ndev.knative.kafka.event-tdt48                dev.knative.kafka.event                 /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo                 default                    True\ngoogle.pubsub.topic.publish-hrxhh            google.pubsub.topic.publish             //pubsub.googleapis.com/knative/topics/testing                                     dev                        False     BrokerIsNotReady\n</code></pre> <p>This example output shows seven different EventType objects in the registry of the <code>default</code> namespace. It assumes that the event sources emitting the events reference a Broker as their sink.</p> </li> </ul>"},{"location":"eventing/event-registry/#view-the-yaml-for-an-eventtype-object","title":"View the YAML for an EventType object","text":"<ul> <li> <p>To see the YAML for an EventType object, run the command:</p> <p><pre><code>kubectl get eventtype &lt;name&gt; -o yaml\n</code></pre> Where <code>&lt;name&gt;</code> is the name of an EventType object and can be found in the <code>NAME</code> column of the registry output. For example, <code>dev.knative.source.github.push-34cnb</code>.</p> </li> </ul> <p>For an example EventType YAML, see About EventType objects earlier on this page.</p>"},{"location":"eventing/event-registry/#about-subscribing-to-events","title":"About subscribing to events","text":"<p>After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events.</p> <p>Here are a some example Triggers that subscribe to events using exact matching on <code>type</code> or <code>source</code>, based on the registry output mentioned earlier:</p> <ul> <li> <p>Subscribes to GitHub pushes from any source:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n name: push-trigger\n namespace: default\nspec:\n broker: default\n filter:\n   attributes:\n     type: dev.knative.source.github.push\n subscriber:\n   ref:\n     apiVersion: serving.knative.dev/v1\n     kind: Service\n     name: push-service\n</code></pre> <p>Note</p> <p>As the example registry output mentioned, only two sources, the <code>knative/eventing</code> and <code>knative/serving</code> GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them.</p> </li> <li> <p>Subscribes to GitHub pull requests from the <code>knative/eventing</code> GitHub repository:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n name: gh-knative-eventing-pull-trigger\n namespace: default\nspec:\n broker: default\n filter:\n   attributes:\n     type: dev.knative.source.github.pull_request\n     source: https://github.com/knative/eventing\n subscriber:\n   ref:\n     apiVersion: serving.knative.dev/v1\n     kind: Service\n     name: gh-knative-eventing-pull-service\n</code></pre> </li> <li> <p>Subscribes to Kafka messages sent to the knative-demo topic:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n name: kafka-knative-demo-trigger\n namespace: default\nspec:\n broker: default\n filter:\n   attributes:\n     type: dev.knative.kafka.event\n     source: /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo\n subscriber:\n   ref:\n     apiVersion: serving.knative.dev/v1\n     kind: Service\n     name: kafka-knative-demo-service\n</code></pre> </li> <li> <p>Subscribes to PubSub messages from GCP's knative project sent to the testing topic:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n name: gcp-pubsub-knative-testing-trigger\n namespace: default\nspec:\n broker: dev\n filter:\n   attributes:\n     source: //pubsub.googleapis.com/knative/topics/testing\n subscriber:\n   ref:\n     apiVersion: serving.knative.dev/v1\n     kind: Service\n     name: gcp-pubsub-knative-testing-service\n</code></pre> <p>Note</p> <p>The example registry output mentioned earlier lists this Broker's readiness as <code>false</code>. This Trigger's subscriber cannot consume events until the Broker becomes ready.</p> </li> </ul>"},{"location":"eventing/event-registry/#next-steps","title":"Next steps","text":"<p>Knative code samples is a useful resource to better understand some of the event sources. Remember, you must point the sources to a Broker if you want automatic registration of EventTypes in the registry.</p>"},{"location":"eventing/brokers/","title":"About Brokers","text":"<p>Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of events. Brokers provide a discoverable endpoint for event ingress, and use Triggers for event delivery. Event producers can send events to a broker by POSTing the event.</p> <p></p>"},{"location":"eventing/brokers/#event-delivery","title":"Event delivery","text":"<p>Event delivery mechanics are an implementation detail that depend on the configured Broker class. Using Brokers and Triggers abstracts the details of event routing from the event producer and event consumer.</p>"},{"location":"eventing/brokers/#advanced-use-cases","title":"Advanced use cases","text":"<p>For most use cases, a single Broker per namespace is sufficient, but there are several use cases where multiple Brokers can simplify architecture. For example, separate Brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules.</p>"},{"location":"eventing/brokers/#next-steps","title":"Next steps","text":"<ul> <li>Create a Broker.</li> <li>Configure default Broker ConfigMap settings.</li> </ul>"},{"location":"eventing/brokers/#additional-resources","title":"Additional resources","text":"<ul> <li>Brokers concept documentation</li> <li>Broker specifications</li> </ul>"},{"location":"eventing/brokers/broker-developer-config-options/","title":"Developer configuration options","text":""},{"location":"eventing/brokers/broker-developer-config-options/#broker-configuration-example","title":"Broker configuration example","text":"<p>The following is a full example of a Channel based Broker object which shows the possible configuration options that you can modify:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: default\n  namespace: default\n  annotations:\n    eventing.knative.dev/broker.class: MTChannelBasedBroker\nspec:\n  config:\n    apiVersion: v1\n    kind: ConfigMap\n    name: config-br-default-channel\n    namespace: knative-eventing\n  delivery:\n    deadLetterSink:\n      ref:\n        kind: Service\n        namespace: example-namespace\n        name: example-service\n        apiVersion: v1\n      uri: example-uri\n    retry: 5\n    backoffPolicy: exponential\n    backoffDelay: \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\"\n</code></pre> <ul> <li>You can specify any valid <code>name</code> for your broker. Using <code>default</code> will create a broker named <code>default</code>.</li> <li>The <code>namespace</code> must be an existing namespace in your cluster. Using <code>default</code> will create the broker in the <code>default</code> namespace.</li> <li>You can set the <code>eventing.knative.dev/broker.class</code> annotation to change the class of the broker. The default broker class is <code>MTChannelBasedBroker</code>, but Knative also supports use of the <code>Kafka</code> and <code>RabbitMQBroker</code> broker class. For more information see the Apache Kafka Broker or RabbitMQ Broker documentation.</li> <li><code>spec.config</code> is used to specify the default backing channel configuration for Channel based Broker implementations. For more information on configuring the default channel type, see the documentation on Configure Broker defaults.</li> <li><code>spec.delivery</code> is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on Event delivery.</li> </ul>"},{"location":"eventing/brokers/create-broker/","title":"Creating a Broker","text":"<p>Once you have installed Knative Eventing and a Broker implementation, you can create an instance of a Broker.</p> <p>Note</p> <p>Knative Eventing provides by default the MTChannelBasedBroker. Its default backing channel is the <code>InMemoryChannel</code>. <code>InMemoryChannel</code> should not be used in production. Other Broker types and their configuration options can be found under Available Broker types.</p> <p>You can create a Broker by using the <code>kn</code> CLI or by applying YAML files using <code>kubectl</code>.</p> knkubectl <ol> <li> <p>You can create a Broker by entering the following command:</p> <pre><code>kn broker create &lt;broker-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>This will create a new Broker of your default Broker class and default Broker configuration (both defined in the <code>config-br-defaults</code> ConfigMap).</p> <p>Note</p> <p>If you choose not to specify a namespace, the Broker will be created in the current namespace.</p> <p>Note</p> <p>If you have multiple Broker classes installed in your cluster, you can specify the Broker class via the <code>--class</code> parameter, e.g.:</p> <pre><code>kn broker create &lt;broker-name&gt; -n &lt;namespace&gt; --class MTChannelBasedBroker\n</code></pre> </li> <li> <p>Optional: Verify that the Broker was created by listing existing Brokers:</p> <pre><code>kn broker list\n</code></pre> </li> <li> <p>Optional: You can also verify the Broker exists by describing the Broker you have created:</p> <pre><code>kn broker describe &lt;broker-name&gt;\n</code></pre> </li> </ol> <p>The YAML in the following example creates a Broker named <code>default</code>.</p> <ol> <li> <p>Create a Broker by creating a YAML file using the following template:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: &lt;broker-name&gt;\n  namespace: &lt;namespace&gt;\n</code></pre> <p>This creates a new Broker using the default Broker class and default Broker configuration, both of which are defined in the <code>config-br-defaults</code> ConfigMap.</p> </li> <li> <p>Apply the YAML file:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> <li> <p>Optional: Verify that the Broker is working correctly:</p> <pre><code>kubectl -n &lt;namespace&gt; get broker &lt;broker-name&gt;\n</code></pre> <p>This shows information about your Broker. If the Broker is working correctly, it shows a <code>READY</code> status of <code>True</code>:</p> <pre><code>NAME      READY   REASON   URL                                                                        AGE\ndefault   True             http://broker-ingress.knative-eventing.svc.cluster.local/default/default   1m\n</code></pre> <p>If the <code>READY</code> status is <code>False</code>, wait a few moments and then run the command again.</p> </li> </ol>"},{"location":"eventing/brokers/create-broker/#broker-class-options","title":"Broker class options","text":"<p>When a Broker is created without a specified <code>eventing.knative.dev/broker.class</code> annotation, the default <code>MTChannelBasedBroker</code> Broker class is used, as specified by default in the <code>config-br-defaults</code> ConfigMap. </p> <p>In case you have multiple Broker classes installed in your cluster and want to use a non-default Broker class for a Broker, you can modify the <code>eventing.knative.dev/broker.class</code> annotation and <code>spec.config</code> for the Broker object.</p> <ol> <li> <p>Set the <code>eventing.knative.dev/broker.class</code> annotation. Replace <code>MTChannelBasedBroker</code> in the following example with the class type you want to use. Be aware that the Broker class annoation is immutable and thus can't be updated after the Broker got created:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: MTChannelBasedBroker\n  name: default\n  namespace: default\n</code></pre> </li> <li> <p>Configure the <code>spec.config</code> with the details of the ConfigMap that defines the required configuration for the Broker class (e.g. with some Channel configurations in case of the <code>MTChannelBasedBroker</code>):</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: MTChannelBasedBroker\n  name: default\n  namespace: default\nspec:\n  config:\n    apiVersion: v1\n    kind: ConfigMap\n    name: config-br-default-channel\n    namespace: knative-eventing\n</code></pre> </li> </ol> <p>For further information about configuring a default Broker class cluster wide or on a per namespace basis, check the Administrator configuration options.</p>"},{"location":"eventing/brokers/broker-types/","title":"Available Broker types","text":"<p>The following broker types are available for use with Knative Eventing.</p>"},{"location":"eventing/brokers/broker-types/#channel-based-broker","title":"Channel-based broker","text":"<p>Knative Eventing provides a Channel based Broker implementation that uses Channels for event routing.</p> <p>Before you can use the Channel based Broker, you must install a Channel implementation.</p>"},{"location":"eventing/brokers/broker-types/#alternative-broker-implementations","title":"Alternative broker implementations","text":"<p>In the Knative Eventing ecosystem, alternative Broker implementations are welcome as long as they respect the Broker specifications.</p> <p>The following is a list of Brokers provided by the community or vendors:</p>"},{"location":"eventing/brokers/broker-types/#knative-broker-for-apache-kafka","title":"Knative Broker for Apache Kafka","text":"<p>This Broker implementation uses Apache Kafka as its backing technology. For more information, see the Knative Broker for Apache Kafka documentation.</p>"},{"location":"eventing/brokers/broker-types/#rabbitmq-broker","title":"RabbitMQ broker","text":"<p>The RabbitMQ Broker uses RabbitMQ for its underlying implementation. For more information, see RabbitMQ Broker or the docs available on GitHub.</p>"},{"location":"eventing/brokers/broker-types/channel-based-broker/","title":"Channel based Broker","text":"<p>The Channel based Broker (<code>MTChannelBasedBroker</code>) uses Channels for event routing. It is shipped by default with Knative Eventing. Users should prefer native Broker implementations (like Knative Broker for Apache Kafka or RabbitMQ Broker) over the MTChannelBasedBroker and Channel combination because it is usually more efficient as they reduce network hops for example. </p>"},{"location":"eventing/brokers/broker-types/channel-based-broker/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have Knative Eventing installed.</li> <li>You have a Channel implementation installed.</li> </ul> <p>As the MTChannelBasedBroker is based on Channels, you need to install a Channel implementation. Check out the available Channels for a (non-exhaustive) list of the available Channels for Knative Eventing.</p> <p>You can install for example the InMemory Channel by running the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml\n</code></pre>"},{"location":"eventing/brokers/broker-types/channel-based-broker/#create-a-mtchannelbasedbroker","title":"Create a MTChannelBasedBroker","text":"<p>You can create a MTChannelBasedBroker by using the <code>kn</code> CLI or by applying YAML files using <code>kubectl</code>.</p> knkubectl <p>You can create a MTChannelBasedBroker by entering the following command:</p> <p><pre><code>kn broker create &lt;broker-name&gt; --class MTChannelBasedBroker\n</code></pre> Where <code>&lt;broker-name&gt;</code> is the name of your Broker.</p> <p>The YAML in the following example creates a Broker.</p> <ol> <li> <p>Create a MTChannelBasedBroker by creating a YAML file using the following template:</p> <p><pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: MTChannelBasedBroker\n  name: &lt;broker-name&gt;\n</code></pre> Where <code>&lt;broker-name&gt;</code> is the name of your Broker.</p> <p>Note</p> <p>Note, that the Broker class is specified via the <code>eventing.knative.dev/broker.class</code> annotation.</p> </li> <li> <p>Apply the YAML file:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/brokers/broker-types/channel-based-broker/#configuration","title":"Configuration","text":"<p>You configure the Broker object itself, or you can define cluster or namespace default values.</p>"},{"location":"eventing/brokers/broker-types/channel-based-broker/#broker-specific-configuration","title":"Broker specific configuration","text":"<p>It is possible to configure each Broker individually by referencing a ConfigMap in the <code>spec.config</code>:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: MTChannelBasedBroker\n  name: default\nspec:\n  # Configuration specific to this broker.\n  config:\n    apiVersion: v1\n    kind: ConfigMap\n    name: my-broker-specific-configuration\n    namespace: default\n</code></pre> <p>The referenced ConfigMap must contain a <code>channel-template-spec</code> that defines the underlining Channel implementation for this Broker, as well as some Channel specific configurations. For example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-broker-specific-configuration\n  namespace: default\ndata:\n  channel-template-spec: |\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n</code></pre> <p>Kafka Channel configuration example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-channel\n  namespace: knative-eventing\ndata:\n  channel-template-spec: |\n    apiVersion: messaging.knative.dev/v1beta1\n    kind: KafkaChannel\n    spec:\n      numPartitions: 3\n      replicationFactor: 1\n</code></pre>"},{"location":"eventing/brokers/broker-types/channel-based-broker/#broker-default-configuration","title":"Broker default configuration","text":"<p>The <code>config-br-defaults</code> ConfigMap defines default values for any Broker that does not specify a <code>spec.config</code> or a Broker class. It is possible to define these defaults cluser wide or on a per namespace basis. Check the Administrator configuration options on how to set Broker defaults cluster wide or on a namespace basis.</p>"},{"location":"eventing/brokers/broker-types/channel-based-broker/#developer-documentation","title":"Developer documentation","text":"<p>For more information about <code>MTChannelBasedBroker</code>, see the MTChannelBasedBroker developer documentation.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/","title":"Knative Broker for Apache Kafka","text":"<p>The Knative Broker for Apache Kafka is an implementation of the Knative Broker API natively targeting Apache Kafka to reduce network hops and offering a better integration with Apache Kafka for the Broker and Trigger API model.</p> <p>Notable features are:</p> <ul> <li>Control plane High Availability</li> <li>Horizontally scalable data plane</li> <li>Extensively configurable</li> <li>Ordered delivery of events based on CloudEvents partitioning extension</li> <li>Support any Kafka version, see compatibility matrix</li> <li>Supports 2 data plane modes: data plane isolation per-namespace or shared data plane</li> </ul> <p>The Knative Kafka Broker stores incoming CloudEvents as Kafka records, using the binary content mode, because it is more efficient due to its optimizations for transport or routing, as well avoid JSON parsing. Using <code>binary content mode</code> means all CloudEvent attributes and extensions are mapped as headers on the Kafka record, while the <code>data</code> of the CloudEvent corresponds to the actual value of the Kafka record. This is another benefit of using <code>binary content mode</code> over <code>structured content mode</code> as it is less obstructive and therefore compatible with systems that do not understand CloudEvents.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have installed Knative Eventing.</li> <li>You have access to an Apache Kafka cluster.</li> </ol> <p>Tip</p> <p>If you need to set up a Kafka cluster, you can do this by following the instructions on the Strimzi Quickstart page.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#installation","title":"Installation","text":"<ol> <li> <p>Install the Kafka controller by entering the following command:</p> <pre><code>kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> <li> <p>Install the Kafka Broker data plane by entering the following command:</p> <pre><code>kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml\n</code></pre> </li> <li> <p>Verify that <code>kafka-controller</code>, <code>kafka-broker-receiver</code> and <code>kafka-broker-dispatcher</code> are running, by entering the following command:</p> <pre><code>kubectl get deployments.apps -n knative-eventing\n</code></pre> <p>Example output:</p> <pre><code>NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\neventing-controller            1/1     1            1           10s\neventing-webhook               1/1     1            1           9s\nkafka-controller               1/1     1            1           3s\nkafka-broker-dispatcher        1/1     1            1           4s\nkafka-broker-receiver          1/1     1            1           5s\n</code></pre> </li> </ol>"},{"location":"eventing/brokers/broker-types/kafka-broker/#create-a-kafka-broker","title":"Create a Kafka Broker","text":"<p>A Kafka Broker object looks like this:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    # case-sensitive\n    eventing.knative.dev/broker.class: Kafka\n    # Optional annotation to point to an externally managed kafka topic:\n    # kafka.eventing.knative.dev/external.topic: &lt;topic-name&gt;\n  name: default\n  namespace: default\nspec:\n  # Configuration specific to this broker.\n  config:\n    apiVersion: v1\n    kind: ConfigMap\n    name: kafka-broker-config\n    namespace: knative-eventing\n  # Optional dead letter sink, you can specify either:\n  #  - deadLetterSink.ref, which is a reference to a Callable\n  #  - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster)\n  delivery:\n    deadLetterSink:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: dlq-service\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/#configure-a-kafka-broker","title":"Configure a Kafka Broker","text":"<p>The <code>spec.config</code> should reference any <code>ConfigMap</code> in any <code>namespace</code> that looks like the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-broker-config\n  namespace: knative-eventing\ndata:\n  # Number of topic partitions\n  default.topic.partitions: \"10\"\n  # Replication factor of topic messages.\n  default.topic.replication.factor: \"3\"\n  # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster)\n  bootstrap.servers: \"my-cluster-kafka-bootstrap.kafka:9092\"\n</code></pre> <p>This <code>ConfigMap</code> is installed in the Knative Eventing <code>SYSTEM_NAMESPACE</code> in the cluster. You can edit the global configuration depending on your needs. You can also override these settings on a per broker base, by referencing a different <code>ConfigMap</code> on a different <code>namespace</code> or with a different <code>name</code> on your Kafka Broker's <code>spec.config</code> field.</p> <p>Note</p> <p>The <code>default.topic.replication.factor</code> value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the <code>default.topic.replication.factor</code> value should not be more than <code>1</code>.</p> <p>Knative supports the full set of topic config options that your version of Kafka supports. To set any of these, you need to add a key to the configmap with the <code>default.topic.config.</code> prefix. For example, to set the <code>retention.ms</code> value you would modify the <code>ConfigMap</code> to look like the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-broker-config\n  namespace: knative-eventing\ndata:\n  # Number of topic partitions\n  default.topic.partitions: \"10\"\n  # Replication factor of topic messages.\n  default.topic.replication.factor: \"3\"\n  # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster)\n  bootstrap.servers: \"my-cluster-kafka-bootstrap.kafka:9092\"\n  # Here is our retention.ms config\n  default.topic.config.retention.ms: \"3600\"\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/#set-as-default-broker-implementation","title":"Set as default broker implementation","text":"<p>To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the <code>config-br-defaults</code> ConfigMap in the <code>knative-eventing</code> namespace.</p> <p>This allows you to avoid configuring individual or per-namespace settings for each broker, such as <code>metadata.annotations.eventing.knative.dev/broker.class</code> or <code>spec.config</code>.</p> <p>The following YAML is an example of a <code>config-br-defaults</code> ConfigMap using Kafka broker as the default implementation.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  default-br-config: |\n    clusterDefault:\n      brokerClass: Kafka\n      apiVersion: v1\n      kind: ConfigMap\n      name: kafka-broker-config\n      namespace: knative-eventing\n    namespaceDefaults:\n      namespace1:\n        brokerClass: Kafka\n        apiVersion: v1\n        kind: ConfigMap\n        name: kafka-broker-config\n        namespace: knative-eventing\n      namespace2:\n        brokerClass: Kafka\n        apiVersion: v1\n        kind: ConfigMap\n        name: kafka-broker-config\n        namespace: knative-eventing\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/#security","title":"Security","text":"<p>Apache Kafka supports different security features, Knative supports the followings:</p> <ul> <li>Authentication using <code>SASL</code> without encryption</li> <li>Authentication using <code>SASL</code> and encryption using <code>SSL</code></li> <li>Authentication and encryption using <code>SSL</code></li> <li>Encryption using <code>SSL</code> without client authentication</li> </ul> <p>To enable security features, in the <code>ConfigMap</code> referenced by <code>broker.spec.config</code>, we can reference a <code>Secret</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n   name: kafka-broker-config\n   namespace: knative-eventing\ndata:\n   # Other configurations\n   # ...\n\n   # Reference a Secret called my_secret\n   auth.secret.ref.name: my_secret\n</code></pre> <p>The <code>Secret</code> <code>my_secret</code> must exist in the same namespace of the <code>ConfigMap</code> referenced by <code>broker.spec.config</code>, in this case: <code>knative-eventing</code>.</p> <p>Note</p> <p>Certificates and keys must be in <code>PEM</code> format.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl","title":"Authentication using SASL","text":"<p>Knative supports the following SASL mechanisms:</p> <ul> <li><code>PLAIN</code></li> <li><code>SCRAM-SHA-256</code></li> <li><code>SCRAM-SHA-512</code></li> </ul> <p>To use a specific SASL mechanism replace <code>&lt;sasl_mechanism&gt;</code> with the mechanism of your choice.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl-without-encryption","title":"Authentication using SASL without encryption","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SASL_PLAINTEXT \\\n  --from-literal=sasl.mechanism=&lt;sasl_mechanism&gt; \\\n  --from-literal=user=&lt;my_user&gt; \\\n  --from-literal=password=&lt;my_password&gt;\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-using-sasl-and-encryption-using-ssl","title":"Authentication using SASL and encryption using SSL","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SASL_SSL \\\n  --from-literal=sasl.mechanism=&lt;sasl_mechanism&gt; \\\n  --from-file=ca.crt=caroot.pem \\\n  --from-literal=user=&lt;my_user&gt; \\\n  --from-literal=password=&lt;my_password&gt;\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/#encryption-using-ssl-without-client-authentication","title":"Encryption using SSL without client authentication","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SSL \\\n  --from-file=ca.crt=&lt;my_caroot.pem_file_path&gt; \\\n  --from-literal=user.skip=true\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/#authentication-and-encryption-using-ssl","title":"Authentication and encryption using SSL","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SSL \\\n  --from-file=ca.crt=&lt;my_caroot.pem_file_path&gt; \\\n  --from-file=user.crt=&lt;my_cert.pem_file_path&gt; \\\n  --from-file=user.key=&lt;my_key.pem_file_path&gt;\n</code></pre> <p>Note</p> <p><code>ca.crt</code> can be omitted to fallback to use system's root CA set.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#bring-your-own-topic","title":"Bring your own topic","text":"<p>By default the Knative Kafka Broker creates its own internal topic, however it is possible to point to an externally managed topic, using the <code>kafka.eventing.knative.dev/external.topic</code> annotation:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    # case-sensitive\n    eventing.knative.dev/broker.class: Kafka\n    kafka.eventing.knative.dev/external.topic: &lt;my-topic-name&gt;\n  name: default\n  namespace: default\nspec:\n  # other spec fields ...\n</code></pre> <p>Note</p> <p>When using an external topic, the Knative Kafka Broker does not own the topic and is not responsible for managing the topic. This includes the topic lifecycle or its general validity. Other restrictions for general access to the topic may apply. See the documentation about using Access Control Lists (ACLs).</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#consumer-offsets-commit-interval","title":"Consumer Offsets Commit Interval","text":"<p>Kafka consumers keep track of the last successfully sent events by committing offsets.</p> <p>Knative Kafka Broker commits the offset every <code>auto.commit.interval.ms</code> milliseconds.</p> <p>Note</p> <p>To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber.</p> <p>The interval can be changed by changing the <code>config-kafka-broker-data-plane</code> <code>ConfigMap</code> in the <code>knative-eventing</code> namespace by modifying the parameter <code>auto.commit.interval.ms</code> as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-kafka-broker-data-plane\n  namespace: knative-eventing\ndata:\n  # Some configurations omitted ...\n  config-kafka-broker-consumer.properties: |\n    # Some configurations omitted ...\n\n    # Commit the offset every 5000 millisecods (5 seconds)\n    auto.commit.interval.ms=5000\n</code></pre> <p>Note</p> <p>Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#kafka-producer-and-consumer-configurations","title":"Kafka Producer and Consumer configurations","text":"<p>Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads.</p> <p>You can change these configurations by modifying the <code>config-kafka-broker-data-plane</code> <code>ConfigMap</code> in the <code>knative-eventing</code> namespace.</p> <p>Documentation for the settings available in this <code>ConfigMap</code> is available on the Apache Kafka website, in particular, Producer configurations and Consumer configurations.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#enable-debug-logging-for-data-plane-components","title":"Enable debug logging for data plane components","text":"<p>The following YAML shows the default logging configuration for data plane components, that is created during the installation step:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-config-logging\n  namespace: knative-eventing\ndata:\n  config.xml: |\n    &lt;configuration&gt;\n      &lt;appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/&gt;\n      &lt;/appender&gt;\n      &lt;root level=\"INFO\"&gt;\n        &lt;appender-ref ref=\"jsonConsoleAppender\"/&gt;\n      &lt;/root&gt;\n    &lt;/configuration&gt;\n</code></pre> <p>To change the logging level to <code>DEBUG</code>, you must:</p> <ol> <li> <p>Apply the following <code>kafka-config-logging</code> <code>ConfigMap</code> or replace <code>level=\"INFO\"</code> with <code>level=\"DEBUG\"</code> to the <code>ConfigMap</code> <code>kafka-config-logging</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-config-logging\n  namespace: knative-eventing\ndata:\n  config.xml: |\n    &lt;configuration&gt;\n      &lt;appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/&gt;\n      &lt;/appender&gt;\n      &lt;root level=\"DEBUG\"&gt;\n        &lt;appender-ref ref=\"jsonConsoleAppender\"/&gt;\n      &lt;/root&gt;\n    &lt;/configuration&gt;\n</code></pre> </li> <li> <p>Restart the <code>kafka-broker-receiver</code> and the <code>kafka-broker-dispatcher</code>, by entering the following commands:</p> <pre><code>kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver\nkubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher\n</code></pre> </li> </ol>"},{"location":"eventing/brokers/broker-types/kafka-broker/#configuring-the-order-of-delivered-events","title":"Configuring the order of delivered events","text":"<p>When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees.</p> <p>You can configure the delivery order of events using the <code>kafka.eventing.knative.dev/delivery.order</code> annotation on the <code>Trigger</code> object:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\n  annotations:\n     kafka.eventing.knative.dev/delivery.order: ordered\nspec:\n  broker: my-kafka-broker\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: my-service\n</code></pre> <p>The supported consumer delivery guarantees are:</p> <ul> <li><code>unordered</code>:  An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. Useful when there is a high demand of parallel consumption and no need for explicit ordering. One example could be processing of click analytics.</li> <li><code>ordered</code>: An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. Useful when there is a need for more strict ordering or if there is a relationship or grouping between events. One example could be processing of customer orders.</li> </ul> <p>The <code>unordered</code> delivery is the default ordering guarantee.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#data-plane-isolation-vs-shared-data-plane","title":"Data plane Isolation vs Shared Data plane","text":"<p>Knative Kafka Broker implementation has 2 planes: control plane and data plane. Control plane consists of controllers that talk to Kubernetes API, watch for custom objects and manage the data plane.</p> <p>Data plane is the collection of components that listen for incoming events, talk to Apache Kafka and also sends events to the event sinks. This is where the events flow. Knative Kafka Broker data plane consists of <code>kafka-broker-receiver</code> and <code>kafka-broker-dispatcher</code> deployments.</p> <p>When using the Broker class <code>Kafka</code>, the Knative Kafka Broker uses a shared data plane. That means, <code>kafka-broker-receiver</code> and <code>kafka-broker-dispatcher</code> deployments in <code>knative-eventing</code> namespace is used for all Kafka Brokers in the cluster.</p> <p>However, when <code>KafkaNamespaced</code> is set as the Broker class, Kafka broker controller creates a new data plane for each namespace that there is a broker exists. This data plane is used by all <code>KafkaNamespaced</code> brokers in that namespace.</p> <p>That provides isolation between the data planes, which means that the <code>kafka-broker-receiver</code> and <code>kafka-broker-dispatcher</code> deployments in the user namespace are only used for the broker in that namespace.</p> <p>Note</p> <p>As a consequence of separate data planes, this security feature creates more deployments and uses more resources. Unless you have such isolation requirements, it is recommended to go with regular Broker with <code>Kafka</code> class.</p> <p>To create a <code>KafkaNamespaced</code> broker, you must set the <code>eventing.knative.dev/broker.class</code> annotation to <code>KafkaNamespaced</code>:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    # case-sensitive\n    eventing.knative.dev/broker.class: KafkaNamespaced\n  name: default\n  namespace: my-namespace\nspec:\n  config:\n     # the referenced `configmap` must be in the same namespace with the `Broker` object, in this case `my-namespace`\n    apiVersion: v1\n    kind: ConfigMap\n    name: my-config\n    # namespace: my-namespace # no need to define, defaults to Broker's namespace\n</code></pre> <p>Note</p> <p>The <code>configmap</code> that is specified in <code>spec.config</code> must be in the same namespace with the <code>Broker</code> object:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\n  namespace: my-namespace\ndata:\n  ...\n</code></pre> <p>Upon the creation of the first <code>Broker</code> with <code>KafkaNamespaced</code> class, the <code>kafka-broker-receiver</code> and <code>kafka-broker-dispatcher</code> deployments are created in the namespace. After that, all the brokers with <code>KafkaNamespaced</code> class in the same namespace use the same data plane. When there are no brokers of <code>KafkaNamespaced</code> class in the namespace, the data plane in the namespace will be deleted.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/#configuring-kafkanamespaced-brokers","title":"Configuring <code>KafkaNamespaced</code> brokers","text":"<p>All the configuration mechanisms that are available for the <code>Kafka</code> Broker class are also available for the brokers with <code>KafkaNamespaced</code> class with these exceptions:</p> <ul> <li>This page describes how producer and consumer configurations is done by modifying the <code>config-kafka-broker-data-plane</code> configmap in the <code>knative-eventing</code> namespace. Since Kafka Broker controller propagates this configmap into the user namespace, currently there is no way to configure producer and consumer configurations per namespace. Any value set in the <code>config-kafka-broker-data-plane</code> <code>ConfigMap</code> in the <code>knative-eventing</code> namespace will be also used in the user namespace.</li> <li>Because of the same propagation, it is also not possible to configure consumer offsets commit interval per namespace.</li> <li>A few more configmaps are propagated: <code>config-tracing</code> and <code>kafka-config-logging</code>. This means, tracing and logging are also not configurable per namespace.</li> <li>Similarly, the data plane deployments are propagated from the <code>knative-eventing</code> namespace to the user namespace. This means that the data plane deployments are not configurable per namespace and will be identical to the ones in the <code>knative-eventing</code> namespace.</li> </ul>"},{"location":"eventing/brokers/broker-types/kafka-broker/#additional-information","title":"Additional information","text":"<ul> <li>To report a bug or request a feature, open an issue in the eventing-kafka-broker repository.</li> </ul>"},{"location":"eventing/brokers/broker-types/kafka-broker/configuring-kafka-features/","title":"Configuring Kafka Features","text":"<p>There are many different configuration options for how Knative Eventing and the Knaitve Broker for Apache Kafka interact with the Apache Kafka clusters.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/configuring-kafka-features/#configure-knative-eventing-kafka-features","title":"Configure Knative Eventing Kafka features","text":"<p>There are various kafka features/default values the Knative Kafka Broker uses when interacting with Kafka.</p>"},{"location":"eventing/brokers/broker-types/kafka-broker/configuring-kafka-features/#consumer-group-id-for-triggers","title":"Consumer Group ID for Triggers","text":"<p>The <code>triggers.consumergroup.template</code> value determines the template used to generate the consumer group ID used by your triggers.</p> <ul> <li>Global key: <code>triggers.consumergroup.template</code></li> <li>Possible values:: Any valid go text/template</li> <li>Default: <code>knative-trigger-{{ .Namespace }}-{{ .Name }}</code></li> </ul> <p>Example:</p> Global (ConfigMap) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-kafka-features\n  namespace: knative-eventing\ndata:\n  triggers.consumergroup.template: \"knative-trigger-{{ .Namespace }}-{{ .Name }}\"\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/configuring-kafka-features/#broker-topic-name-template","title":"Broker topic name template","text":"<p>The <code>brokers.topic.template</code> values determines the template used to generate the Kafka topic names used by your brokers.</p> <ul> <li>Global Key: <code>brokers.topic.template</code></li> <li>Possible values: Any valid go text/template</li> <li>Default: <code>knative-broker-{{ .Namespace }}-{{ .Name }}</code></li> </ul> <p>Example:</p> Global (ConfigMap) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-kafka-features\n  namespace: knative-eventing\ndata:\n  brokers.topic.template: \"knative-broker-{{ .Namespace }}-{{ .Name }}\"\n</code></pre>"},{"location":"eventing/brokers/broker-types/kafka-broker/configuring-kafka-features/#channel-topic-name-template","title":"Channel topic name template","text":"<p>The <code>channels.topic.template</code> value determines the template used to generate the kafka topic names used by your channels.</p> <ul> <li>Global Key: <code>channels.topic.template</code></li> <li>Possible values: Any valid go text/template</li> <li>Default: <code>messaging-kafka.{{ .Namespace }}.{{ .Name }}</code></li> </ul> <p>Example:</p> Global (ConfigMap) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-kafka-features\n  namespace: knative-eventing\ndata:\n  channels.topic.template: \"messaging-kafka.{{ .Namespace }}.{{ .Name }}\"\n</code></pre>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/","title":"Creating a RabbitMQ Broker","text":"<p>This topic describes how to create a RabbitMQ Broker.</p>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have installed Knative Eventing.</li> <li>You have installed CertManager v1.5.4 - easiest integration with RabbitMQ Messaging Topology Operator</li> <li>You have installed RabbitMQ Messaging Topology Operator - our recommendation is latest release with CertManager</li> <li>You have access to a working RabbitMQ instance. You can create a RabbitMQ instance by using the RabbitMQ Cluster Kubernetes Operator. For more information see the RabbitMQ website.</li> </ol>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#install-the-rabbitmq-controller","title":"Install the RabbitMQ controller","text":"<ol> <li> <p>Install the RabbitMQ controller by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml\n</code></pre> </li> <li> <p>Verify that <code>rabbitmq-broker-controller</code> and <code>rabbitmq-broker-webhook</code> are running:</p> <pre><code>kubectl get deployments.apps -n knative-eventing\n</code></pre> <p>Example output:</p> <pre><code>NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\neventing-controller            1/1     1            1           10s\neventing-webhook               1/1     1            1           9s\nrabbitmq-broker-controller     1/1     1            1           3s\nrabbitmq-broker-webhook        1/1     1            1           4s\n</code></pre> </li> </ol>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#create-a-rabbitmqbrokerconfig-object","title":"Create a RabbitMQBrokerConfig object","text":"<ol> <li> <p>Create a YAML file using the following template:     <pre><code>apiVersion: eventing.knative.dev/v1alpha1\nkind: RabbitmqBrokerConfig\nmetadata:\n  name: &lt;rabbitmq-broker-config-name&gt;\nspec:\n  rabbitmqClusterReference:\n    # Configure name if a RabbitMQ Cluster Operator is being used.\n    name: &lt;cluster-name&gt;\n    # Configure connectionSecret if an external RabbitMQ cluster is being used.\n    connectionSecret:\n      name: rabbitmq-secret-credentials\n  queueType: quorum\n</code></pre>     Where:</p> <ul> <li> is the name you want for your RabbitMQBrokerConfig object. <li> is the name of the RabbitMQ cluster you created earlier. <p>Note</p> <p>You cannot set <code>name</code> and <code>connectionSecret</code> at the same time, since <code>name</code> is for a RabbitMQ Cluster Operator instance running in the same cluster as the Broker, and <code>connectionSecret</code> is for an external RabbitMQ server.</p> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl create -f &lt;filename&gt;\n</code></pre>    Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#create-a-rabbitmqbroker-object","title":"Create a RabbitMQBroker object","text":"<ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: RabbitMQBroker\n  name: &lt;broker-name&gt;\nspec:\n  config:\n    apiVersion: eventing.knative.dev/v1alpha1\n    kind: RabbitmqBrokerConfig\n    name: &lt;rabbitmq-broker-config-name&gt;\n</code></pre> Where <code>&lt;rabbitmq-broker-config-name&gt;</code> is the name you gave your RabbitMQBrokerConfig in the step above.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#configure-message-ordering","title":"Configure message ordering","text":"<p>By default, Triggers will consume messages one at a time to preserve ordering. If ordering of events isn't important and higher performance is desired, you can configure this by using the <code>parallelism</code> annotation. Setting <code>parallelism</code> to <code>n</code> creates <code>n</code> workers for the Trigger that will all consume messages in parallel.</p> <p>The following YAML shows an example of a Trigger with parallelism set to <code>10</code>:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: high-throughput-trigger\n  annotations:\n    rabbitmq.eventing.knative.dev/parallelism: \"10\"\n...\n</code></pre>"},{"location":"eventing/brokers/broker-types/rabbitmq-broker/#additional-information","title":"Additional information","text":"<ul> <li>For more samples visit the <code>eventing-rabbitmq</code> Github repository samples directory</li> <li>To report a bug or request a feature, open an issue in the <code>eventing-rabbitmq</code> Github repository.</li> </ul>"},{"location":"eventing/channels/","title":"Channels","text":"<p>Channels are Kubernetes custom resources that define a single event forwarding and persistence layer.</p> <p>A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services.</p> <p></p>"},{"location":"eventing/channels/#next-steps","title":"Next steps","text":"<ul> <li>Learn about default available channel types</li> <li>Create a channel</li> <li>Create a subscription</li> </ul>"},{"location":"eventing/channels/channel-types-defaults/","title":"Channel types and defaults","text":"<p>Knative uses two types of Channels:</p> <ul> <li>A generic Channel object.</li> <li>Channel implementations that each have their own custom resource definitions (CRDs), such as InMemoryChannel and KafkaChannel. The KafkaChannel supports an ordered consumer delivery guarantee, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition.</li> </ul> <p>Custom Channel implementations each have their own event delivery mechanisms, such as in-memory or Broker-based. Examples of Brokers include KafkaBroker and the GCP Pub/Sub Broker.</p> <p>Knative provides the InMemoryChannel Channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS Channels.</p> <p>You can use the generic Channel object if you want to create a Channel without specifying which Channel implementation CRD is used. This is useful if you do not care about the properties a particular Channel implementation provides, such as ordering and persistence, and you want to use the implementation selected by the cluster administrator.</p> <p>Cluster administrators can modify the default Channel implementation settings by editing the <code>default-ch-webhook</code> ConfigMap in the <code>knative-eventing</code> namespace.</p> <p>For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource.</p> <p>Default Channels can be configured for the cluster, a namespace on the cluster, or both.</p> <p>Note</p> <p>If a default Channel implementation is configured for a namespace, this will overwrite the configuration for the cluster.</p> <p>In the following example, the cluster default Channel implementation is InMemoryChannel, while the namespace default Channel implementation for the <code>example-namespace</code> is KafkaChannel.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: default-ch-webhook\n  namespace: knative-eventing\ndata:\n  default-ch-config: |\n    clusterDefault:\n      apiVersion: messaging.knative.dev/v1\n      kind: InMemoryChannel\n    namespaceDefaults:\n      example-namespace:\n        apiVersion: messaging.knative.dev/v1beta1\n        kind: KafkaChannel\n        spec:\n          numPartitions: 2\n          replicationFactor: 1\n</code></pre> <p>Note</p> <p>InMemoryChannel Channels must not be used in production environments.</p>"},{"location":"eventing/channels/channel-types-defaults/#next-steps","title":"Next steps","text":"<ul> <li>Create an InMemoryChannel</li> </ul>"},{"location":"eventing/channels/channels-crds/","title":"Available Channels","text":"<p>This is a non-exhaustive list of the available Channels for Knative Eventing.</p> <p>Note</p> <p>Inclusion in this list is not an endorsement, nor does it imply any level of support.</p> Name Status Maintainer Description InMemoryChannel Stable Knative In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel Beta Knative Channels are backed by Apache Kafka topics. NatssChannel Alpha Knative Channels are backed by NATS Streaming."},{"location":"eventing/channels/create-default-channel/","title":"Creating a Channel using cluster or namespace defaults","text":"<p>Developers can create Channels of any supported implementation type by creating an instance of a Channel object.</p> <p>To create a Channel:</p> <ol> <li> <p>Create a YAML file for the Channel object using the following template:</p> <p><pre><code>apiVersion: messaging.knative.dev/v1\nkind: Channel\nmetadata:\n  name: &lt;example-channel&gt;\n  namespace: &lt;namespace&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;example-channel&gt;</code> is the name of the Channel you want to create.</li> <li><code>&lt;namespace&gt;</code> is the name of your target namespace.</li> </ul> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>If you create this object in the <code>default</code> namespace, according to the default ConfigMap example in Channel types and defaults, it is an InMemoryChannel Channel implementation.</p> <p>After the Channel object is created, a mutating admission webhook sets the <code>spec.channelTemplate</code> based on the default Channel implementation:</p> <p><pre><code>apiVersion: messaging.knative.dev/v1\nkind: Channel\nmetadata:\n  name: &lt;example-channel&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  channelTemplate:\n    apiVersion: messaging.knative.dev/v1\n    kind: &lt;channel-template-kind&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;example-channel&gt;</code> is the name of the Channel you want to create.</li> <li><code>&lt;namespace&gt;</code> is the name of your target namespace.</li> <li><code>&lt;channel-template-kind&gt;</code> is the kind of Channel, such as InMemoryChannel or KafkaChannel, based on the default ConfigMap. See an example in Channel types and defaults.</li> </ul> <p>Note</p> <p>The <code>spec.channelTemplate</code> property cannot be changed after creation, because it is set by the default Channel mechanism, not the user.</p> <p>The Channel controller creates a backing Channel instance based on the <code>spec.channelTemplate</code>.</p> <p>When this mechanism is used two objects are created; a generic Channel object and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object by copying its Subscriptions to, and setting its status to, that of the InMemoryChannel object.</p> <p>Note</p> <p>Defaults are only applied by the webhook when a Channel or Sequence is initially created. If the default settings are changed, the new defaults will only be applied to newly created Channels, Brokers, or Sequences. Existing resources are not updated automatically to use the new configuration.</p>"},{"location":"eventing/channels/subscriptions/","title":"Subscriptions","text":"<p>After you have created a Channel and a Sink, you can create a Subscription to enable event delivery.</p> <p>The Subscription consists of a Subscription object, which specifies the Channel and the Sink (also known as the Subscriber) to deliver events to. You can also specify some Sink-specific options, such as how to handle failures.</p> <p>For more information about Subscription objects, see Subscription.</p>"},{"location":"eventing/channels/subscriptions/#creating-a-subscription","title":"Creating a Subscription","text":"knYAML <p>Create a Subscription between a Channel and a Sink by running:</p> <pre><code>kn subscription create &lt;subscription-name&gt; \\\n  --channel &lt;Group:Version:Kind&gt;:&lt;channel-name&gt; \\\n  --sink &lt;sink-prefix&gt;:&lt;sink-name&gt; \\\n  --sink-reply &lt;sink-prefix&gt;:&lt;sink-name&gt; \\\n  --sink-dead-letter &lt;sink-prefix&gt;:&lt;sink-name&gt;\n</code></pre> <ul> <li> <p><code>--channel</code> specifies the source for cloud events that should be processed. You must provide the Channel name. If you are not using the default Channel that is backed by the Channel resource, you must prefix the Channel name with the <code>&lt;Group:Version:Kind&gt;</code> for the specified Channel type. For example, this is <code>messaging.knative.dev:v1beta1:KafkaChannel</code> for a Kafka-backed Channel.</p> </li> <li> <p><code>--sink</code> specifies the target destination to which the event should be delivered. By default, the <code>&lt;sink-name&gt;</code> is interpreted as a Knative service of this name, in the same namespace as the Subscription. You can specify the type of the Sink by using one of the following prefixes:</p> <ul> <li><code>ksvc</code>: A Knative service.</li> <li><code>svc</code>: A Kubernetes Service.</li> <li><code>channel</code>: A Channel that should be used as the destination. You can only reference default Channel types here.</li> <li><code>broker</code>: An Eventing Broker.</li> <li><code>--sink-reply</code> is an optional argument you can use to specify where the Sink reply is sent. It uses the same naming conventions for specifying the Sink as the <code>--sink</code> flag.</li> <li> <p><code>--sink-dead-letter</code> is an optional argument you can use to specify where to send the CloudEvent in case of a failure. It uses the same naming conventions for specifying the Sink as the <code>--sink</code> flag.</p> <ul> <li><code>ksvc</code>: A Knative service.</li> <li><code>svc</code>: A Kubernetes Service.</li> <li><code>channel</code>: A Channel that should be used as destination. Only default Channel types can be referenced here.</li> <li><code>broker</code>: An Eventing Broker.</li> </ul> </li> <li> <p><code>--sink-reply</code> and <code>--sink-dead-letter</code> are optional arguments. They can be used to specify where the Sink reply is sent, and where to send the CloudEvent in case of a failure, respectively. Both use the same naming conventions for specifying the Sink as the <code>--sink</code> flag.</p> </li> </ul> </li> </ul> <p>This example command creates a Subscription named <code>mysubscription</code> that routes events from a Channel named <code>mychannel</code> to a Knative service named <code>myservice</code>.</p> <p>Note</p> <p>The Sink prefix is optional. You can also specify the service for <code>--sink</code> as just <code>--sink &lt;service-name&gt;</code> and omit the <code>ksvc</code> prefix.</p> <ol> <li> <p>Create a YAML file for the Subscription object using the following example:</p> <pre><code>apiVersion: messaging.knative.dev/v1\nkind: Subscription\nmetadata:\n  name: &lt;subscription-name&gt;\n  # Name of the Subscription.\n  namespace: default\nspec:\n  channel:\n    apiVersion: messaging.knative.dev/v1\n    kind: Channel\n    name: &lt;channel-name&gt;\n    # Name of the Channel that the Subscription connects to.\n  delivery:\n    # Optional delivery configuration settings for events.\n    deadLetterSink:\n    # When this is configured, events that failed to be consumed are sent to the deadLetterSink.\n    # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system.\n    # The deadLetterSink value must be a Destination.\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: &lt;service-name&gt;\n  reply:\n    # Optional configuration settings for the reply event.\n    # This is the event Sink that events replied from the subscriber are delivered to.\n    ref:\n      apiVersion: messaging.knative.dev/v1\n      kind: InMemoryChannel\n      name: &lt;service-name&gt;\n  subscriber:\n    # Required configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel.\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: &lt;service-name&gt;\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/channels/subscriptions/#listing-subscriptions","title":"Listing Subscriptions","text":"<p>You can list all existing Subscriptions by using the <code>kn</code> CLI tool.</p> <ul> <li> <p>List all Subscriptions:</p> <pre><code>kn subscription list\n</code></pre> </li> <li> <p>List Subscriptions in YAML format:</p> <pre><code>kn subscription list -o yaml\n</code></pre> </li> </ul>"},{"location":"eventing/channels/subscriptions/#describing-a-subscription","title":"Describing a Subscription","text":"<p>You can print details about a Subscription by using the <code>kn</code> CLI tool:</p> <pre><code>kn subscription describe &lt;subscription-name&gt;\n</code></pre>"},{"location":"eventing/channels/subscriptions/#deleting-subscriptions","title":"Deleting Subscriptions","text":"<p>You can delete a Subscription by using the <code>kn</code> or <code>kubectl</code> CLI tools.</p> knkubectl <pre><code>kn subscription delete &lt;subscription-name&gt;\n</code></pre> <pre><code>kubectl subscription delete &lt;subscription-name&gt;\n</code></pre>"},{"location":"eventing/channels/subscriptions/#next-steps","title":"Next steps","text":"<ul> <li>Creating a Channel using cluster or namespace defaults</li> </ul>"},{"location":"eventing/configuration/broker-configuration/","title":"Configure Broker defaults","text":"<p>If you have cluster administrator permissions for your Knative installation, you can modify ConfigMaps to change the global default configuration options for Brokers on the cluster.</p> <p>Knative Eventing provides a <code>config-br-defaults</code> ConfigMap that contains the configuration settings that govern default Broker creation.</p> <p>The default <code>config-br-defaults</code> ConfigMap is as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  # Configures the default for any Broker that does not specify a spec.config or Broker class.\n  default-br-config: |\n    clusterDefault:\n      brokerClass: MTChannelBasedBroker\n      apiVersion: v1\n      kind: ConfigMap\n      name: config-br-default-channel\n      namespace: knative-eventing\n</code></pre> <p>In this case, each new Broker created in the cluster would use by default the <code>MTChannelBasedBroker</code> Broker class and the <code>config-br-default-channel</code> ConfigMap from the <code>knative-eventing</code> namespace for its configuration if not other specified in the Brokers <code>eventing.knative.dev/broker.class</code> annotation and/or <code>.spec.config</code> (see Developer configuration options).</p> <p>However, if you would like like for example a Kafka Channel to be used as the default Channel implementation for any Broker that is created, you can change the <code>config-br-defaults</code> ConfigMap to look as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  # Configures the default for any Broker that does not specify a spec.config or Broker class.\n  default-br-config: |\n    clusterDefault:\n      brokerClass: MTChannelBasedBroker\n      apiVersion: v1\n      kind: ConfigMap\n      name: kafka-channel\n      namespace: knative-eventing\n</code></pre> <p>Now every Broker created in the cluster that does not have a <code>spec.config</code> will be configured to use the <code>kafka-channel</code> ConfigMap. For more information about creating a <code>kafka-channel</code> ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation.</p> <p>You can also modify the default Broker configuration for one or more dedicated namespaces, by defining it in the <code>namespaceDefaults</code> section. For example, if you want to use the <code>config-br-default-channel</code> ConfigMap for all Brokers by default, but want to use <code>kafka-channel</code> ConfigMap for <code>namespace-1</code> and <code>namespace-2</code>, you would use the following ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  default-br-config: |\n    clusterDefault:\n      brokerClass: MTChannelBasedBroker\n      apiVersion: v1\n      kind: ConfigMap\n      name: config-br-default-channel\n      namespace: knative-eventing\n    namespaceDefaults:\n      namespace-1:\n        apiVersion: v1\n        kind: ConfigMap\n        name: kafka-channel\n        namespace: knative-eventing\n      namespace-2:\n        apiVersion: v1\n        kind: ConfigMap\n        name: kafka-channel\n        namespace: knative-eventing\n</code></pre>"},{"location":"eventing/configuration/broker-configuration/#configuring-the-broker-class","title":"Configuring the Broker class","text":"<p>Besides configuring the Broker class for each broker individually, it is possible to define the default Broker class cluster wide or on a per namespace basis:</p>"},{"location":"eventing/configuration/broker-configuration/#configuring-the-default-broker-class-for-the-cluster","title":"Configuring the default Broker class for the cluster","text":"<p>You can configure the <code>clusterDefault</code> Broker class so that any Broker created in the cluster that does not have a <code>eventing.knative.dev/broker.class</code> annotation uses this default Broker class:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  # Configures the default for any Broker that does not specify a spec.config or Broker class.\n  default-br-config: |\n    clusterDefault:\n      brokerClass: MTChannelBasedBroker\n</code></pre>"},{"location":"eventing/configuration/broker-configuration/#configuring-the-default-broker-class-for-namespaces","title":"Configuring the default Broker class for namespaces","text":"<p>You can modify the default Broker class for one or more namespaces.</p> <p>For example, if you want to use a <code>KafkaBroker</code> Broker class for all other Brokers created on the cluster, but you want to use the <code>MTChannelBasedBroker</code> Broker class for Brokers created in <code>namespace-1</code> and <code>namespace-2</code>, you would use the following ConfigMap settings:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  # Configures the default for any Broker that does not specify a spec.config or Broker class.\n  default-br-config: |\n    clusterDefault:\n      brokerClass: KafkaBroker\n    namespaceDefaults:\n      namespace1:\n        brokerClass: MTChannelBasedBroker\n      namespace2:\n        brokerClass: MTChannelBasedBroker\n</code></pre> <p>Note</p> <p>Be aware that different Broker classes usually require different configuration ConfigMaps. See the configuration options of the different Broker implementations on how their referenced ConfigMaps have to look like (e.g. for MTChannelBasedBroker or Knative Broker for Apache Kafka).</p>"},{"location":"eventing/configuration/broker-configuration/#configuring-delivery-spec-defaults","title":"Configuring delivery spec defaults","text":"<p>You can configure default event delivery parameters for Brokers that are applied in cases where an event fails to be delivered:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  # Configures the default for any Broker that does not specify a spec.config or Broker class.\n  default-br-config: |\n    clusterDefault:\n      brokerClass: MTChannelBasedBroker\n      apiVersion: v1\n      kind: ConfigMap\n      name: kafka-channel\n      namespace: knative-eventing\n      delivery:\n        retry: 10\n        backoffDelay: PT0.2S\n        backoffPolicy: exponential\n    namespaceDefaults:\n      namespace-1:\n        apiVersion: v1\n        kind: ConfigMap\n        name: config-br-default-channel\n        namespace: knative-eventing\n        delivery:\n          deadLetterSink:\n            ref:\n              kind: Service\n              namespace: example-namespace\n              name: example-service\n              apiVersion: v1\n            uri: example-uri\n          retry: 10\n          backoffPolicy: exponential\n          backoffDelay: \"PT0.2S\"\n</code></pre>"},{"location":"eventing/configuration/broker-configuration/#dead-letter-sink","title":"Dead letter sink","text":"<p>You can configure the <code>deadLetterSink</code> delivery parameter so that if an event fails to be delivered it is sent to the specified event sink.</p>"},{"location":"eventing/configuration/broker-configuration/#retries","title":"Retries","text":"<p>You can set a minimum number of times that the delivery must be retried before the event is sent to the dead letter sink, by configuring the <code>retry</code> delivery parameter with an integer value.</p>"},{"location":"eventing/configuration/broker-configuration/#back-off-delay","title":"Back off delay","text":"<p>You can set the <code>backoffDelay</code> delivery parameter to specify the time delay before an event delivery retry is attempted after a failure. The duration of the <code>backoffDelay</code> parameter is specified using the ISO 8601 format.</p>"},{"location":"eventing/configuration/broker-configuration/#back-off-policy","title":"Back off policy","text":"<p>The <code>backoffPolicy</code> delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential. When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential backoff policy, the back off delay is equal to <code>backoffDelay*2^&lt;numberOfRetries&gt;</code>.</p>"},{"location":"eventing/configuration/broker-configuration/#integrating-istio-with-knative-brokers","title":"Integrating Istio with Knative Brokers","text":""},{"location":"eventing/configuration/broker-configuration/#protect-a-knative-broker-by-using-json-web-token-jwt-and-istio","title":"Protect a Knative Broker by using JSON Web Token (JWT) and Istio","text":""},{"location":"eventing/configuration/broker-configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have installed Knative Eventing.</li> <li>You have installed Istio.</li> </ul>"},{"location":"eventing/configuration/broker-configuration/#procedure","title":"Procedure","text":"<ol> <li> <p>Label the <code>knative-eventing</code> namespace, so that Istio can handle JWT-based user authentication, by running the command:</p> <pre><code>kubectl label namespace knative-eventing istio-injection=enabled\n</code></pre> </li> <li> <p>Restart the broker ingress pod, so that the <code>istio-proxy</code> container can be injected as a sidecar, by running the command:</p> <pre><code>kubectl delete pod &lt;broker-ingress-pod-name&gt; -n knative-eventing\n</code></pre> <p>Where <code>&lt;broker-ingress-pod-name&gt;</code> is the name of your Broker ingress pod.</p> <p>The pod now has two containers:</p> <pre><code>knative-eventing     &lt;broker-ingress-pod-name&gt;           2/2     Running   1              175m\n</code></pre> </li> <li> <p>Create a broker, then use get the URL of your Broker by running the command:</p> <pre><code>kubectl get broker &lt;broker-name&gt;\n</code></pre> <p>Where <code>&lt;broker-name&gt;</code> is the name of your Broker.</p> <p>Example output:</p> <pre><code>NAMESPACE   NAME        URL                                                                          AGE   READY   REASON\ndefault     my-broker   http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker   6s    True\n</code></pre> </li> <li> <p>Start a <code>curl</code> pod by running the following command:</p> <pre><code>kubectl -n default run curl --image=radial/busyboxplus:curl -i --tty\n</code></pre> </li> <li> <p>Send a CloudEvent with an HTTP POST against the Broker URL by running the following command:</p> <pre><code>curl -X POST -v \\\n-H \"content-type: application/json\"  \\\n-H \"ce-specversion: 1.0\"  \\\n-H \"ce-source: my/curl/command\"  \\\n-H \"ce-type: my.demo.event\"  \\\n-H \"ce-id: 0815\"  \\\n-d '{\"value\":\"Hello Knative\"}' \\\n&lt;broker-URL&gt;\n</code></pre> <p>Where <code>&lt;broker-URL&gt;</code> is the URL of your Broker. For example:</p> <pre><code>curl -X POST -v \\\n-H \"content-type: application/json\"  \\\n-H \"ce-specversion: 1.0\"  \\\n-H \"ce-source: my/curl/command\"  \\\n-H \"ce-type: my.demo.event\"  \\\n-H \"ce-id: 0815\"  \\\n-d '{\"value\":\"Hello Knative\"}' \\\nhttp://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker\n</code></pre> </li> <li> <p>You will receive a <code>202</code> HTTP response code, that the broker did accept the request:</p> <pre><code>...\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 202 Accepted\n&lt; allow: POST, OPTIONS\n&lt; date: Tue, 15 Mar 2022 13:37:57 GMT\n&lt; content-length: 0\n&lt; x-envoy-upstream-service-time: 79\n&lt; server: istio-envoy\n&lt; x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*\n</code></pre> </li> <li> <p>Apply a <code>AuthorizationPolicy</code> object in the <code>knative-eventing</code> namespace to describe that the path to the Broker is restricted to a given user:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: require-jwt\n  namespace: knative-eventing\nspec:\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n       requestPrincipals: [\"testing@secure.istio.io/testing@secure.istio.io\"]\n    to:\n    - operation:\n        methods: [\"POST\"]\n        paths: [\"/default/my-broker\"]\n</code></pre> </li> <li> <p>Create a <code>RequestAuthentication</code> object for the user <code>requestPrincipal</code> in the <code>istio-system</code> namespace:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\nmetadata:\n  name: \"jwt-example\"\n  namespace: istio-system\nspec:\n  jwtRules:\n  - issuer: \"testing@secure.istio.io\"\n    jwksUri: \"https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/jwks.json\"\n</code></pre> </li> <li> <p>Now retrying the <code>curl</code> command results in a <code>403 - Forbidden</code> response code from the server:</p> <pre><code>...\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 403 Forbidden\n&lt; content-length: 19\n&lt; content-type: text/plain\n&lt; date: Tue, 15 Mar 2022 13:47:53 GMT\n&lt; server: istio-envoy\n&lt; connection: close\n&lt; x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*\n</code></pre> </li> <li> <p>To access the Broker, add the Bearer JSON Web Token as part of the request:</p> <pre><code>TOKEN=$(curl https://raw.githubusercontent.com/istio/istio/release-1.13/security/tools/jwt/samples/demo.jwt -s)\n\ncurl -X POST -v \\\n-H \"content-type: application/json\"  \\\n-H \"Authorization: Bearer ${TOKEN}\"  \\\n-H \"ce-specversion: 1.0\"  \\\n-H \"ce-source: my/curl/command\"  \\\n-H \"ce-type: my.demo.event\"  \\\n-H \"ce-id: 0815\"  \\\n-d '{\"value\":\"Hello Knative\"}' \\\n&lt;broker-URL&gt;\n</code></pre> <p>The server now responds with a <code>202</code> response code, indicating that it has accepted the HTTP request:</p> <pre><code>* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 202 Accepted\n&lt; allow: POST, OPTIONS\n&lt; date: Tue, 15 Mar 2022 14:05:09 GMT\n&lt; content-length: 0\n&lt; x-envoy-upstream-service-time: 40\n&lt; server: istio-envoy\n&lt; x-envoy-decorator-operation: broker-ingress.knative-eventing.svc.cluster.local:80/*\n</code></pre> </li> </ol>"},{"location":"eventing/configuration/channel-configuration/","title":"Configure Channel defaults","text":"<p>Knative Eventing provides a <code>default-ch-webhook</code> ConfigMap that contains the configuration settings that govern default Channel creation.</p> <p>The default <code>default-ch-webhook</code> ConfigMap is as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: default-ch-webhook\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    app.kubernetes.io/version: devel\n    app.kubernetes.io/part-of: knative-eventing\ndata:\n  default-ch-config: |\n    clusterDefault:\n      apiVersion: messaging.knative.dev/v1\n      kind: InMemoryChannel\n    namespaceDefaults:\n      some-namespace:\n        apiVersion: messaging.knative.dev/v1\n        kind: InMemoryChannel\n</code></pre> <p>By changing the <code>data.default-ch-config</code> property we can define the clusterDefaults and per Namespace defaults.</p> <p>This configuration is used by the Channel custom resource definition (CRD) to create platform specific implementations.</p> <p>Note</p> <p>The <code>clusterDefault</code> setting determines the global, cluster-wide default Channel type. You can configure Channel defaults for individual namespaces by using the <code>namespaceDefaults</code> setting.</p>"},{"location":"eventing/configuration/kafka-channel-configuration/","title":"Configure Channels for Apache Kafka","text":"<p>Note</p> <p>This guide assumes Knative Eventing is installed in the <code>knative-eventing</code> namespace. If you have installed Knative Eventing in a different namespace, replace <code>knative-eventing</code> with the name of that namespace.</p> <p>To use Kafka Channels, you must:</p> <ol> <li>Install the KafkaChannel custom resource definition (CRD).</li> <li>Create a ConfigMap that specifies default configurations for how KafkaChannel instances are created.</li> </ol>"},{"location":"eventing/configuration/kafka-channel-configuration/#create-a-kafka-channel-configmap","title":"Create a <code>kafka-channel</code> ConfigMap","text":"<ol> <li> <p>Create a YAML file for the <code>kafka-channel</code> ConfigMap using the following template:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-channel\n  namespace: knative-eventing\ndata:\n  channel-template-spec: |\n    apiVersion: messaging.knative.dev/v1beta1\n    kind: KafkaChannel\n    spec:\n      numPartitions: 3\n      replicationFactor: 1\n</code></pre> <p>Note</p> <p>This example specifies two extra parameters that are specific to Kafka Channels; <code>numPartitions</code> and <code>replicationFactor</code>.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> <li> <p>Optional. To create a Broker that uses Kafka Channels, specify the <code>kafka-channel</code> ConfigMap in the Broker spec. You can do this by creating a YAML file using the following template:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  annotations:\n    eventing.knative.dev/broker.class: MTChannelBasedBroker\n  name: kafka-backed-broker\n  namespace: default\nspec:\n  config:\n    apiVersion: v1\n    kind: ConfigMap\n    name: kafka-channel\n    namespace: knative-eventing\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/configuration/sources-configuration/","title":"Configure event source defaults","text":"<p>This topic describes how to configure defaults for Knative event sources. You can configure event sources depending on how they generate events.</p>"},{"location":"eventing/configuration/sources-configuration/#configure-defaults-for-pingsource","title":"Configure defaults for PingSource","text":"<p>PingSource is an event source that produces events with a fixed payload on a specified cron schedule. For how to create a new PingSource, see Creating a PingSource object. For the available parameters, see PingSource reference.</p> <p>In addition to the parameters that you can configure in the PingSource resource, there is a global ConfigMap called <code>config-ping-defaults</code>. This ConfigMap allows you to change the maximum amount of data that the PingSource adds to the CloudEvents it produces.</p> <p>The <code>data-max-size</code> parameter allows you to set the maximum number of bytes allowed to be sent for a message excluding any base64 decoding. The default value, <code>-1</code>, sets no limit for data.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-ping-defaults\n  namespace: knative-eventing\ndata:\n  data-max-size: -1\n</code></pre> <p>You can edit this ConfigMap by running the command:</p> <pre><code>kubectl edit cm config-ping-defaults -n knative-eventing\n</code></pre>"},{"location":"eventing/configuration/sugar-configuration/","title":"Configure Sugar Controller","text":"<p>This topic describes how to configure the Sugar Controller. You can configure the Sugar controller to create a Broker when a Namespace or Trigger is created with configured labels. See Knative Eventing Sugar Controller for an example.</p> <p>The default <code>config-sugar</code> ConfigMap disables Sugar Controller, by setting <code>namespace-selector</code> and <code>trigger-selector</code> to an empty string.</p> <p>To enable the Sugar Controller</p> <ul> <li>for Namespaces, the LabelSelector <code>namespace-selector</code> can be configured.</li> <li>for Triggers, the LabelSelector <code>trigger-selector</code> can be configured.</li> </ul> <p>Sample configuration to enable Sugar Controller on selected Namespaces and Triggers</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: config-sugar\nnamespace: knative-eventing\nlabels:\n    eventing.knative.dev/release: devel\ndata:\n  namespace-selector: |\n    matchExpressions:\n    - key: \"eventing.knative.dev/injection\"\n      operator: \"In\"\n      values: [\"enabled\"]\n\n  trigger-selector: |\n    matchExpressions:\n    - key: \"eventing.knative.dev/injection\"\n      operator: \"In\"\n      values: [\"enabled\"]\n</code></pre> <p>The Sugar Controller will only operate on Namespaces or Triggers with the label <code>eventing.knative.dev/injection: enabled</code>. This also emulates the legacy Sugar Controller behavior for Namespaces.</p> <p>You can edit this ConfigMap by running the command:</p> <pre><code>kubectl edit cm config-sugar -n knative-eventing\n</code></pre>"},{"location":"eventing/custom-event-source/","title":"Custom event sources","text":"<p>If you need to ingress events from an event producer that is not included in Knative, or from a producer that emits events which are not in the CloudEvent format that is used by Knative, you can do this by using one of the following methods:</p> <ul> <li>Create a custom Knative event source.</li> <li>Use a PodSpecable object as an event source, by creating a SinkBinding.</li> <li>Use a container as an event source, by creating a ContainerSource.</li> </ul>"},{"location":"eventing/custom-event-source/containersource/","title":"Create a ContainerSource","text":"<p>The ContainerSource object starts a container image that generates events and sends messages to a sink URI. You can also use ContainerSource to support your own event sources in Knative.</p> <p>To create a custom event source using ContainerSource, you must create a container image, and a ContainerSource that uses your image URI.</p>"},{"location":"eventing/custom-event-source/containersource/#before-you-begin","title":"Before you begin","text":"<p>Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster.</p>"},{"location":"eventing/custom-event-source/containersource/#develop-build-and-publish-a-container-image","title":"Develop, build and publish a container image","text":"<p>You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines:</p> <ul> <li>Two environments variables are injected by the ContainerSource controller; <code>K_SINK</code> and <code>K_CE_OVERRIDES</code>, resolved from <code>spec.sink</code> and <code>spec.ceOverrides</code> respectively.</li> <li>The event messages are sent to the sink URI specified in <code>K_SINK</code>. The message must be sent as a POST in CloudEvents HTTP format.</li> </ul>"},{"location":"eventing/custom-event-source/containersource/#create-a-containersource-object","title":"Create a ContainerSource object","text":"<ol> <li> <p>Build an image of your event source and publish it to your image repository. Your image must read the environment variable <code>K_SINK</code> and post messages to the URL specified in <code>K_SINK</code>.</p> <p>You can use the following YAML to deploy a demo <code>heartbeats</code> event source:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: ContainerSource\nmetadata:\n  name: heartbeat-source\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest\n          name: heartbeats\n  sink:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: event-display\n</code></pre> </li> <li> <p>Create a namespace for your ContainerSource by running the command:</p> <pre><code>kubectl create namespace &lt;namespace&gt;\n</code></pre> <p>Where <code>&lt;namespace&gt;</code> is the namespace that you want your ContainerSource to use. For example, <code>heartbeat-source</code>.</p> </li> <li> <p>Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log:</p> <p>Note</p> <p>To create a Knative service you must have Knative Serving installed on your cluster.</p> knYAML <ul> <li> <p>To create a sink, run the command:</p> <pre><code>kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> </li> </ul> <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: event-display\nspec:\n  replicas: 1\n  selector:\n    matchLabels: &amp;labels\n      app: event-display\n  template:\n    metadata:\n      labels: *labels\n    spec:\n      containers:\n        - name: event-display\n          image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: event-display\nspec:\n  selector:\n    app: event-display\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>Create a concrete ContainerSource with specific arguments and environment settings:</p> knYAML <ul> <li> <p>To create the ContainerSource, run the command:</p> <p><pre><code>kn source container create &lt;name&gt; --image &lt;image-uri&gt; --sink &lt;sink&gt; -e POD_NAME=&lt;pod-name&gt; -e POD_NAMESPACE=&lt;pod-namespace&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;name&gt;</code> is the name you want for your ContainerSource object, for example, <code>test-heartbeats</code>.</li> <li><code>&lt;image-uri&gt;</code> corresponds to the image URI you built and published in step 1, for example, <code>gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats</code>.</li> <li><code>&lt;pod-name&gt;</code> is the name of the Pod that the container runs in, for example, <code>mypod</code>.</li> <li><code>&lt;pod-namespace&gt;</code> is the namespace that the Pod runs in, for example, <code>event-test</code>.</li> <li><code>&lt;sink&gt;</code> is the name of your sink, for example, <code>event-display</code>. For a list of available options, see the Knative client documentation.</li> </ul> </li> </ul> <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: sources.knative.dev/v1\nkind: ContainerSource\nmetadata:\n  name: &lt;containersource-name&gt;\nspec:\n  template:\n    spec:\n      containers:\n        - image: &lt;event-source-image-uri&gt;\n          name: &lt;container-name&gt;\n          env:\n            - name: POD_NAME\n              value: \"&lt;pod-name&gt;\"\n            - name: POD_NAMESPACE\n              value: \"&lt;pod-namespace&gt;\"\n  sink:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: &lt;sink&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;namespace&gt;</code> is the namespace you created for your ContainerSource, for example, <code>containersource-example</code>.</li> <li><code>&lt;containersource-name&gt;</code> is the name you want for your ContainerSource, for example, <code>test-heartbeats</code>.</li> <li><code>&lt;event-source-image-uri&gt;</code> corresponds to the image URI you built and published in step 1, for example, <code>gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats</code>.</li> <li><code>&lt;container-name&gt;</code> is the name of your event source, for example, <code>heartbeats</code>.</li> <li><code>&lt;pod-name&gt;</code> is the name of the Pod that the container runs in, for example, <code>mypod</code>.</li> <li><code>&lt;pod-namespace&gt;</code> is the namespace that the Pod runs in, for example, <code>event-test</code>.</li> <li><code>&lt;sink&gt;</code> is the name of your sink, for example, <code>event-display</code>.</li> </ul> <p>For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>Note</p> <p>Arguments and environment variables are set and are passed to the container.</p> </li> </ol>"},{"location":"eventing/custom-event-source/containersource/#verify-the-containersource-object","title":"Verify the ContainerSource object","text":"<ol> <li> <p>View the logs for your event consumer by running the command:</p> <p><pre><code>kubectl -n &lt;namespace&gt; logs -l &lt;pod-name&gt; --tail=200\n</code></pre> Where:</p> <ul> <li><code>&lt;namespace&gt;</code> is the namespace that contains the ContainerSource object.</li> <li><code>&lt;pod-name&gt;</code> is the name of the Pod that the container runs in.</li> </ul> <p>For example:</p> <pre><code>$ kubectl -n containersource-example logs -l app=event-display --tail=200\n</code></pre> </li> <li> <p>Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the <code>Attributes</code> and <code>Data</code> properties of the events that the ContainerSource sent to the <code>event-display</code> Service:</p> <pre><code>\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.eventing.samples.heartbeat\n  source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod\n  id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596\n  time: 2019-10-18T15:23:20.809775386Z\n  contenttype: application/json\nExtensions,\n  beats: true\n  heart: yes\n  the: 42\nData,\n  {\n    \"id\": 2,\n    \"label\": \"\"\n  }\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/containersource/#delete-the-containersource-object","title":"Delete the ContainerSource object","text":"<p>To delete the ContainerSource object and all of the related resources in the namespace:</p> <ul> <li> <p>Delete the namespace by running the command:</p> <pre><code>kubectl delete namespace &lt;namespace&gt;\n</code></pre> <p>Where <code>&lt;namespace&gt;</code> is the namespace that contains the ContainerSource object.</p> </li> </ul>"},{"location":"eventing/custom-event-source/containersource/#reference-documentation","title":"Reference Documentation","text":"<p>See the ContainerSource reference.</p>"},{"location":"eventing/custom-event-source/containersource/reference/","title":"ContainerSource reference","text":"<p>This topic provides reference information about the configurable fields for the ContainerSource object.</p>"},{"location":"eventing/custom-event-source/containersource/reference/#containersource","title":"ContainerSource","text":"<p>A ContainerSource definition supports the following fields:</p> Field Description Required or optional <code>apiVersion</code> Specifies the API version, for example <code>sources.knative.dev/v1</code>. Required <code>kind</code> Identifies this resource object as a ContainerSource object. Required <code>metadata</code> Specifies metadata that uniquely identifies the ContainerSource object. For example, a <code>name</code>. Required <code>spec</code> Specifies the configuration information for this ContainerSource object. Required <code>spec.sink</code> A reference to an object that resolves to a URI to use as the sink. Required <code>spec.template</code> A <code>template</code> in the shape of <code>Deployment.spec.template</code> to be used for this ContainerSource. Required <code>spec.ceOverrides</code> Defines overrides to control the output format and modifications to the event sent to the sink. Optional"},{"location":"eventing/custom-event-source/containersource/reference/#template-parameter","title":"Template parameter","text":"<p>This is a <code>template</code> in the shape of <code>Deployment.spec.template</code> to use for the ContainerSource. For more information, see the Kubernetes Documentation.</p>"},{"location":"eventing/custom-event-source/containersource/reference/#example-template-parameter","title":"Example: template parameter","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ContainerSource\nmetadata:\n  name: test-heartbeats\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats\n          name: heartbeats\n          args:\n            - --period=1\n          env:\n            - name: POD_NAME\n              value: \"mypod\"\n            - name: POD_NAMESPACE\n              value: \"event-test\"\n  ...\n</code></pre>"},{"location":"eventing/custom-event-source/containersource/reference/#cloudevent-overrides","title":"CloudEvent Overrides","text":"<p>CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink.</p> <p>A <code>ceOverrides</code> definition supports the following fields:</p> Field Description Required or optional <code>extensions</code> Specifies which attributes are added or overridden on the outbound event. Each <code>extensions</code> key-value pair is set independently on the event as an attribute extension. Optional <p>Note</p> <p>Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the <code>type</code> attribute.</p>"},{"location":"eventing/custom-event-source/containersource/reference/#example-cloudevent-overrides","title":"Example: CloudEvent Overrides","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ContainerSource\nmetadata:\n  name: test-heartbeats\nspec:\n  ...\n  ceOverrides:\n    extensions:\n      extra: this is an extra attribute\n      additional: 42\n</code></pre> <p>Contract</p> <p>This results in the <code>K_CE_OVERRIDES</code> environment variable being set on the <code>subject</code> as follows:  <pre><code>{ \"extensions\": { \"extra\": \"this is an extra attribute\", \"additional\": \"42\" } }\n</code></pre></p>"},{"location":"eventing/custom-event-source/custom-event-source/","title":"Create a custom event source","text":"<p>If you want to create a custom event source for a specific event producer type, you must create the components that enable forwarding events from that producer type to a Knative sink.</p> <p>This type of integration requires more effort than using some simpler integration types, such as SinkBinding or ContainerSource; however, this provides the most polished result and is the easiest integration type for users to consume. By providing a custom resource definition (CRD) for your source rather than a general container definition, it is easier to expose meaningful configuration options and documentation to users and hide implementation details.</p> <p>Note</p> <p>If you have created a new event source type that is not a part of the core Knative project, you can open a pull request to add it to the list of Third-Party Sources, and announce the new source in the #knative-eventing Slack channel.</p> <p>You can also add your event source to the <code>knative-extensions</code> organization, by following the instructions to create a extensions repository.</p>"},{"location":"eventing/custom-event-source/custom-event-source/#required-components","title":"Required components","text":"<p>To create a custom event source, you must create the following components:</p> Component Description Receive adapter Contains logic that specifies how to get events from a producer, what the sink URI is, and how to translate events into the CloudEvent format. Kubernetes controller Manages the event source and reconciles underlying receive adapter deployments. Custom resource definition (CRD) Provides the configuration that the controller uses to manage the receive adapter."},{"location":"eventing/custom-event-source/custom-event-source/#using-the-sample-source","title":"Using the sample source","text":"<p>The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter.</p> <p>For more information on using the sample source, see the documentation.</p>"},{"location":"eventing/custom-event-source/custom-event-source/#additional-resources","title":"Additional resources","text":"<ul> <li> <p>Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed.</p> </li> <li> <p>Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD.</p> </li> </ul>"},{"location":"eventing/custom-event-source/custom-event-source/controller/","title":"Create a controller","text":"<p>You can use the sample repository <code>update-codegen.sh</code> script to generate and inject the required components (the <code>clientset</code>, <code>cache</code>, <code>informers</code>, and <code>listers</code>) into your custom controller.</p> <p>Example controller:</p> <pre><code>import (\n    // ...\n    sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\"\n    samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\"\n)\n// ...\nfunc NewController(ctx context.Context, cmw configmap.Watcher) *controller.Impl {\n    sampleSourceInformer := samplesourceinformer.Get(ctx)\n        r := &amp;Reconciler{\n        // ...\n        samplesourceClientSet: sampleSourceClient.Get(ctx),\n        samplesourceLister:    sampleSourceInformer.Lister(),\n        // ...\n}\n</code></pre>"},{"location":"eventing/custom-event-source/custom-event-source/controller/#procedure","title":"Procedure","text":"<ol> <li> <p>Generate the components by running the command:</p> <pre><code>${CODEGEN_PKG}/generate-groups.sh \"deepcopy,client,informer,lister\" \\\n  knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\\n  \"samples:v1alpha1\" \\\n  --go-header-file ${REPO_ROOT}/hack/boilerplate/boilerplate.go.txt\n</code></pre> </li> <li> <p>Inject the components by running the command:</p> <pre><code># Injection\n${KNATIVE_CODEGEN_PKG}/hack/generate-knative.sh \"injection\" \\\n  knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\\n  \"samples:v1alpha1\" \\\n  --go-header-file ${REPO_ROOT}/hack/boilerplate/boilerplate.go.txt\n</code></pre> </li> <li> <p>Pass the new controller implementation to the <code>sharedmain</code> method:</p> <pre><code>import (\n    // The set of controllers this controller process runs.\n    \"knative.dev/sample-source/pkg/reconciler/sample\"\n\n    // This defines the shared main for injected controllers.\n    \"knative.dev/pkg/injection/sharedmain\"\n)\n\nfunc main() {\n    sharedmain.Main(\"sample-source-controller\", sample.NewController)\n}\n</code></pre> </li> <li> <p>Define the <code>NewController</code> implementation:</p> <pre><code>func NewController(\n    ctx context.Context,\n    cmw configmap.Watcher,\n) *controller.Impl {\n    // ...\n    deploymentInformer := deploymentinformer.Get(ctx)\n    sinkBindingInformer := sinkbindinginformer.Get(ctx)\n    sampleSourceInformer := samplesourceinformer.Get(ctx)\n\n    r := &amp;Reconciler{\n    dr:  &amp;reconciler.DeploymentReconciler{KubeClientSet: kubeclient.Get(ctx)},\n    sbr: &amp;reconciler.SinkBindingReconciler{EventingClientSet: eventingclient.Get(ctx)},\n    // Config accessor takes care of tracing/config/logging config propagation to the receive adapter\n    configAccessor: reconcilersource.WatchConfigurations(ctx, \"sample-source\", cmw),\n}\n</code></pre> <p>A <code>configmap.Watcher</code> and a context, which the injected listers use for the reconciler struct arguments, are passed to this implementation.</p> </li> <li> <p>Import the base reconciler from the <code>knative.dev/pkg</code> dependency:</p> <pre><code>import (\n    // ...\n    reconcilersource \"knative.dev/eventing/pkg/reconciler/source\"\n    // ...\n)\n</code></pre> </li> <li> <p>Ensure that the event handlers are being filtered to the correct informers:</p> <pre><code>    sampleSourceInformer.Informer().AddEventHandler(controller.HandleAll(impl.Enqueue))\n</code></pre> </li> <li> <p>Ensure that informers are configured correctly for the secondary resources used by the sample source to deploy and bind the event source and the receive adapter:</p> <pre><code>    deploymentInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{\n        FilterFunc: controller.FilterGroupKind(v1alpha1.Kind(\"SampleSource\")),\n        Handler:    controller.HandleAll(impl.EnqueueControllerOf),\n    })\n\n    sinkBindingInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{\n        FilterFunc: controller.FilterGroupKind(v1alpha1.Kind(\"SampleSource\")),\n        Handler:    controller.HandleAll(impl.EnqueueControllerOf),\n    })\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/","title":"Publish an event source to your cluster","text":"<ol> <li> <p>Start a minikube cluster:</p> <pre><code>minikube start\n</code></pre> </li> <li> <p>Setup <code>ko</code> to use the minikube docker instance and local registry:</p> <pre><code>eval $(minikube docker-env)\nexport KO_DOCKER_REPO=ko.local\n</code></pre> </li> <li> <p>Apply the CRD and configuration YAML:</p> <pre><code>ko apply -f config\n</code></pre> </li> <li> <p>Once the <code>sample-source-controller-manager</code> is running in the <code>knative-samples</code> namespace, you can apply the <code>example.yaml</code> to connect our <code>sample-source</code> every <code>10s</code> directly to a <code>ksvc</code>.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: event-display\n  namespace: knative-samples\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n---\napiVersion: samples.knative.dev/v1alpha1\nkind: SampleSource\nmetadata:\n  name: sample-source\n  namespace: knative-samples\nspec:\n  interval: \"10s\"\n  sink:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: event-display\n</code></pre> <pre><code>ko apply -f example.yaml\n</code></pre> </li> <li> <p>Once reconciled, you can confirm the <code>ksvc</code> is outputting valid cloudevents every <code>10s</code> to align with our specified interval.</p> <pre><code>% kubectl -n knative-samples logs -l serving.knative.dev/service=event-display -c user-container -f\n</code></pre> <pre><code>\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.sample\n  source: http://sample.knative.dev/heartbeat-source\n  id: d4619592-363e-4a41-82d1-b1586c390e24\n  time: 2019-12-17T01:31:10.795588888Z\n  datacontenttype: application/json\nData,\n  {\n    \"Sequence\": 0,\n    \"Heartbeat\": \"10s\"\n  }\n\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.sample\n  source: http://sample.knative.dev/heartbeat-source\n  id: db2edad0-06bc-4234-b9e1-7ea3955841d6\n  time: 2019-12-17T01:31:20.825969504Z\n  datacontenttype: application/json\nData,\n  {\n    \"Sequence\": 1,\n    \"Heartbeat\": \"10s\"\n  }\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/","title":"Create a receive adapter","text":"<p>As part of the source reconciliation process, you must create and deploy the underlying receive adapter.</p> <p>The receive adapter requires an injection-based <code>main</code> method that is located in <code>cmd/receiver_adapter/main.go</code>:</p> <pre><code>// This Adapter generates events at a regular interval.\npackage main\n\nimport (\n    \"knative.dev/eventing/pkg/adapter\"\n    myadapter \"knative.dev/sample-source/pkg/adapter\"\n)\n\nfunc main() {\n    adapter.Main(\"sample-source\", myadapter.NewEnv, myadapter.NewAdapter)\n}\n</code></pre> <p>The receive adapter's <code>pkg</code> implementation consists of two main functions:</p> <ol> <li> <p>A <code>NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {}</code> call, which creates the new adapter with passed variables via the <code>EnvConfigAccessor</code>. The created adapter is passed the CloudEvents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or <code>ceClient</code> in the Knative ecosystem.  The return value is a reference to the adapter as defined by the adapter's local struct.</p> <p>In the case of the sample source:</p> <pre><code>// Adapter generates events at a regular interval.\ntype Adapter struct {\n    logger   *zap.Logger\n    interval time.Duration\n    nextID   int\n    client   cloudevents.Client\n}\n</code></pre> </li> <li> <p>A <code>Start</code> function, implemented as an interface to the adapter <code>struct</code>:</p> <pre><code>func (a *Adapter) Start(stopCh &lt;-chan struct{}) error {\n</code></pre> <p><code>stopCh</code> is the signal to stop the adapter. Otherwise, the role of the function is to process the next event.</p> <p>In the case of the <code>sample-source</code>, this function creates a CloudEvent to forward to the specified sink every X interval, as specified by the <code>EnvConfigAccessor</code> parameter, which is loaded by the resource YAML:</p> <pre><code>func (a *Adapter) Start(stopCh &lt;-chan struct{}) error {\n    a.logger.Infow(\"Starting heartbeat\", zap.String(\"interval\", a.interval.String()))\n    for {\n        select {\n        case &lt;-time.After(a.interval):\n            event := a.newEvent()\n            a.logger.Infow(\"Sending new event\", zap.String(\"event\", event.String()))\n            if result := a.client.Send(context.Background(), event); !cloudevents.IsACK(result) {\n                a.logger.Infow(\"failed to send event\", zap.String(\"event\", event.String()), zap.Error(result))\n                // We got an error but it could be transient, try again next interval.\n                continue\n            }\n        case &lt;-stopCh:\n            a.logger.Info(\"Shutting down...\")\n            return nil\n        }\n    }\n}\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#managing-the-receive-adapter-in-the-controller","title":"Managing the Receive Adapter in the Controller","text":"<ol> <li> <p>Update the <code>ObservedGeneration</code> and initialize the <code>Status</code> conditions, as defined in the <code>samplesource_lifecycle.go</code> and <code>samplesource_types.go</code> files:</p> <pre><code>src.Status.InitializeConditions()\nsrc.Status.ObservedGeneration = src.Generation\n</code></pre> </li> <li> <p>Create a receive adapter.</p> <ol> <li> <p>Verify that the specified Kubernetes resources are valid, and update the <code>Status</code> accordingly.</p> </li> <li> <p>Assemble the <code>ReceiveAdapterArgs</code>:</p> <pre><code>raArgs := resources.ReceiveAdapterArgs{\n        EventSource:    src.Namespace + \"/\" + src.Name,\n        Image:          r.ReceiveAdapterImage,\n        Source:         src,\n        Labels:         resources.Labels(src.Name),\n        AdditionalEnvs: r.configAccessor.ToEnvVars(), // Grab config envs for tracing/logging/metrics\n    }\n</code></pre> <p>Note</p> <p>The exact arguments may change based on functional requirements. Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment. Example: pkg/reconciler/sample/resources/receive_adapter.go</p> </li> <li> <p>Fetch the existing receive adapter deployment:</p> <pre><code>namespace := owner.GetObjectMeta().GetNamespace()\nra, err := r.KubeClientSet.AppsV1().Deployments(namespace).Get(expected.Name, metav1.GetOptions{})\n</code></pre> </li> <li> <p>If there is no existing receive adapter deployment, create one:</p> <pre><code>ra, err = r.KubeClientSet.AppsV1().Deployments(namespace).Create(expected)\n</code></pre> </li> <li> <p>Check if the expected spec is different from the existing spec, and update the deployment if required:</p> <pre><code>} else if r.podSpecImageSync(expected.Spec.Template.Spec, ra.Spec.Template.Spec) {\n    ra.Spec.Template.Spec = expected.Spec.Template.Spec\n    if ra, err = r.KubeClientSet.AppsV1().Deployments(namespace).Update(ra); err != nil {\n        return ra, err\n    }\n</code></pre> </li> <li> <p>If updated, record the event:</p> <pre><code>return pkgreconciler.NewEvent(corev1.EventTypeNormal, \"DeploymentUpdated\", \"updated deployment: \\\"%s/%s\\\"\", namespace, name)\n</code></pre> </li> <li> <p>If successful, update the <code>Status</code> and <code>MarkDeployed</code>:</p> <pre><code>src.Status.PropagateDeploymentAvailability(ra)\n</code></pre> </li> </ol> </li> <li> <p>Create a SinkBinding to bind the receive adapter with the sink.</p> <ol> <li> <p>Create a <code>Reference</code> for the receive adapter deployment. This deployment is the SinkBinding's source:</p> <pre><code>tracker.Reference{\n    APIVersion: appsv1.SchemeGroupVersion.String(),\n    Kind:       \"Deployment\",\n    Namespace:  ra.Namespace,\n    Name:       ra.Name,\n}\n</code></pre> </li> <li> <p>Fetch the existing SinkBinding:</p> <pre><code>namespace := owner.GetObjectMeta().GetNamespace()\nsb, err := r.EventingClientSet.SourcesV1alpha2().SinkBindings(namespace).Get(expected.Name, metav1.GetOptions{})\n</code></pre> </li> <li> <p>If there is no existing SinkBinding, create one:</p> <pre><code>sb, err = r.EventingClientSet.SourcesV1alpha2().SinkBindings(namespace).Create(expected)\n</code></pre> </li> <li> <p>Check if the expected spec is different to the existing spec, and update the SinkBinding if required:</p> <pre><code>else if r.specChanged(sb.Spec, expected.Spec) {\n    sb.Spec = expected.Spec\n    if sb, err = r.EventingClientSet.SourcesV1alpha2().SinkBindings(namespace).Update(sb); err != nil {\n        return sb, err\n    }\n</code></pre> </li> <li> <p>If updated, record the event:</p> <pre><code>return pkgreconciler.NewEvent(corev1.EventTypeNormal, \"SinkBindingUpdated\", \"updated SinkBinding: \\\"%s/%s\\\"\", namespace, name)\n</code></pre> </li> <li> <p><code>MarkSink</code> with the result:</p> <pre><code>src.Status.MarkSink(sb.Status.SinkURI)\n</code></pre> </li> </ol> </li> <li> <p>Return a new reconciler event stating that the process is done:</p> <pre><code>return pkgreconciler.NewEvent(corev1.EventTypeNormal, \"SampleSourceReconciled\", \"SampleSource reconciled: \\\"%s/%s\\\"\", namespace, name)\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/","title":"Using the Knative sample repository","text":"<p>The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter.</p>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#prerequisites","title":"Prerequisites","text":"<ul> <li>You are familiar with Kubernetes and Go development.</li> <li>You have installed Git.</li> <li>You have installed Go.</li> <li>You have cloned the <code>sample-source</code> repository.</li> </ul> <p>Optional:</p> <ul> <li>Install the <code>ko</code> CLI tool.</li> <li>Install the <code>kubectl</code> CLI tool.</li> <li>Set up minikube or kind.</li> </ul>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#sample-files-overview","title":"Sample files overview","text":""},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#receiver-adapter-files","title":"Receiver adapter files","text":"<ul> <li> <p><code>cmd/receive_adapter/main.go</code> - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system.</p> </li> <li> <p><code>pkg/adapter/adapter.go</code> - The functions that support receiver adapter translation of events to CloudEvents.</p> </li> </ul>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#controller-files","title":"Controller files","text":"<ul> <li> <p><code>cmd/controller/main.go</code> - Passes the event source's <code>NewController</code> implementation to the shared <code>main</code> method.</p> </li> <li> <p><code>pkg/reconciler/sample/controller.go</code> - The <code>NewController</code> implementation that is passed to the shared <code>main</code> method.</p> </li> </ul>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#crd-files","title":"CRD files","text":"<ul> <li><code>pkg/apis/samples/VERSION/samplesource_types.go</code> - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file.</li> </ul>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#reconciler-files","title":"Reconciler files","text":"<ul> <li> <p><code>pkg/reconciler/sample/samplesource.go</code> - The reconciliation functions for the receive adapter.</p> </li> <li> <p><code>pkg/apis/samples/VERSION/samplesource_lifecycle.go</code> - Contains status information for the event source\u2019s reconciliation details:</p> <ul> <li>Source ready</li> <li>Sink provided</li> <li>Deployed</li> <li>EventType provided</li> <li>Kubernetes resources correct</li> </ul> </li> </ul>"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#procedure","title":"Procedure","text":"<ol> <li> <p>Define the types required in the resource\u2019s schema in <code>pkg/apis/samples/v1alpha1/samplesource_types.go</code>.</p> <p>This includes the fields that are required in the resource YAML, as well as those referenced in the controller using the source\u2019s clientset and API:</p> <pre><code>// +genclient\n// +genreconciler\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// +k8s:openapi-gen=true\ntype SampleSource struct {\n    metav1.TypeMeta `json:\",inline\"`\n    // +optional\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    // Spec holds the desired state of the SampleSource (from the client).\n    Spec SampleSourceSpec `json:\"spec\"`\n\n    // Status communicates the observed state of the SampleSource (from the controller).\n    // +optional\n    Status SampleSourceStatus `json:\"status,omitempty\"`\n}\n\n// SampleSourceSpec holds the desired state of the SampleSource (from the client).\ntype SampleSourceSpec struct {\n    // inherits duck/v1 SourceSpec, which currently provides:\n    // * Sink - a reference to an object that will resolve to a domain name or\n    //   a URI directly to use as the sink.\n    // * CloudEventOverrides - defines overrides to control the output format\n    //   and modifications of the event sent to the sink.\n    duckv1.SourceSpec `json:\",inline\"`\n\n    // ServiceAccountName holds the name of the Kubernetes service account\n    // as which the underlying K8s resources should be run. If unspecified\n    // this will default to the \"default\" service account for the namespace\n    // in which the SampleSource exists.\n    // +optional\n    ServiceAccountName string `json:\"serviceAccountName,omitempty\"`\n\n    // Interval is the time interval between events.\n    //\n    // The string format is a sequence of decimal numbers, each with optional\n    // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time\n    // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified\n    // this will default to \"10s\".\n    Interval string `json:\"interval\"`\n}\n\n// SampleSourceStatus communicates the observed state of the SampleSource (from the controller).\ntype SampleSourceStatus struct {\n    duckv1.Status `json:\",inline\"`\n\n    // SinkURI is the current active sink URI that has been configured\n    // for the SampleSource.\n    // +optional\n    SinkURI *apis.URL `json:\"sinkUri,omitempty\"`\n}\n</code></pre> </li> <li> <p>Define the lifecycle to be reflected in the <code>status</code> and <code>SinkURI</code> fields:</p> <pre><code>const (\n    // SampleConditionReady has status True when the SampleSource is ready to send events.\n    SampleConditionReady = apis.ConditionReady\n    // ...\n)\n</code></pre> </li> <li> <p>Set the lifecycle conditions by defining the functions to be called from the reconciler functions. This is typically done in <code>pkg/apis/samples/VERSION/samplesource_lifecycle.go</code>:</p> <pre><code>// InitializeConditions sets relevant unset conditions to Unknown state.\nfunc (s *SampleSourceStatus) InitializeConditions() {\n    SampleCondSet.Manage(s).InitializeConditions()\n}\n\n...\n\n// MarkSink sets the condition that the source has a sink configured.\nfunc (s *SampleSourceStatus) MarkSink(uri *apis.URL) {\n    s.SinkURI = uri\n    if len(uri.String()) &gt; 0 {\n        SampleCondSet.Manage(s).MarkTrue(SampleConditionSinkProvided)\n    } else {\n        SampleCondSet.Manage(s).MarkUnknown(SampleConditionSinkProvided, \"SinkEmpty\", \"Sink has resolved to empty.%s\", \"\")\n    }\n}\n\n// MarkNoSink sets the condition that the source does not have a sink configured.\nfunc (s *SampleSourceStatus) MarkNoSink(reason, messageFormat string, messageA ...interface{}) {\n    SampleCondSet.Manage(s).MarkFalse(SampleConditionSinkProvided, reason, messageFormat, messageA...)\n}\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/sinkbinding/","title":"SinkBinding","text":"<p>The SinkBinding object supports decoupling event production from delivery addressing.</p> <p>You can use sink binding to direct a subject to a sink. A subject is a Kubernetes resource that embeds a PodSpec template and produces events. A sink is an addressable Kubernetes object that can receive events.</p> <p>The SinkBinding object injects environment variables into the PodTemplateSpec of the sink. Because of this, the application code does not need to interact directly with the Kubernetes API to locate the event destination. These environment variables are as follows:</p> <ul> <li><code>K_SINK</code> - The URL of the resolved sink.</li> <li><code>K_CE_OVERRIDES</code> - A JSON object that specifies overrides to the outbound   event.</li> </ul>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/","title":"Create a SinkBinding","text":"<p>This topic describes how to create a SinkBinding object. SinkBinding resolves a sink as a URI, sets the URI in the environment variable <code>K_SINK</code>, and adds the URI to a subject using <code>K_SINK</code>. If the URI changes, SinkBinding updates the value of <code>K_SINK</code>.</p> <p>In the following examples, the sink is a Knative Service and the subject is a CronJob. If you have an existing subject and sink, you can replace the examples with your own values.</p>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#before-you-begin","title":"Before you begin","text":"<p>Before you can create a SinkBinding object:</p> <ul> <li>You must have Knative Eventing installed on your cluster.</li> <li>Optional: If you want to use <code>kn</code> commands with SinkBinding, install the <code>kn</code> CLI.</li> </ul>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#optional-choose-sinkbinding-namespace-selection-behavior","title":"Optional: Choose SinkBinding namespace selection behavior","text":"<p>The SinkBinding object operates in one of two modes: <code>exclusion</code> or <code>inclusion</code>.</p> <p>The default mode is <code>exclusion</code>. In exclusion mode, SinkBinding behavior is enabled for the namespace by default. To disallow a namespace from being evaluated for mutation you must exclude it using the label <code>bindings.knative.dev/exclude: true</code>.</p> <p>In inclusion mode, SinkBinding behavior is not enabled for the namespace. Before a namespace can be evaluated for mutation, you must explicitly include it using the label <code>bindings.knative.dev/include: true</code>.</p> <p>To set the SinkBinding object to inclusion mode:</p> <ol> <li> <p>Change the value of <code>SINK_BINDING_SELECTION_MODE</code> from <code>exclusion</code> to <code>inclusion</code> by running:</p> <pre><code>kubectl -n knative-eventing set env deployments eventing-webhook --containers=\"eventing-webhook\" SINK_BINDING_SELECTION_MODE=inclusion\n</code></pre> </li> <li> <p>To verify that <code>SINK_BINDING_SELECTION_MODE</code> is set as desired, run:</p> <pre><code>kubectl -n knative-eventing set env deployments eventing-webhook --containers=\"eventing-webhook\" --list | grep SINK_BINDING\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-namespace","title":"Create a namespace","text":"<p>If you do not have an existing namespace, create a namespace for the SinkBinding object:</p> <p><pre><code>kubectl create namespace &lt;namespace&gt;\n</code></pre> Where <code>&lt;namespace&gt;</code> is the namespace that you want your SinkBinding to use. For example, <code>sinkbinding-example</code>.</p> <p>Note</p> <p>If you have selected inclusion mode, you must add the <code>bindings.knative.dev/include: true</code> label to the namespace to enable SinkBinding behavior.</p>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sink","title":"Create a sink","text":"<p>The sink can be any addressable Kubernetes object that can receive events.</p> <p>If you do not have an existing sink that you want to connect to the SinkBinding object, create a Knative service.</p> <p>Note</p> <p>To create a Knative service you must have Knative Serving installed on your cluster.</p> knYAML <p>Create a Knative service by running:</p> <p><pre><code>kn service create &lt;app-name&gt; --image &lt;image-url&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;app-name&gt;</code> is the name of the application.</li> <li><code>&lt;image-url&gt;</code> is the URL of the image container.</li> </ul> <p>For example:</p> <pre><code>$ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> <ol> <li> <p>Create a YAML file for the Knative service using the following template:</p> <p><pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: &lt;app-name&gt;\nspec:\n  template:\n    spec:\n      containers:\n        - image: &lt;image-url&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;app-name&gt;</code> is the name of the application. For example, <code>event-display</code>.</li> <li><code>&lt;image-url&gt;</code> is the URL of the image container. For example, <code>gcr.io/knative-releases/knative.dev/eventing/cmd/event_display</code>.</li> </ul> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-subject","title":"Create a subject","text":"<p>The subject must be a PodSpecable resource. You can use any PodSpecable resource in your cluster, for example:</p> <ul> <li><code>Deployment</code></li> <li><code>Job</code></li> <li><code>DaemonSet</code></li> <li><code>StatefulSet</code></li> <li><code>Service.serving.knative.dev</code></li> </ul> <p>If you do not have an existing PodSpecable subject that you want to use, you can use the following sample to create a CronJob object as the subject. The following CronJob makes a single cloud event that targets <code>K_SINK</code> and adds any extra overrides given by <code>CE_OVERRIDES</code>.</p> <ol> <li> <p>Create a YAML file for the CronJob using the following example:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: heartbeat-cron\nspec:\n  # Run every minute\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    metadata:\n      labels:\n        app: heartbeat-cron\n    spec:\n      template:\n        spec:\n          restartPolicy: Never\n          containers:\n            - name: single-heartbeat\n              image: gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats\n              args:\n              - --period=1\n              env:\n                - name: ONE_SHOT\n                  value: \"true\"\n                - name: POD_NAME\n                  valueFrom:\n                    fieldRef:\n                      fieldPath: metadata.name\n                - name: POD_NAMESPACE\n                  valueFrom:\n                    fieldRef:\n                      fieldPath: metadata.namespace\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sinkbinding-object","title":"Create a SinkBinding object","text":"<p>Create a <code>SinkBinding</code> object that directs events from your subject to the sink.</p> knYAML <p>Create a <code>SinkBinding</code> object by running:</p> <p><pre><code>kn source binding create &lt;name&gt; \\\n  --namespace &lt;namespace&gt; \\\n  --subject \"&lt;subject&gt;\" \\\n  --sink &lt;sink&gt; \\\n  --ce-override \"&lt;cloudevent-overrides&gt;\"\n</code></pre> Where:</p> <ul> <li><code>&lt;name&gt;</code> is the name of the SinkBinding object you want to create.</li> <li><code>&lt;namespace&gt;</code> is the namespace you created for your SinkBinding to use.</li> <li><code>&lt;subject&gt;</code> is the subject to connect. Examples:<ul> <li><code>Job:batch/v1:app=heartbeat-cron</code> matches all jobs in namespace with label <code>app=heartbeat-cron</code>.</li> <li><code>Deployment:apps/v1:myapp</code> matches a deployment called <code>myapp</code> in the namespace.</li> <li><code>Service:serving.knative.dev/v1:hello</code> matches the service called <code>hello</code>.</li> </ul> </li> <li><code>&lt;sink&gt;</code> is the sink to connect. For example <code>http://event-display.svc.cluster.local</code>.</li> <li>Optional: <code>&lt;cloudevent-overrides&gt;</code> in the form <code>key=value</code>. Cloud Event overrides control the output format and modifications of the event sent to the sink and are applied before sending the event. You can provide this flag multiple times.</li> </ul> <p>For a list of available options, see the Knative client documentation.</p> <p>For example: <pre><code>$ kn source binding create bind-heartbeat \\\n  --namespace sinkbinding-example \\\n  --subject \"Job:batch/v1:app=heartbeat-cron\" \\\n  --sink http://event-display.svc.cluster.local \\\n  --ce-override \"sink=bound\"\n</code></pre></p> <ol> <li> <p>Create a YAML file for the <code>SinkBinding</code> object using the following template:</p> <p><pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: &lt;name&gt;\nspec:\n  subject:\n    apiVersion: &lt;api-version&gt;\n    kind: &lt;kind&gt;\n    selector:\n      matchLabels:\n        &lt;label-key&gt;: &lt;label-value&gt;\n  sink:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: &lt;sink&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;name&gt;</code> is the name of the SinkBinding object you want to create. For example, <code>bind-heartbeat</code>.</li> <li><code>&lt;api-version&gt;</code> is the API version of the subject. For example <code>batch/v1</code>.</li> <li><code>&lt;kind&gt;</code> is the Kind of your subject. For example <code>Job</code>.</li> <li><code>&lt;label-key&gt;: &lt;label-value&gt;</code> is a map of key-value pairs to select subjects that have a matching label. For example, <code>app: heartbeat-cron</code> selects any subject with the label <code>app=heartbeat-cron</code>.</li> <li><code>&lt;sink&gt;</code> is the sink to connect. For example <code>event-display</code>.</li> </ul> <p>For more information about the fields you can configure for the SinkBinding object, see Sink Binding Reference.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#verify-the-sinkbinding-object","title":"Verify the SinkBinding object","text":"<ol> <li> <p>Verify that a message was sent to the Knative eventing system by looking at the service logs for your sink:</p> <p><pre><code>kubectl logs -l &lt;sink&gt; -c &lt;container&gt; --since=10m\n</code></pre> Where:</p> <ul> <li><code>&lt;sink&gt;</code> is the name of your sink.</li> <li><code>&lt;container&gt;</code> is the name of the container your sink is running in.</li> </ul> <p>For example: <pre><code>$ kubectl logs -l serving.knative.dev/service=event-display -c user-container --since=10m\n</code></pre></p> </li> <li> <p>From the output, observe the lines showing the request headers and body of the event message, sent by the source to the display function. For example:</p> <pre><code>  \u2601\ufe0f  cloudevents.Event\n  Validation: valid\n  Context Attributes,\n    specversion: 1.0\n    type: dev.knative.eventing.samples.heartbeat\n    source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz\n    id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687\n    time: 2020-02-19T13:47:10.41428688Z\n    datacontenttype: application/json\n  Extensions,\n    beats: true\n    heart: yes\n    the: 42\n  Data,\n    {\n      \"id\": 1,\n      \"label\": \"\"\n    }\n</code></pre> </li> </ol>"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#delete-a-sinkbinding","title":"Delete a SinkBinding","text":"<p>To delete the SinkBinding object and all of the related resources in the namespace, delete the namespace by running:</p> <p><pre><code>kubectl delete namespace &lt;namespace&gt;\n</code></pre> Where <code>&lt;namespace&gt;</code> is the name of the namespace that contains the SinkBinding object.</p>"},{"location":"eventing/custom-event-source/sinkbinding/reference/","title":"SinkBinding reference","text":"<p>This topic provides reference information about the configurable parameters for SinkBinding objects.</p>"},{"location":"eventing/custom-event-source/sinkbinding/reference/#supported-parameters","title":"Supported parameters","text":"<p>A <code>SinkBinding</code> resource supports the following parameters:</p> Field Description Required or optional <code>apiVersion</code> Specifies the API version, for example <code>sources.knative.dev/v1</code>. Required <code>kind</code> Identifies this resource object as a <code>SinkBinding</code> object. Required <code>metadata</code> Specifies metadata that uniquely identifies the <code>SinkBinding</code> object. For example, a <code>name</code>. Required <code>spec</code> Specifies the configuration information for this <code>SinkBinding</code> object. Required <code>spec.sink</code> A reference to an object that resolves to a URI to use as the sink. Required <code>spec.subject</code> A reference to the resources for which the \"runtime contract\" is augmented by Binding implementations. Required <code>spec.ceOverrides</code> Defines overrides to control the output format and modifications to the event sent to the sink. Optional"},{"location":"eventing/custom-event-source/sinkbinding/reference/#subject-parameter","title":"Subject parameter","text":"<p>The Subject parameter references the resources for which the \"runtime contract\" is augmented by Binding implementations.</p> <p>A <code>subject</code> definition supports the following fields:</p> Field Description Required or optional <code>apiVersion</code> API version of the referent. Required <code>kind</code> Kind of the referent. Required <code>namespace</code> Namespace of the referent. If omitted, this defaults to the object holding it. Optional <code>name</code> Name of the referent. Do not use if you configure <code>selector</code>. <code>selector</code> Selector of the referents. Do not use if you configure <code>name</code>. <code>selector.matchExpressions</code> A list of label selector requirements. The requirements are ANDed. Use one of <code>matchExpressions</code> or <code>matchLabels</code> <code>selector.matchExpressions.key</code> The label key that the selector applies to. Required if using <code>matchExpressions</code> <code>selector.matchExpressions.operator</code> Represents a key's relationship to a set of values. Valid operators are <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code>. Required if using <code>matchExpressions</code> <code>selector.matchExpressions.values</code> An array of string values. If <code>operator</code> is <code>In</code> or <code>NotIn</code>, the values array must be non-empty. If <code>operator</code> is <code>Exists</code> or <code>DoesNotExist</code>, the values array must be empty. This array is replaced during a strategic merge patch. Required if using <code>matchExpressions</code> <code>selector.matchLabels</code> A map of key-value pairs. Each key-value pair in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>, where the key field is <code>matchLabels.&lt;key&gt;</code>, the <code>operator</code> is <code>In</code>, and the <code>values</code> array contains only \"matchLabels.\". The requirements are ANDed. Use one of <code>matchExpressions</code> or <code>matchLabels</code>"},{"location":"eventing/custom-event-source/sinkbinding/reference/#subject-parameter-examples","title":"Subject parameter examples","text":"<p>Given the following YAML, the <code>Deployment</code> named <code>mysubject</code> in the <code>default</code> namespace is selected:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: bind-heartbeat\nspec:\n  subject:\n    apiVersion: apps/v1\n    kind: Deployment\n    namespace: default\n    name: mysubject\n  ...\n</code></pre> <p>Given the following YAML, any <code>Job</code> with the label <code>working=example</code> in the <code>default</code> namespace is selected:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: bind-heartbeat\nspec:\n  subject:\n    apiVersion: batch/v1\n    kind: Job\n    namespace: default\n    selector:\n      matchLabels:\n        working: example\n  ...\n</code></pre> <p>Given the following YAML, any <code>Pod</code> with the label <code>working=example</code> OR <code>working=sample</code> in the <code>default</code> namespace is selected:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: bind-heartbeat\nspec:\n  subject:\n    apiVersion: v1\n    kind: Pod\n    namespace: default\n    selector:\n      - matchExpression:\n        key: working\n        operator: In\n        values:\n          - example\n          - sample\n  ...\n</code></pre>"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides","title":"CloudEvent Overrides","text":"<p>CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink.</p> <p>A <code>ceOverrides</code> definition supports the following fields:</p> Field Description Required or optional <code>extensions</code> Specifies which attributes are added or overridden on the outbound event. Each <code>extensions</code> key-value pair is set independently on the event as an attribute extension. Optional <p>Note</p> <p>Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the <code>type</code> attribute.</p>"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides-example","title":"CloudEvent Overrides example","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: bind-heartbeat\nspec:\n  ...\n  ceOverrides:\n    extensions:\n      extra: this is an extra attribute\n      additional: 42\n</code></pre> <p>Contract</p> <p>This results in the <code>K_CE_OVERRIDES</code> environment variable being set on the <code>subject</code> as follows: <pre><code>{ \"extensions\": { \"extra\": \"this is an extra attribute\", \"additional\": \"42\" } }\n</code></pre></p>"},{"location":"eventing/faq/","title":"Eventing FAQs","text":""},{"location":"eventing/faq/#what-is-a-sugar-controller","title":"What is a Sugar Controller?","text":"<p>The Sugar Controller is an optional controller which will automatically create and clean up brokers based on the creation/deletion of namespaces and triggers. For more information on how to enable and configure the sugar controller, read the sugar controller documentation.</p>"},{"location":"eventing/features/","title":"Eventing Features","text":"<p>To keep Knative innovative, the maintainers of this project have developed an experimental features process that allows new, experimental features to be delivered and tested by users without affecting the stability of the core project.</p> <p>Warning</p> <p>Features are stable and unstable features and may cause issues in your Knative setup or even your cluster setup. These features should be used with caution, and should never be tested on a production environment. For more information about quality guarantees for features at different stages of development, see the Feature stage definition documentation.</p> <p>This document explains how to enable features and which ones are available today.</p>"},{"location":"eventing/features/#before-you-begin","title":"Before you begin","text":"<p>You must have a Knative cluster running with Knative Eventing installed.</p>"},{"location":"eventing/features/#features-configuration","title":"Features configuration","text":"<p>When you install Knative Eventing, the <code>config-features</code> ConfigMap is added to your cluster in the <code>knative-eventing</code> namespace.</p> <p>To enable a feature, you must add it to the <code>config-features</code> ConfigMap under the <code>data</code> spec, and set the value for the feature to <code>enabled</code>. For example, to enable a feature called <code>new-cool-feature</code>, you would add the following ConfigMap entry:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-features\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/config-category: eventing\ndata:\n  new-cool-feature: enabled\n</code></pre> <p>To disable it, you can either remove the flag or set it to <code>disabled</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-features\n  namespace: knative-eventing\n  labels:\n    eventing.knative.dev/release: devel\n    knative.dev/config-category: eventing\ndata:\n  new-cool-feature: disabled\n</code></pre>"},{"location":"eventing/features/#available-features","title":"Available Features","text":"<p>The following table gives an overview of the available features in Knative Eventing:</p> Feature Flag Description Maturity DeliverySpec.RetryAfterMax field <code>delivery-retryafter</code> Specify a maximum retry duration that overrides HTTP Retry-After headers when calculating backoff times for retrying 429 and 503 responses. Alpha, disabled by default DeliverySpec.Timeout field <code>delivery-timeout</code> When using the <code>delivery</code> spec to configure event delivery parameters, you can use  the<code>timeout</code> field to specify the timeout for each sent HTTP request. Beta, enabled by default KReference.Group field <code>kreference-group</code> Specify the API <code>group</code> of <code>KReference</code> resources without the API version. Alpha, disabled by default Knative reference mapping <code>kreference-mapping</code> Provide mappings from a Knative reference to a templated URI. Alpha, disabled by default New trigger filters <code>new-trigger-filters</code> Enables a new Trigger <code>filters</code> field that supports a set of powerful filter expressions. Beta, enabled by default Transport encryption <code>transport-encryption</code> Enables components to encrypt traffic using TLS by exposing HTTPS URL. Beta, disabled by default Sender Identity <code>authentication-oidc</code> Enables Eventing sources to send authenticated requests and addressables to require authenticated requests. Alpha, disabled by default Eventing with Istio <code>istio</code> Enables Eventing components to communicate with workloads in an Istio mesh. Beta, disabled by default"},{"location":"eventing/features/delivery-retryafter/","title":"DeliverySpec.RetryAfterMax field","text":"<p>Flag name: <code>delivery-retryafter</code></p> <p>Stage: Alpha, disabled by default</p> <p>Tracking issue: #5811</p> <p>Persona: Developer</p> <p>When using the <code>delivery</code> spec to configure event delivery parameters, you can use the <code>retryAfterMax</code> field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a <code>delivery</code> spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the <code>delivery</code> field.</p> <p>The <code>retryAfterMax</code> field only takes effect if you configure the <code>delivery</code> spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of <code>PT0S</code> effectively disables Retry-After support.</p> <p>Prior to this feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support.  To begin, the feature is opt-in, but the final state will be opt-out as follows:</p> Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation &amp; Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted  by Webhook Validation &amp; Retry-After headers NOT enforced Accepted by Webhook Validation &amp; Retry-After headers enforced if max override &gt; 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override &gt; 0 <p>The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds:</p> <pre><code>apiVersion: messaging.knative.dev/v1\nkind: Subscription\nmetadata:\n  name: example-subscription\n  namespace: example-namespace\nspec:\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: example-sink\n  delivery:\n    backoffDelay: PT2S\n    backoffPolicy: linear\n    retry: 3\n    retryAfterMax: PT120S\n</code></pre> <p>Note</p> <p>While the feature flag enforces all DeliverySpec usage of the <code>retryAfterMax</code> field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field.  The shared <code>HTTPMessageSender.SendWithRetries()</code> logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit.  Extensions implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the <code>retryAfterMax</code> field.</p>"},{"location":"eventing/features/delivery-timeout/","title":"DeliverySpec.Timeout field","text":"<p>Flag name: <code>delivery-timeout</code></p> <p>Stage: Beta, enabled by default</p> <p>Tracking issue: #5148</p> <p>Persona: Developer</p> <p>When using the <code>delivery</code> spec to configure event delivery parameters, you can use <code>timeout</code> field to specify the timeout for each sent HTTP request. The duration of the <code>timeout</code> parameter is specified using the ISO 8601 format.</p> <p>The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds:</p> <pre><code>apiVersion: messaging.knative.dev/v1\nkind: Subscription\nmetadata:\n  name: example-subscription\n  namespace: example-namespace\nspec:\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: example-sink\n  delivery:\n    backoffDelay: PT2S\n    backoffPolicy: linear\n    retry: 3\n    timeout: PT5S\n</code></pre> <p>You can specify a <code>delivery</code> spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the <code>delivery</code> field.</p>"},{"location":"eventing/features/eventtype-auto-creation/","title":"EventType auto creation for improved discoverability","text":"<p>Flag name: <code>eventtype-auto-create</code></p> <p>Stage: Alpha, disabled by default</p> <p>Tracking issue: #7044</p> <p>Persona: Developer</p>"},{"location":"eventing/features/eventtype-auto-creation/#overview","title":"Overview","text":"<p>With the <code>eventtype-auto-creation</code> feature, we have possibliy to auto create EventTypes that are received and ingressed by the Knative Broker and Channel implementations.</p> <p>For making use of this opt-in feature, we must turn it on in the <code>config-features</code>, by setting the <code>eventtype-auto-create</code> flag to <code>enabled</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-features\n  namespace: knative-eventing\ndata:\n  eventtype-auto-create: \"enabled\"\n...\n</code></pre> <p>With this feature enabled, we get <code>EventType</code>s on the broker/channel ingress for free, instead of manually creating them as yaml manifests along the application code that talks to the <code>Broker</code> or <code>Channel</code> API. </p>"},{"location":"eventing/features/eventtype-auto-creation/#example","title":"Example","text":""},{"location":"eventing/features/eventtype-auto-creation/#create-a-broker","title":"Create a Broker","text":"<p>To check the feature is working, create a simple broker:</p> kn CLIApply YAML <pre><code>kn broker create my-broker\n</code></pre> <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  namespace: default\n  name: my-broker\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre>   Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/features/eventtype-auto-creation/#produce-events-to-the-broker","title":"Produce Events to the Broker","text":"<p>The auto-creation feature is triggered by processed events. Therefore to verify the functionality we need to send a sample event with desired type. This can be achieved in a severals ways, below are two examples using <code>kn-plugin-event</code> and <code>cURL</code> container in a cluster.</p> kn eventcURL container <p>Below is an example that sends an event with the <code>kn</code> CLI, for improved developer productivity. The <code>kn-plugin-event</code> plugin can be installed with Homebrew or downloaded directly from GitHub releases, for more details please refer to plugin instalation steps.</p> <ol> <li> <p>Setup <code>kn event</code> plugin     <pre><code>brew install knative-extensions/kn-plugins/event\n</code></pre></p> </li> <li> <p>Send event     <pre><code>kn event send \\\n  --to Broker:eventing.knative.dev/v1:my-broker\\\n  --type com.corp.integration.warning \\\n  -f message=\"There might be a problem\"\n</code></pre></p> </li> </ol> <p>An event can be send via <code>curl</code> in a cluster:</p> <p><pre><code>kubectl run curl  --image=docker.io/curlimages/curl --rm=true --restart=Never -ti \\\n  -- -X POST -v \\\n  -H \"content-type: application/json\" \\\n  -H \"ce-specversion: 1.0\" \\\n  -H \"ce-source: my/curl/command\" \\\n  -H \"ce-type: my.demo.event\" \\\n  -H \"ce-id: 6cf17c7b-30b1-45a6-80b0-4cf58c92b947\" \\\n  -d '{\"name\":\"Knative Demo\"}' \\\n  http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker\n</code></pre> This is more complex, as we have to craft the event as part of the <code>curl</code> HTTP POST request.</p>"},{"location":"eventing/features/eventtype-auto-creation/#event-discovery","title":"Event Discovery","text":"<p>After the two produced events, we should be able to have discoverable events in the system, based on the <code>eventtype-auto-creation</code> feature:</p> <pre><code>k get eventtypes.eventing.knative.dev -A \nNAMESPACE   NAME     TYPE                           SOURCE            SCHEMA   BROKER      DESCRIPTION   READY   REASON\ndefault     &lt;...&gt;    com.corp.integration.warning   kn-event/v1.9.0            my-broker                 True    \ndefault     &lt;...&gt;    my.demo.event                  my/curl/command            my-broker                 True    \n</code></pre>"},{"location":"eventing/features/eventtype-auto-creation/#conclusion-and-recommendation","title":"Conclusion and Recommendation","text":"<p>With out this feature we would not see the two <code>EventType</code> instances in the system, so we have improved the discoverablilty of events, for consumption. However while this opt-in feature is handy for automatic event creation, we strongly recommend to create the actual <code>EventType</code> manifests for all events that your application <code>deployments</code> produce, as part of your Gitops pipeline, rather than relying on this auto-create feature.</p>"},{"location":"eventing/features/istio-integration/","title":"Eventing integration with Istio service mesh","text":"<p>Flag name: <code>istio</code></p> <p>Stage: Beta, disabled by default</p> <p>Tracking issue: #6596</p>"},{"location":"eventing/features/istio-integration/#overview","title":"Overview","text":"<p>Administrators can use Istio with Eventing to encrypt, authenticate and authorize requests to Eventing components.</p>"},{"location":"eventing/features/istio-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>In order to enable the istio integration, you will need to install Istio by   following the Istio installation guides.</li> </ul>"},{"location":"eventing/features/istio-integration/#installation","title":"Installation","text":"<p>Some Eventing components use services of type <code>ExternalName</code> and with such services, Istio need to be manually configured to connect to such services using mutual TLS.</p> <p>Eventing releases a controller that automatically configures Istio so that any pod that is part of an Istio mesh can communicate with Eventing components that are also part of the same Istio mesh.</p> <ol> <li>Create the Eventing namespace and enable Istio injection:     <pre><code>kubectl create namespace knative-eventing --dry-run=client -oyaml | kubectl apply -f -\nkubectl label namespace knative-eventing istio-injection=enabled\n</code></pre></li> <li> <p>Follow Eventing installation</p> </li> <li> <p>Install <code>eventing-istio-controller</code>:     <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-istio/latest/eventing-istio.yaml\n</code></pre></p> </li> <li>Verify <code>eventing-istio-controller</code> is ready:     <pre><code>kubectl get deployment -n knative-eventing\n</code></pre>    Example output:     <pre><code>NAME                        ...   READY\neventing-istio-controller   ...   True \n# other deployments omitted ...\n</code></pre></li> </ol>"},{"location":"eventing/features/istio-integration/#enable-istio-integration","title":"Enable istio integration","text":"<p>The <code>istio</code> feature flag is an enum configuration that configures the <code>eventing-istio-controller</code> to create Istio resources for Eventing resources.</p> <p>The possible values for <code>istio</code> are:</p> <ul> <li><code>disabled</code><ul> <li>Disable Eventing integration with Istio</li> </ul> </li> <li><code>enabled</code><ul> <li>Enabled Eventing integration with Istio</li> </ul> </li> </ul> <p>For example, to enable <code>istio</code> integration, the <code>config-features</code> ConfigMap will look like the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-features\n  namespace: knative-eventing\ndata:\n  istio: \"enabled\"\n</code></pre>"},{"location":"eventing/features/kreference-group/","title":"KReference.Group field","text":"<p>Flag name: <code>kreference-group</code></p> <p>Stage: Alpha, disabled by default</p> <p>Tracking issue: #5086</p> <p>Persona: Developer</p> <p>When using the <code>KReference</code> type to refer to another Knative resource, you can just specify the API <code>group</code> of the resource, instead of the full <code>APIVersion</code>.</p> <p>For example, in order to refer to an <code>InMemoryChannel</code>, instead of the following spec:</p> <pre><code>apiVersion: messaging.knative.dev/v1\nkind: InMemoryChannel\nname: my-channel\n</code></pre> <p>You can use the following:</p> <pre><code>group: messaging.knative.dev\nkind: InMemoryChannel\nname: my-channel\n</code></pre> <p>With this feature you can allow Knative to resolve the full <code>APIVersion</code> and further upgrades, deprecations and removals of the referred CRD without affecting existing resources.</p> <p>Note</p> <p>At the moment this feature is implemented only for <code>Subscription.Spec.Subscriber.Ref</code> and <code>Subscription.Spec.Channel</code>.</p>"},{"location":"eventing/features/kreference-mapping/","title":"Knative reference mapping","text":"<p>Flag name: <code>kreference-mapping</code></p> <p>Stage: Alpha, disabled by default</p> <p>Tracking issue: #5593</p> <p>Persona: Administrator, Developer</p> <p>When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI.</p> <p>Note</p> <p>Currently only PingSource supports this feature.</p> <p>For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber.</p> <p>Mappings are defined by a cluster administrator in the <code>config-reference-mapping</code> ConfigMap. The following example maps <code>JobDefinition</code> to a Job runner service:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-kreference-mapping\n  namespace: knative-eventing\ndata:\n  JobDefinition.v1.mygroup: \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\"\n</code></pre> <p>The key must be of the form <code>&lt;Kind&gt;.&lt;version&gt;.&lt;group&gt;</code>. The value must resolved to a valid URI. Currently, the following template data are supported:</p> <ul> <li>Name: The name of the referenced object</li> <li>Namespace: The namespace of the referenced object</li> <li>UID: The UID of the referenced object</li> <li>SystemNamespace: The namespace of where Knative Eventing is installed</li> </ul> <p>Given the above mapping, the following example shows how you can directly reference <code>JobDefinition</code> objects in a PingSource:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: trigger-job-every-minute\nspec:\n  schedule: \"*/1 * * * *\"\n  sink:\n    ref:\n      apiVersion: mygroup/v1\n      kind: JobDefinition\n      name: ajob\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/","title":"New trigger filters","text":"<p>Flag name: <code>new-apiserversource-filters</code></p> <p>Stage: Alpha, disabled by default</p> <p>Tracking issue: #7791</p>"},{"location":"eventing/features/new-apiserversource-filters/#overview","title":"Overview","text":"<p>This feature enables a new <code>filters</code> field in APIServerSource that conforms to the filters API field defined in the <code>CloudEvents Subscriptions API</code>. It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event.</p> <p>The following example shows a APIServerSource using the new <code>filters</code> field:</p> <pre><code>---\napiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: my-apiserversource\n namespace: default\nspec:\n  filters:\n  - any:\n    - exact:\n        type: dev.knative.apiserver.ref.add\n\n  serviceAccountName: apiserversource\n  mode: Reference\n  resources: ...\n  sink: ...\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#about-the-filters-field","title":"About the filters field","text":"<ul> <li>An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the <code>sink</code>.</li> <li>Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression.</li> </ul>"},{"location":"eventing/features/new-apiserversource-filters/#supported-filter-dialects","title":"Supported filter dialects","text":"<p>The <code>filters</code> field supports the following dialects:</p>"},{"location":"eventing/features/new-apiserversource-filters/#exact","title":"<code>exact</code>","text":"<p>CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - exact:\n        type: com.github.push\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#prefix","title":"<code>prefix</code>","text":"<p>CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - prefix:\n        type: com.github.\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#suffix","title":"<code>suffix</code>","text":"<p>CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - suffix:\n        type: .created\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#all","title":"<code>all</code>","text":"<p>All nested filter expressions must evaluate to true.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - all:\n        - exact:\n            type: com.github.push\n        - exact:\n            subject: https://github.com/cloudevents/spec\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#any","title":"<code>any</code>","text":"<p>At least one nested filter expression must evaluate to true.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - any:\n        - exact:\n            type: com.github.push\n        - exact:\n            subject: https://github.com/cloudevents/spec\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#not","title":"<code>not</code>","text":"<p>The nested expression evaluated must evaluate to false.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n      - not:\n          exact:\n              type: com.github.push\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#cesql","title":"<code>cesql</code>","text":"<p>The provided CloudEvents SQL Expression must evaluate to true.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: APIServerSource\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - cesql: \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\"\n</code></pre>"},{"location":"eventing/features/new-apiserversource-filters/#faq","title":"FAQ","text":""},{"location":"eventing/features/new-apiserversource-filters/#which-are-the-event-types-apiserversource-provide","title":"Which are the event types APIServerSource provide?","text":"<ul> <li><code>dev.knative.apiserver.resource.add</code></li> <li><code>dev.knative.apiserver.resource.update</code></li> <li><code>dev.knative.apiserver.resource.delete</code></li> <li><code>dev.knative.apiserver.ref.add</code></li> <li><code>dev.knative.apiserver.ref.update</code></li> <li><code>dev.knative.apiserver.ref.delete</code></li> </ul>"},{"location":"eventing/features/new-trigger-filters/","title":"New trigger filters","text":"<p>Flag name: <code>new-trigger-filters</code></p> <p>Stage: Beta, enabled by default</p> <p>Tracking issue: #5204</p>"},{"location":"eventing/features/new-trigger-filters/#overview","title":"Overview","text":"<p>This feature enables a new <code>filters</code> field in Triggers that conforms to the filters API field defined in the <code>CloudEvents Subscriptions API</code>. It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event.</p> <p>The following example shows a Trigger using the new <code>filters</code> field:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\nspec:\n  broker: default\n  filters:\n    - cesql: \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\"\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: my-service\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#about-the-filters-field","title":"About the filters field","text":"<ul> <li>An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the <code>subscriber</code>.</li> <li>Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression.</li> </ul>"},{"location":"eventing/features/new-trigger-filters/#supported-filter-dialects","title":"Supported filter dialects","text":"<p>The <code>filters</code> field supports the following dialects:</p>"},{"location":"eventing/features/new-trigger-filters/#exact","title":"<code>exact</code>","text":"<p>CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - exact:\n        type: com.github.push\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#prefix","title":"<code>prefix</code>","text":"<p>CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - prefix:\n        type: com.github.\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#suffix","title":"<code>suffix</code>","text":"<p>CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - suffix:\n        type: .created\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#all","title":"<code>all</code>","text":"<p>All nested filter expressions must evaluate to true.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - all:\n        - exact:\n            type: com.github.push\n        - exact:\n            subject: https://github.com/cloudevents/spec\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#any","title":"<code>any</code>","text":"<p>At least one nested filter expression must evaluate to true.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - any:\n        - exact:\n            type: com.github.push\n        - exact:\n            subject: https://github.com/cloudevents/spec\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#not","title":"<code>not</code>","text":"<p>The nested expression evaluated must evaluate to false.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n      - not:\n          exact:\n              type: com.github.push\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#cesql","title":"<code>cesql</code>","text":"<p>The provided CloudEvents SQL Expression must evaluate to true.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  ...\nspec:\n  ...\n  filters:\n    - cesql: \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\"\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#conflict-with-the-current-filter-field","title":"Conflict with the current <code>filter</code> field","text":"<p>The current <code>filter</code> field will continue to be supported. However, if you enable this feature and an object includes both <code>filter</code> and <code>filters</code>, the new <code>filters</code> field overrides the <code>filter</code> field. This allows you to try the new <code>filters</code> field without compromising existing filters, and you can introduce it to existing <code>Trigger</code> objects gradually.</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\nspec:\n  broker: default\n  # Current filter field. Will be ignored.\n  filter:\n    attributes:\n      type: dev.knative.foo.bar\n      myextension: my-extension-value\n  # Enhanced filters field. This will override the old filter field.\n  filters:\n    - cesql: \"type = 'dev.knative.foo.bar' AND myextension = 'my-extension-value'\"\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: my-service\n</code></pre>"},{"location":"eventing/features/new-trigger-filters/#faq","title":"FAQ","text":""},{"location":"eventing/features/new-trigger-filters/#why-add-yet-another-field-why-not-make-the-current-filter-field-more-robust","title":"Why add yet another field? Why not make the current <code>filter</code> field more robust?","text":"<p>The reason is twofold. First, at the time of developing <code>Trigger</code> APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old <code>filter</code> field, and give them the possibility to transition to the new <code>filters</code> field.</p>"},{"location":"eventing/features/new-trigger-filters/#why-filters-and-not-another-name-that-wouldnt-conflict-with-the-filter-field","title":"Why <code>filters</code> and not another name that wouldn't conflict with the <code>filter</code> field?","text":"<p>We considered other names, such as <code>cefilters</code>, <code>subscriptionsAPIFilters</code>, or <code>enhancedFilters</code>, but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being a feature.</p>"},{"location":"eventing/features/sender-identity/","title":"Sender Identity for Knative Eventing","text":"<p>Flag name: <code>authentication-oidc</code></p> <p>Stage: Alpha, disabled by default</p> <p>Tracking issue: #6806</p>"},{"location":"eventing/features/sender-identity/#overview","title":"Overview","text":"<p>Currently, event delivery within the cluster is unauthenticated, and an addressable event consumer cannot determine the identity of any sender.</p> <p>Knative Eventing Addressables expose their OIDC Audience in their status as part of their address (e.g. <code>.status.address.audience</code>) and require requests containing an OIDC access token issued for this audience.</p> <p>Knative Eventing Souces request an OIDC access token for the targets audience and add them to the request. A per-source dedicated Service Account is used as the identity for the request.</p>"},{"location":"eventing/features/sender-identity/#prerequisites","title":"Prerequisites","text":"<ul> <li>Eventing installation</li> </ul> <p>Note</p> <p>To not provide the access token in cleartext over the wire, transport-encryption should be enabled as well. Take a look at Transport-Encryption, which explains how to enable the transport encryption feature flag.</p>"},{"location":"eventing/features/sender-identity/#compatibility","title":"Compatibility","text":"<p>OIDC authentication is currently supported for the following components:</p> <ul> <li>Brokers:<ul> <li>MTChannelBasedBroker</li> <li>Knative Broker for Apache Kafka</li> </ul> </li> <li>Channels:<ul> <li>InMemoryChannel</li> <li>KafkaChannel</li> </ul> </li> <li>Sources:<ul> <li>ApiServerSource</li> <li>PingSource</li> <li>KafkaSource</li> </ul> </li> </ul>"},{"location":"eventing/features/sender-identity/#sender-identity-configuration","title":"Sender Identity Configuration","text":"<p>The possible values for <code>authentication-oidc</code> are:</p> <ul> <li><code>disabled</code><ul> <li>No change in behaviour</li> </ul> </li> <li><code>enabled</code><ul> <li>Addressables announce their audience in their status</li> <li>Sources add an Authorization Header to their request containing an access token for their target</li> </ul> </li> </ul> <p>For example, to enable sender identity, the <code>config-features</code> ConfigMap will look like the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-features\n  namespace: knative-eventing\ndata:\n  authentication-oidc: \"enabled\"\n</code></pre>"},{"location":"eventing/features/sender-identity/#verifying-that-the-feature-is-working","title":"Verifying that the feature is working","text":"<p>Save the following YAML into a file called <code>default-broker-example.yaml</code></p> <pre><code># default-broker-example.yaml\n\napiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: br\n\n---\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: tr\nspec:\n  broker: br\n  subscriber:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: event-display\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: event-display\nspec:\n  selector:\n    app: event-display\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: event-display\n  labels:\n    app: event-display\nspec:\n  containers:\n    - name: event-display\n      image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n      imagePullPolicy: Always\n      ports:\n        - containerPort: 8080\n</code></pre> <p>Apply the <code>default-broker-example.yaml</code> file into a test namespace <code>authentication-oidc-test</code>:</p> <pre><code>kubectl create namespace authentication-oidc-test\n\nkubectl apply -n authentication-oidc-test -f default-broker-example.yaml\n</code></pre> <p>Verify that the Broker announces its audience: <pre><code>kubectl -n authentication-oidc-test get broker br -o yaml\n</code></pre></p> <p>Example output:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: br\n  namespace: authentication-oidc-test\nspec:\n  config:\n    # ...\n  delivery:\n    # ...\nstatus:\n  address:\n    audience: eventing.knative.dev/broker/authentication-oidc-test/br\n    name: http\n    url: http://broker-ingress.knative-eventing.svc.cluster.local/authentication-oidc-test/br\n  annotations:\n  # ...\n</code></pre> <p>Send events to the Broker using OIDC authentication:</p> <ol> <li> <p>Create an OIDC token (access token):     <pre><code>kubectl -n authentication-oidc-test create serviceaccount oidc-test-user; kubectl -n authentication-oidc-test create token oidc-test-user --audience eventing.knative.dev/broker/authentication-oidc-test/br\n</code></pre></p> <p>Example output: <pre><code>serviceaccount/oidc-test-user created\neyJhbGciOiJSUzI1NiIsImtpZCI6IlZBWmppNEVJZkVSVDZoYTA4dU1xTWJxSHFYQTgtbE00VU1tMmpFZUNuakUifQ.eyJhdWQiOlsiZXZlbnRpbmcua25hdGl2ZS5kZXYvYnJva2VyL2F1dGhlbnRpY2F0aW9uLW9pZGMtdGVzdC9iciJdLCJleHAiOjE3MDU5MzQyMTQsImlhdCI6MTcwNTkzMDYxNCwiaXNzIjoiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJhdXRoZW50aWNhdGlvbi1vaWRjLXRlc3QiLCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoib2lkYy10ZXN0LXVzZXIiLCJ1aWQiOiJkNGM5MjkzMy1kZThlLTRhNDYtYjkxYS04NjRjNTZkZDU4YzIifX0sIm5iZiI6MTcwNTkzMDYxNCwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmF1dGhlbnRpY2F0aW9uLW9pZGMtdGVzdDpvaWRjLXRlc3QtdXNlciJ9.Taqk11LRC7FKMbt_1VvmRjMolJL54CFGbRT85ZgNdG8YT6MXiw_S2rMHxLyC9RyX0hb720szHhiVIPj15jbz597egSBbcuk-f_MCsUFMyK1Nb95blo6UNDFKIQxC5_aleoT-qaGtXlt4OEE6RjA28mFeeSCjcJUCRdLGLuSiQT47lxLqNK5OfKjd4wGMiUsbBzOcXor9ouJc1lr4gFlCzzIMJNLfXU0O_AB8J--yh6wP07Q-2AWwwv7J1CtZCrIqaPBFjWnplLqtBgo33ZNbqomXyYVdO_0HlEN9XtlK_y_2veEvKOkINzpic_ipf5ZhTxEpXWaztZzdkWd-e2mHMg\n</code></pre></p> </li> <li> <p>Send a curl request to the Broker     <pre><code>kubectl -n authentication-oidc-test run curl --image=curlimages/curl -i --tty -- sh\n\n# Send unauthenticated request (should result in a 401)\ncurl -v http://broker-ingress.knative-eventing.svc.cluster.local/authentication-oidc-test/br -H \"Content-Type:application/json\" -H \"Ce-Id:1\" -H \"Ce-Source:cloud-event-example\" -H \"Ce-Type:myCloudEventGreeting\" -H \"Ce-Specversion:1.0\" -d \"{\\\"name\\\": \\\"unauthenticated\\\"}\"\n\n# Send authenticated request (should request in 202)\ncurl -v http://broker-ingress.knative-eventing.svc.cluster.local/authentication-oidc-test/br -H \"Content-Type:application/json\" -H \"Ce-Id:1\" -H \"Ce-Source:cloud-event-example\" -H \"Ce-Type:myCloudEventGreeting\" -H \"Ce-Specversion:1.0\" -H \"Authorization: Bearer &lt;YOUR-TOKEN-FROM-STEP-1&gt;\" -d \"{\\\"name\\\": \\\"authenticated\\\"}\"\n</code></pre></p> <p>Example output:  <pre><code>$ curl -v http://broker-ingress.knative-eventing.svc.cluster.local/authentication-oidc-test/br -H \"Content-Type:application/json\" -H \"Ce-Id:1\" -H \"Ce-Source:cloud-event-example\" -H \"Ce-Type:myCloudEventGreeting\" -H \"Ce-Specversion:1.0\" -d \"{\\\"name\\\": \\\"unauthenticated\\\"}\"\n\n* Host broker-ingress.knative-eventing.svc.cluster.local:80 was resolved.\n* IPv6: (none)\n* IPv4: 10.96.110.167\n*   Trying 10.96.110.167:80...\n* Connected to broker-ingress.knative-eventing.svc.cluster.local (10.96.110.167) port 80\n&gt; POST /authentication-oidc-test/br HTTP/1.1\n&gt; Host: broker-ingress.knative-eventing.svc.cluster.local\n&gt; User-Agent: curl/8.5.0\n&gt; Accept: */*\n&gt; Content-Type:application/json\n&gt; Ce-Id:1\n&gt; Ce-Source:cloud-event-example\n&gt; Ce-Type:myCloudEventGreeting\n&gt; Ce-Specversion:1.0\n&gt; Content-Length: 27\n&gt; \n&lt; HTTP/1.1 401 Unauthorized\n&lt; Allow: POST, OPTIONS\n&lt; Date: Mon, 22 Jan 2024 13:33:57 GMT\n&lt; Content-Length: 0\n&lt; \n* Connection #0 to host broker-ingress.knative-eventing.svc.cluster.local left intact\n\n~ $ curl -v http://broker-ingress.knative-eventing.svc.cluster.local/authentication-oidc-test/br -H \"Content-Type:application/json\" -H \"Ce-Id:1\" -H \"Ce-Source:cloud-event-example\" -H \"Ce-Type:myCloudEventGreeting\" -H \"Ce-Specversion:1.0\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlZBWmppNEVJZkVSV\nDZoYTA4dU1xTWJxSHFYQTgtbE00VU1tMmpFZUNuakUifQ.eyJhdWQiOlsiZXZlbnRpbmcua25hdGl2ZS5kZXYvYnJva2VyL2F1dGhlbnRpY2F0aW9uLW9pZGMtdGVzdC9iciJdLCJleHAiOjE3MDU5MzQwMDgsImlhdCI6MTcwNTkzMDQwOCwiaXNzIjoiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJhdXRoZW50aWNhdGlvbi1vaWRjLXRlc3QiLCJ\nzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoib2lkYy10ZXN0LXVzZXIiLCJ1aWQiOiI3MTlkMWI3ZC1hZjBkLTQzMDAtOGUxNy1lNTk4YmZmN2VmYTIifX0sIm5iZiI6MTcwNTkzMDQwOCwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmF1dGhlbnRpY2F0aW9uLW9pZGMtdGVzdDpvaWRjLXRlc3QtdXNlciJ9.UrleSi54mxgThesyrC4kzG7rO3-Fic1B3kPOY8k1l-oslhvw3dbT0n24bvP96m7Ke4ZGoXE3Efo\n966LZM_61-bfntFbw8kTRe_w6wGXVGpadrBSZsIChVgFYqsPNX_7r1LSNTy5tFXze9phVz6EpO7XeUct_PXyYLASNw0LNXWyqbcEqBNtgWmDKHaS_1pIscFP6MaoGVj968hpVqli8O6okQUQitIoPwFEGAIbaBlIX6Z5ZqlGwL9eqbIiNEMEgjlduv9dyZVmpDc0hsF6GHk2RnAhLeOniUNdUo4VO3z27TJY5JYK7xIMBD6Z5dUAhud9ofA8VWEl7Mziw4fsdCw\" -d \"{\\\"name\\\": \\\"authenticated\\\"}\"\n* Host broker-ingress.knative-eventing.svc.cluster.local:80 was resolved.\n* IPv6: (none)\n* IPv4: 10.96.110.167\n*   Trying 10.96.110.167:80...\n* Connected to broker-ingress.knative-eventing.svc.cluster.local (10.96.110.167) port 80\n&gt; POST /authentication-oidc-test/br HTTP/1.1\n&gt; Host: broker-ingress.knative-eventing.svc.cluster.local\n&gt; User-Agent: curl/8.5.0\n&gt; Accept: */*\n&gt; Content-Type:application/json\n&gt; Ce-Id:1\n&gt; Ce-Source:cloud-event-example\n&gt; Ce-Type:myCloudEventGreeting\n&gt; Ce-Specversion:1.0\n&gt; Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IlZBWmppNEVJZkVSVDZoYTA4dU1xTWJxSHFYQTgtbE00VU1tMmpFZUNuakUifQ.eyJhdWQiOlsiZXZlbnRpbmcua25hdGl2ZS5kZXYvYnJva2VyL2F1dGhlbnRpY2F0aW9uLW9pZGMtdGVzdC9iciJdLCJleHAiOjE3MDU5MzQwMDgsImlhdCI6MTcwNTkzMDQwOCwiaXNzIjoiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJhdXRoZW50aWNhdGlvbi1vaWRjLXRlc3QiLCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoib2lkYy10ZXN0LXVzZXIiLCJ1aWQiOiI3MTlkMWI3ZC1hZjBkLTQzMDAtOGUxNy1lNTk4YmZmN2VmYTIifX0sIm5iZiI6MTcwNTkzMDQwOCwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmF1dGhlbnRpY2F0aW9uLW9pZGMtdGVzdDpvaWRjLXRlc3QtdXNlciJ9.UrleSi54mxgThesyrC4kzG7rO3-Fic1B3kPOY8k1l-oslhvw3dbT0n24bvP96m7Ke4ZGoXE3Efo966LZM_61-bfntFbw8kTRe_w6wGXVGpadrBSZsIChVgFYqsPNX_7r1LSNTy5tFXze9phVz6EpO7XeUct_PXyYLASNw0LNXWyqbcEqBNtgWmDKHaS_1pIscFP6MaoGVj968hpVqli8O6okQUQitIoPwFEGAIbaBlIX6Z5ZqlGwL9eqbIiNEMEgjlduv9dyZVmpDc0hsF6GHk2RnAhLeOniUNdUo4VO3z27TJY5JYK7xIMBD6Z5dUAhud9ofA8VWEl7Mziw4fsdCw\n&gt; Content-Length: 25\n&gt; \n&lt; HTTP/1.1 202 Accepted\n&lt; Allow: POST, OPTIONS\n&lt; Date: Mon, 22 Jan 2024 13:34:27 GMT\n&lt; Content-Length: 0\n&lt; \n* Connection #0 to host broker-ingress.knative-eventing.svc.cluster.local left intact\n~ $\n</code></pre> 3. Verify the 2nd event reached the event-display pod <pre><code>kubectl -n authentication-oidc-test logs event-display\n</code></pre></p> <p>Example output: <pre><code>\u2601\ufe0f  cloudevents.Event\nContext Attributes,\n  specversion: 1.0\n  type: myCloudEventGreeting\n  source: cloud-event-example\n  id: 1\n  datacontenttype: application/json\nExtensions,\n  knativearrivaltime: 2024-01-22T13:34:26.032199371Z\nData,\n  {\n    \"name\": \"authenticated\"\n  }\n</code></pre></p> </li> </ol>"},{"location":"eventing/features/sender-identity/#limitations-with-istio","title":"Limitations with Istio","text":"<p>You might experience issues with the eventing integration with Istio and having the <code>authentication-oidc</code> feature flag enabeled, when the JWKS URI is represented via an IP. E.g. like in the following case:</p> <pre><code>$ kubectl get --raw /.well-known/openid-configuration | jq\n{\n  \"issuer\": \"https://kubernetes.default.svc\",\n  \"jwks_uri\": \"https://172.18.0.3:6443/openid/v1/jwks\",\n  ...\n}\n</code></pre> <p>In this case you need to add the <code>traffic.sidecar.istio.io/excludeOutboundIPRanges: &lt;JWKS IP&gt;/32</code> annotation to the pod templates of the following deployments:</p> <ul> <li><code>imc-dispatcher</code></li> <li><code>mt-broker-ingress</code></li> <li><code>mt-broker-filter</code></li> </ul> <p>For example:</p> <pre><code>$ kubectl -n knative-eventing patch deploy imc-dispatcher --patch '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"traffic.sidecar.istio.io/excludeOutboundIPRanges\":\"172.18.0.3/32\"}}}}}'\ndeployment.apps/imc-dispatcher patched\n</code></pre>"},{"location":"eventing/features/transport-encryption/","title":"Transport Encryption for Knative Eventing","text":"<p>Flag name: <code>transport-encryption</code></p> <p>Stage: Beta, disabled by default</p> <p>Tracking issue: #5957</p>"},{"location":"eventing/features/transport-encryption/#overview","title":"Overview","text":"<p>By default, event delivery within the cluster is unencrypted. This limits the types of events which can be transmitted to those of low compliance value (or a relaxed compliance posture) or, alternatively, forces administrators to use a service mesh or encrypted CNI to encrypt the traffic, which poses many challenges to Knative Eventing adopters.</p> <p>Knative Brokers and Channels provides HTTPS endpoints to receive events. Given that these endpoints typically do not have public DNS names (e.g. svc.cluster.local or the like), these need to be signed by a non-public CA (cluster or organization specific CA).</p> <p>Event producers are be able to connect to HTTPS endpoints with cluster-internal CA certificates.</p>"},{"location":"eventing/features/transport-encryption/#prerequisites","title":"Prerequisites","text":"<ul> <li>In order to enable the transport encryption feature, you will need to install cert-manager   operator by   following the cert-manager operator installation instructions.</li> <li>Eventing installation</li> </ul>"},{"location":"eventing/features/transport-encryption/#installation","title":"Installation","text":""},{"location":"eventing/features/transport-encryption/#setup-selfsigned-clusterissuer","title":"Setup <code>SelfSigned</code> <code>ClusterIssuer</code>","text":"<p>Note</p> <p>ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. All cert-manager certificates require a referenced issuer that is in a ready condition to attempt to honor the request. Reference: cert-manager.io/docs/concepts/issuer/</p> <p>Important</p> <p>For the simplicity of this guide, we will use a <code>SelfSigned</code> issuer as root certificate, however, be aware of the implications and limitations as documented at cert-manager.io/docs/configuration/selfsigned/ of this method. If you\u2019re running your company specific Private Key Infrastructure (PKI), we recommend the CA issuer. Refer to the cert-manager documentation for more details: cert-manager.io/docs/configuration/ca/, however, you can use any other issuer that is usable for cluster-local services.</p> <ol> <li>Create a <code>SelfSigned</code> <code>ClusterIssuer</code>:     <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: knative-eventing-selfsigned-issuer\nspec:\n  selfSigned: {}\n</code></pre></li> <li>Apply the <code>ClusterIssuer</code> resource:     <pre><code>$ kubectl apply -f &lt;filename&gt;\n</code></pre></li> <li>Create a root certificate using the previously created <code>SelfSigned</code> <code>ClusterIssuer</code>:     <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: knative-eventing-selfsigned-ca\n  namespace: cert-manager # the cert-manager operator namespace\nspec:\n   # Secret name later used for the ClusterIssuer for Eventing\n  secretName: knative-eventing-ca\n\n  isCA: true\n  commonName: selfsigned-ca\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n\n  issuerRef:\n    name: knative-eventing-selfsigned-issuer\n    kind: ClusterIssuer\n    group: cert-manager.io\n</code></pre></li> <li>Apply the <code>Certificate</code> resource:     <pre><code>$ kubectl apply -f &lt;filename&gt;\n</code></pre></li> </ol>"},{"location":"eventing/features/transport-encryption/#setup-clusterissuer-for-eventing","title":"Setup <code>ClusterIssuer</code> for Eventing","text":"<ol> <li> <p>Create the <code>knative-eventing-ca-issuer</code> <code>ClusterIssuer</code> for Eventing:     <pre><code># This is the issuer that every Eventing component use to issue their server's certs.\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: knative-eventing-ca-issuer\nspec:\n  ca:\n    # Secret name in the Cert-Manager Operator namespace (cert-manager by default) containing\n    # the certificate that can then be used by Knative Eventing components for new certificates.\n    secretName: knative-eventing-ca \n</code></pre>    !!! important         The name of the <code>ClusterIssuer</code> must be <code>knative-eventing-ca-issuer</code>.</p> </li> <li> <p>Apply the <code>ClusterIssuer</code> resource:     <pre><code>$ kubectl apply -f &lt;filename&gt;\n</code></pre></p> </li> </ol>"},{"location":"eventing/features/transport-encryption/#install-the-certificates-for-eventing-components","title":"Install the certificates for Eventing components","text":"<p>Eventing components use cert-manager issuers and certificates to provision TLS certificates and in the release assets, we release the certificates for Eventing servers that can be customized as necessary.</p> <ol> <li>Install certificates, run the following command:     <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-tls-networking.yaml\n</code></pre></li> <li>[Optional] If you're using Eventing Kafka components, install certificates for Kafka components    by running the following command:     <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-tls-networking.yaml\n</code></pre></li> <li>Verify issuers and certificates are ready     <pre><code>kubectl get certificates.cert-manager.io -n knative-eventing\n</code></pre>    Example output:     <pre><code>NAME                           READY   SECRET                         AGE\nimc-dispatcher-server-tls      True    imc-dispatcher-server-tls      14s\nmt-broker-filter-server-tls    True    mt-broker-filter-server-tls    14s\nmt-broker-ingress-server-tls   True    mt-broker-ingress-server-tls   14s\nselfsigned-ca                  True    eventing-ca                    14s\n...\n</code></pre></li> </ol>"},{"location":"eventing/features/transport-encryption/#transport-encryption-configuration","title":"Transport Encryption configuration","text":"<p>The <code>transport-encryption</code> feature flag is an enum configuration that configures how Addressables ( Broker, Channel, Sink) should accept events.</p> <p>The possible values for <code>transport-encryption</code> are:</p> <ul> <li><code>disabled</code> (this is equivalent to the current behavior)<ul> <li>Addressables may accept events to HTTPS endpoints</li> <li>Producers may send events to HTTPS endpoints</li> </ul> </li> <li><code>permissive</code><ul> <li>Addressables should accept events on both HTTP and HTTPS endpoints</li> <li>Addressables should advertise both HTTP and HTTPS endpoints</li> <li>Producers should prefer sending events to HTTPS endpoints, if available</li> </ul> </li> <li><code>strict</code><ul> <li>Addressables must not accept events to non-HTTPS endpoints</li> <li>Addressables must only advertise HTTPS endpoints</li> </ul> </li> </ul> <p>Important</p> <p>The <code>strict</code> is only enforced on the Broker and Channel receiver/ingress.  When a broker or channel sends events to a subscriber, if that subscriber only has an HTTP address, the broker or channel can still send events over HTTP instead of HTTPS</p> <p>For example, to enable <code>strict</code> transport encryption, the <code>config-features</code> ConfigMap will look like the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-features\n  namespace: knative-eventing\ndata:\n  transport-encryption: \"strict\"\n</code></pre>"},{"location":"eventing/features/transport-encryption/#configure-additional-ca-trust-bundles","title":"Configure additional CA trust bundles","text":"<p>By default, Eventing clients trusts the system root CA (public CA).</p> <p>If you need to add additional CA bundles for Eventing, you can do so by creating ConfigMaps in the <code>knative-eventing</code> namespace with label <code>networking.knative.dev/trust-bundle: true</code>:</p> <p>Important</p> <p>Whenever CA bundles <code>ConfigMaps</code> are updated, the Eventing clients will automatically add them to their trusted CA bundles when a new connection is established.</p> <ol> <li>Create a CA bundle for Eventing:     <pre><code>kind: ConfigMap\nmetadata:\n  name: my-org-eventing-bundle\n  namespace: knative-eventing\n  labels:\n    networking.knative.dev/trust-bundle: \"true\"\n# All data keys containing valid PEM-encoded CA bundles will be trusted by Eventing clients.\ndata:\n  ca.crt: ...\n  ca1.crt: ...\n  tls.crt: ...\n</code></pre></li> </ol> <p>Important</p> <p>Use a name that is unlikely to conflict with existing or future Eventing-provided <code>ConfigMap</code> name.</p> <p>For distributing CA trust bundles, you can leverage trust-manager, however, it is not required.</p>"},{"location":"eventing/features/transport-encryption/#trusting-ca-for-a-specific-event-sender","title":"Trusting CA for a specific event sender","text":"<p>Event sources, triggers or subscriptions are considered event senders, and they can be configured to trust specific CA certificates.</p> <p>Important</p> <p>The CA certs must be PEM formatted certificates. Since it's a multi-line YAML string make sure that the <code>CACerts</code> value is indented correctly, otherwise when creating the resource it will be rejected.</p> <p>Triggers and subscriptions can be configured as follows:</p> <pre><code>spec:\n  # ...\n\n  subscriber:\n    uri: https://mycorp-internal-example.com/v1/api\n    CACerts: |-\n      -----BEGIN CERTIFICATE-----\n      MIIFWjCCA0KgAwIBAgIQT9Irj/VkyDOeTzRYZiNwYDANBgkqhkiG9w0BAQsFADBH\n      MQswCQYDVQQGEwJDTjERMA8GA1UECgwIVW5pVHJ1c3QxJTAjBgNVBAMMHFVDQSBF\n      eHRlbmRlZCBWYWxpZGF0aW9uIFJvb3QwHhcNMTUwMzEzMDAwMDAwWhcNMzgxMjMx\n      MDAwMDAwWjBHMQswCQYDVQQGEwJDTjERMA8GA1UECgwIVW5pVHJ1c3QxJTAjBgNV\n      BAMMHFVDQSBFeHRlbmRlZCBWYWxpZGF0aW9uIFJvb3QwggIiMA0GCSqGSIb3DQEB\n      AQUAA4ICDwAwggIKAoICAQCpCQcoEwKwmeBkqh5DFnpzsZGgdT6o+uM4AHrsiWog\n      D4vFsJszA1qGxliG1cGFu0/GnEBNyr7uaZa4rYEwmnySBesFK5pI0Lh2PpbIILvS\n      sPGP2KxFRv+qZ2C0d35qHzwaUnoEPQc8hQ2E0B92CvdqFN9y4zR8V05WAT558aop\n      O2z6+I9tTcg1367r3CTueUWnhbYFiN6IXSV8l2RnCdm/WhUFhvMJHuxYMjMR83dk\n      sHYf5BA1FxvyDrFspCqjc/wJHx4yGVMR59mzLC52LqGj3n5qiAno8geK+LLNEOfi\n      c0CTuwjRP+H8C5SzJe98ptfRr5//lpr1kXuYC3fUfugH0mK1lTnj8/FtDw5lhIpj\n      VMWAtuCeS31HJqcBCF3RiJ7XwzJE+oJKCmhUfzhTA8ykADNkUVkLo4KRel7sFsLz\n      KuZi2irbWWIQJUoqgQtHB0MGcIfS+pMRKXpITeuUx3BNr2fVUbGAIAEBtHoIppB/\n      TuDvB0GHr2qlXov7z1CymlSvw4m6WC31MJixNnI5fkkE/SmnTHnkBVfblLkWU41G\n      sx2VYVdWf6/wFlthWG82UBEL2KwrlRYaDh8IzTY0ZRBiZtWAXxQgXy0MoHgKaNYs\n      1+lvK9JKBZP8nm9rZ/+I8U6laUpSNwXqxhaN0sSZ0YIrO7o1dfdRUVjzyAfd5LQD\n      fwIDAQABo0IwQDAdBgNVHQ4EFgQU2XQ65DA9DfcS3H5aBZ8eNJr34RQwDwYDVR0T\n      AQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAYYwDQYJKoZIhvcNAQELBQADggIBADaN\n      l8xCFWQpN5smLNb7rhVpLGsaGvdftvkHTFnq88nIua7Mui563MD1sC3AO6+fcAUR\n      ap8lTwEpcOPlDOHqWnzcSbvBHiqB9RZLcpHIojG5qtr8nR/zXUACE/xOHAbKsxSQ\n      VBcZEhrxH9cMaVr2cXj0lH2RC47skFSOvG+hTKv8dGT9cZr4QQehzZHkPJrgmzI5\n      c6sq1WnIeJEmMX3ixzDx/BR4dxIOE/TdFpS/S2d7cFOFyrC78zhNLJA5wA3CXWvp\n      4uXViI3WLL+rG761KIcSF3Ru/H38j9CHJrAb+7lsq+KePRXBOy5nAliRn+/4Qh8s\n      t2j1da3Ptfb/EX3C8CSlrdP6oDyp+l3cpaDvRKS+1ujl5BOWF3sGPjLtx7dCvHaj\n      2GU4Kzg1USEODm8uNBNA4StnDG1KQTAYI1oyVZnJF+A83vbsea0rWBmirSwiGpWO\n      vpaQXUJXxPkUAzUrHC1RVwinOt4/5Mi0A3PCwSaAuwtCH60NryZy2sy+s6ODWA2C\n      xR9GUeOcGMyNm43sSet1UNWMKFnKdDTajAshqx7qG+XH/RU+wBeq+yNuJkbL+vmx\n      cmtpzyKEC2IPrNkZAJSidjzULZrtBJ4tBmIQN1IchXIbJ+XMxjHsN+xjWZsLHXbM\n      fjKaiJUINlK73nZfdklJrX+9ZSCyycErdhh2n1ax\n      -----END CERTIFICATE-----\n</code></pre> <p>Similarly, sources can be configured as follows:</p> <pre><code>spec:\n  # ...\n\n  sink:\n    uri: https://mycorp-internal-example.com/v1/api\n    CACerts: |-\n      -----BEGIN CERTIFICATE-----\n      MIIFWjCCA0KgAwIBAgIQT9Irj/VkyDOeTzRYZiNwYDANBgkqhkiG9w0BAQsFADBH\n      MQswCQYDVQQGEwJDTjERMA8GA1UECgwIVW5pVHJ1c3QxJTAjBgNVBAMMHFVDQSBF\n      eHRlbmRlZCBWYWxpZGF0aW9uIFJvb3QwHhcNMTUwMzEzMDAwMDAwWhcNMzgxMjMx\n      MDAwMDAwWjBHMQswCQYDVQQGEwJDTjERMA8GA1UECgwIVW5pVHJ1c3QxJTAjBgNV\n      BAMMHFVDQSBFeHRlbmRlZCBWYWxpZGF0aW9uIFJvb3QwggIiMA0GCSqGSIb3DQEB\n      AQUAA4ICDwAwggIKAoICAQCpCQcoEwKwmeBkqh5DFnpzsZGgdT6o+uM4AHrsiWog\n      D4vFsJszA1qGxliG1cGFu0/GnEBNyr7uaZa4rYEwmnySBesFK5pI0Lh2PpbIILvS\n      sPGP2KxFRv+qZ2C0d35qHzwaUnoEPQc8hQ2E0B92CvdqFN9y4zR8V05WAT558aop\n      O2z6+I9tTcg1367r3CTueUWnhbYFiN6IXSV8l2RnCdm/WhUFhvMJHuxYMjMR83dk\n      sHYf5BA1FxvyDrFspCqjc/wJHx4yGVMR59mzLC52LqGj3n5qiAno8geK+LLNEOfi\n      c0CTuwjRP+H8C5SzJe98ptfRr5//lpr1kXuYC3fUfugH0mK1lTnj8/FtDw5lhIpj\n      VMWAtuCeS31HJqcBCF3RiJ7XwzJE+oJKCmhUfzhTA8ykADNkUVkLo4KRel7sFsLz\n      KuZi2irbWWIQJUoqgQtHB0MGcIfS+pMRKXpITeuUx3BNr2fVUbGAIAEBtHoIppB/\n      TuDvB0GHr2qlXov7z1CymlSvw4m6WC31MJixNnI5fkkE/SmnTHnkBVfblLkWU41G\n      sx2VYVdWf6/wFlthWG82UBEL2KwrlRYaDh8IzTY0ZRBiZtWAXxQgXy0MoHgKaNYs\n      1+lvK9JKBZP8nm9rZ/+I8U6laUpSNwXqxhaN0sSZ0YIrO7o1dfdRUVjzyAfd5LQD\n      fwIDAQABo0IwQDAdBgNVHQ4EFgQU2XQ65DA9DfcS3H5aBZ8eNJr34RQwDwYDVR0T\n      AQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAYYwDQYJKoZIhvcNAQELBQADggIBADaN\n      l8xCFWQpN5smLNb7rhVpLGsaGvdftvkHTFnq88nIua7Mui563MD1sC3AO6+fcAUR\n      ap8lTwEpcOPlDOHqWnzcSbvBHiqB9RZLcpHIojG5qtr8nR/zXUACE/xOHAbKsxSQ\n      VBcZEhrxH9cMaVr2cXj0lH2RC47skFSOvG+hTKv8dGT9cZr4QQehzZHkPJrgmzI5\n      c6sq1WnIeJEmMX3ixzDx/BR4dxIOE/TdFpS/S2d7cFOFyrC78zhNLJA5wA3CXWvp\n      4uXViI3WLL+rG761KIcSF3Ru/H38j9CHJrAb+7lsq+KePRXBOy5nAliRn+/4Qh8s\n      t2j1da3Ptfb/EX3C8CSlrdP6oDyp+l3cpaDvRKS+1ujl5BOWF3sGPjLtx7dCvHaj\n      2GU4Kzg1USEODm8uNBNA4StnDG1KQTAYI1oyVZnJF+A83vbsea0rWBmirSwiGpWO\n      vpaQXUJXxPkUAzUrHC1RVwinOt4/5Mi0A3PCwSaAuwtCH60NryZy2sy+s6ODWA2C\n      xR9GUeOcGMyNm43sSet1UNWMKFnKdDTajAshqx7qG+XH/RU+wBeq+yNuJkbL+vmx\n      cmtpzyKEC2IPrNkZAJSidjzULZrtBJ4tBmIQN1IchXIbJ+XMxjHsN+xjWZsLHXbM\n      fjKaiJUINlK73nZfdklJrX+9ZSCyycErdhh2n1ax\n      -----END CERTIFICATE-----\n</code></pre>"},{"location":"eventing/features/transport-encryption/#configure-custom-event-sources-to-trust-the-eventing-ca","title":"Configure custom event sources to trust the Eventing CA","text":"<p>The recommended way of creating custom event sources is using a SinkBinding, SinkBinding will inject the configured CA trust bundles as projected volume into each container using the directory <code>/knative-custom-certs</code>.</p> <p>Note</p> <p>Some organizations might inject company specific CA trust bundles into base container images and automatically configure runtimes (openjdk, node, etc) to trust that CA bundle. In that case, you might not need to configure your clients.</p> <p>Using the previous example of the my-org-eventing-bundle ConfigMap with data keys being ca.crt, ca1.crt and tls.crt, you will have a <code>/knative-custom-certs</code> directory that will have the following layout:</p> <pre><code>/knative-custom-certs/ca.crt\n/knative-custom-certs/ca1.crt\n/knative-custom-certs/tls.crt\n</code></pre> <p>Those files can then be used to add CA trust bundles to HTTP clients sending events to Eventing.</p> <p>Note</p> <p>Depending on the runtime, programming language or library that you\u2019re using, there are different ways of configuring custom CA certs files using command line flags, environment variables, or by reading the content of those files. Refer to their documentation for more details.</p>"},{"location":"eventing/features/transport-encryption/#adding-selfsigned-clusterissuer-to-ca-trust-bundles","title":"Adding <code>SelfSigned</code> <code>ClusterIssuer</code> to CA trust bundles","text":"<p>In case you are using a SelfSigned ClusterIssuer as described in the Setup SelfSigned ClusterIssuer section, you can add the CA to the Eventing CA trust bundles by running the following commands:</p> <ol> <li>Export the CA from the knative-eventing-ca secret in the OpenShift Cert-Manager Operator namespace, cert-manager by default:     <pre><code>$ kubectl get secret -n cert-manager knative-eventing-ca -o=jsonpath='{.data.ca\\.crt}' | base64 -d &gt; ca.crt\n</code></pre></li> <li>Create a CA trust bundle in the <code>knative-eventing</code> namespace:     <pre><code>$ kubectl create configmap -n knative-eventing my-org-selfsigned-ca-bundle --from-file=ca.crt\n</code></pre></li> <li>Label the ConfigMap with networking.knative.dev/trust-bundle: \"true\" label:     <pre><code>$ kubectl label configmap -n knative-eventing my-org-selfsigned-ca-bundle networking.knative.dev/trust-bundle=true\n</code></pre></li> </ol>"},{"location":"eventing/features/transport-encryption/#verifying-that-the-feature-is-working","title":"Verifying that the feature is working","text":"<p>Save the following YAML into a file called <code>default-broker-example.yaml</code></p> <pre><code># default-broker-example.yaml\n\napiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: br\n\n---\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: tr\nspec:\n  broker: br\n  subscriber:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: event-display\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: event-display\nspec:\n  selector:\n    app: event-display\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: event-display\n  labels:\n    app: event-display\nspec:\n  containers:\n    - name: event-display\n      image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n      imagePullPolicy: Always\n      ports:\n        - containerPort: 8080\n</code></pre> <p>Apply the <code>default-broker-example.yaml</code> file into a test namespace  <code>transport-encryption-test</code>:</p> <pre><code>kubectl create namespace transport-encryption-test\n\nkubectl apply -n transport-encryption-test -f defautl-broker-example.yaml\n</code></pre> <p>Verify that addresses are all <code>HTTPS</code>:</p> <pre><code>kubectl get brokers.eventing.knative.dev -n transport-encryption-test br -oyaml\n</code></pre> <p>Example output:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  # ...\n  name: br\n  namespace: transport-encryption-test\n# ...\nstatus:\n  address:\n    CACerts: |\n      -----BEGIN CERTIFICATE-----\n      MIIBbzCCARagAwIBAgIQAur7vdEcreEWSEQatCYlNjAKBggqhkjOPQQDAjAYMRYw\n      FAYDVQQDEw1zZWxmc2lnbmVkLWNhMB4XDTIzMDgwMzA4MzA1N1oXDTIzMTEwMTA4\n      MzA1N1owGDEWMBQGA1UEAxMNc2VsZnNpZ25lZC1jYTBZMBMGByqGSM49AgEGCCqG\n      SM49AwEHA0IABBqkD9lAwrnXCo/OOdpKzJROSbzCeC73FE/Np+/j8n862Ox5xYwJ\n      tAp/o3RDpDa3omhzqZoYumqdtneozGFY/zGjQjBAMA4GA1UdDwEB/wQEAwICpDAP\n      BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSHoKjXzfxfudt3mVGU3VudSi6TrTAK\n      BggqhkjOPQQDAgNHADBEAiA5z0/TpD7T6vRpN9VQisQMtum/Zg3tThnYA5nFnAW7\n      KAIgKR/EzW7f8BPcnlcgXt5kp3Fdqye1SAkjxZzr2a0Zik8=\n      -----END CERTIFICATE-----\n    name: https\n    url: https://broker-ingress.knative-eventing.svc.cluster.local/transport-encryption-test/br\n  addresses:\n  - CACerts: |\n      -----BEGIN CERTIFICATE-----\n      MIIBbzCCARagAwIBAgIQAur7vdEcreEWSEQatCYlNjAKBggqhkjOPQQDAjAYMRYw\n      FAYDVQQDEw1zZWxmc2lnbmVkLWNhMB4XDTIzMDgwMzA4MzA1N1oXDTIzMTEwMTA4\n      MzA1N1owGDEWMBQGA1UEAxMNc2VsZnNpZ25lZC1jYTBZMBMGByqGSM49AgEGCCqG\n      SM49AwEHA0IABBqkD9lAwrnXCo/OOdpKzJROSbzCeC73FE/Np+/j8n862Ox5xYwJ\n      tAp/o3RDpDa3omhzqZoYumqdtneozGFY/zGjQjBAMA4GA1UdDwEB/wQEAwICpDAP\n      BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSHoKjXzfxfudt3mVGU3VudSi6TrTAK\n      BggqhkjOPQQDAgNHADBEAiA5z0/TpD7T6vRpN9VQisQMtum/Zg3tThnYA5nFnAW7\n      KAIgKR/EzW7f8BPcnlcgXt5kp3Fdqye1SAkjxZzr2a0Zik8=\n      -----END CERTIFICATE-----\n    name: https\n    url: https://broker-ingress.knative-eventing.svc.cluster.local/transport-encryption-test/br\n  annotations:\n    knative.dev/channelAPIVersion: messaging.knative.dev/v1\n    knative.dev/channelAddress: https://imc-dispatcher.knative-eventing.svc.cluster.local/transport-encryption-test/br-kne-trigger\n    knative.dev/channelCACerts: |\n      -----BEGIN CERTIFICATE-----\n      MIIBbzCCARagAwIBAgIQAur7vdEcreEWSEQatCYlNjAKBggqhkjOPQQDAjAYMRYw\n      FAYDVQQDEw1zZWxmc2lnbmVkLWNhMB4XDTIzMDgwMzA4MzA1N1oXDTIzMTEwMTA4\n      MzA1N1owGDEWMBQGA1UEAxMNc2VsZnNpZ25lZC1jYTBZMBMGByqGSM49AgEGCCqG\n      SM49AwEHA0IABBqkD9lAwrnXCo/OOdpKzJROSbzCeC73FE/Np+/j8n862Ox5xYwJ\n      tAp/o3RDpDa3omhzqZoYumqdtneozGFY/zGjQjBAMA4GA1UdDwEB/wQEAwICpDAP\n      BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSHoKjXzfxfudt3mVGU3VudSi6TrTAK\n      BggqhkjOPQQDAgNHADBEAiA5z0/TpD7T6vRpN9VQisQMtum/Zg3tThnYA5nFnAW7\n      KAIgKR/EzW7f8BPcnlcgXt5kp3Fdqye1SAkjxZzr2a0Zik8=\n      -----END CERTIFICATE-----\n    knative.dev/channelKind: InMemoryChannel\n    knative.dev/channelName: br-kne-trigger\n  conditions:\n  # ...\n</code></pre> <p>Sending events to the Broker using HTTPS endpoints:</p> <pre><code>kubectl run curl -n transport-encryption-test --image=curlimages/curl -i --tty -- sh\n</code></pre> <p>Save the CA certs from the Broker's <code>.status.address.CACerts</code> field into <code>/tmp/cacerts.pem</code></p> <pre><code>cat &lt;&lt;EOF &gt;&gt; /tmp/cacerts.pem\n-----BEGIN CERTIFICATE-----\nMIIBbzCCARagAwIBAgIQAur7vdEcreEWSEQatCYlNjAKBggqhkjOPQQDAjAYMRYw\nFAYDVQQDEw1zZWxmc2lnbmVkLWNhMB4XDTIzMDgwMzA4MzA1N1oXDTIzMTEwMTA4\nMzA1N1owGDEWMBQGA1UEAxMNc2VsZnNpZ25lZC1jYTBZMBMGByqGSM49AgEGCCqG\nSM49AwEHA0IABBqkD9lAwrnXCo/OOdpKzJROSbzCeC73FE/Np+/j8n862Ox5xYwJ\ntAp/o3RDpDa3omhzqZoYumqdtneozGFY/zGjQjBAMA4GA1UdDwEB/wQEAwICpDAP\nBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSHoKjXzfxfudt3mVGU3VudSi6TrTAK\nBggqhkjOPQQDAgNHADBEAiA5z0/TpD7T6vRpN9VQisQMtum/Zg3tThnYA5nFnAW7\nKAIgKR/EzW7f8BPcnlcgXt5kp3Fdqye1SAkjxZzr2a0Zik8=\n-----END CERTIFICATE-----\nEOF\n</code></pre> <p>Send the event by running the following command:</p> <pre><code>curl -v -X POST -H \"content-type: application/json\" -H \"ce-specversion: 1.0\" -H \"ce-source: my/curl/command\" -H \"ce-type: my.demo.event\" -H \"ce-id: 6cf17c7b-30b1-45a6-80b0-4cf58c92b947\" -d '{\"name\":\"Knative Demo\"}' --cacert /tmp/cacert\ns.pem https://broker-ingress.knative-eventing.svc.cluster.local/transport-encryption-test/br\n</code></pre> <p>Example output:</p> <pre><code>* processing: https://broker-ingress.knative-eventing.svc.cluster.local/transport-encryption-test/br\n*   Trying 10.96.174.249:443...\n* Connected to broker-ingress.knative-eventing.svc.cluster.local (10.96.174.249) port 443\n* ALPN: offers h2,http/1.1\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n*  CAfile: /tmp/cacerts.pem\n*  CApath: none\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n* TLSv1.3 (IN), TLS handshake, Finished (20):\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\n* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: O=local\n*  start date: Aug  3 08:31:02 2023 GMT\n*  expire date: Nov  1 08:31:02 2023 GMT\n*  subjectAltName: host \"broker-ingress.knative-eventing.svc.cluster.local\" matched cert's \"broker-ingress.knative-eventing.svc.cluster.local\"\n*  issuer: CN=selfsigned-ca\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* using HTTP/2\n* h2 [:method: POST]\n* h2 [:scheme: https]\n* h2 [:authority: broker-ingress.knative-eventing.svc.cluster.local]\n* h2 [:path: /transport-encryption-test/br]\n* h2 [user-agent: curl/8.2.1]\n* h2 [accept: */*]\n* h2 [content-type: application/json]\n* h2 [ce-specversion: 1.0]\n* h2 [ce-source: my/curl/command]\n* h2 [ce-type: my.demo.event]\n* h2 [ce-id: 6cf17c7b-30b1-45a6-80b0-4cf58c92b947]\n* h2 [content-length: 23]\n* Using Stream ID: 1\n&gt; POST /transport-encryption-test/br HTTP/2\n&gt; Host: broker-ingress.knative-eventing.svc.cluster.local\n&gt; User-Agent: curl/8.2.1\n&gt; Accept: */*\n&gt; content-type: application/json\n&gt; ce-specversion: 1.0\n&gt; ce-source: my/curl/command\n&gt; ce-type: my.demo.event\n&gt; ce-id: 6cf17c7b-30b1-45a6-80b0-4cf58c92b947\n&gt; Content-Length: 23\n&gt; \n&lt; HTTP/2 202 \n&lt; allow: POST, OPTIONS\n&lt; content-length: 0\n&lt; date: Thu, 03 Aug 2023 10:08:22 GMT\n&lt; \n* Connection #0 to host broker-ingress.knative-eventing.svc.cluster.local left intact\n</code></pre>"},{"location":"eventing/flows/","title":"Eventing Flows","text":"<p>Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows:</p> <ul> <li>Sequence is for defining an in-order list of functions.</li> <li>Parallel is for defining a list of branches, each receiving the same CloudEvent.</li> </ul>"},{"location":"eventing/flows/parallel/","title":"Parallel","text":"<p>Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch.</p> <p>Parallel creates <code>Channel</code>s and <code>Subscription</code>s under the hood.</p>"},{"location":"eventing/flows/parallel/#usage","title":"Usage","text":""},{"location":"eventing/flows/parallel/#parallel-spec","title":"Parallel Spec","text":"<p>Parallel has three parts for the Spec:</p> <ol> <li><code>branches</code> defines the list of <code>filter</code> and <code>subscriber</code> pairs, one per branch,    and optionally a <code>reply</code> object. For each branch:</li> <li>(optional) the <code>filter</code> is evaluated and when it returns an event the <code>subscriber</code> is       executed. Both <code>filter</code> and <code>subscriber</code> must be <code>Addressable</code>.</li> <li>the event returned by the <code>subscriber</code> is sent to the branch <code>reply</code>       object. When the <code>reply</code> is empty, the event is sent to the <code>spec.reply</code>       object.</li> <li>(optional) <code>channelTemplate</code> defines the Template which will be used to    create <code>Channel</code>s.</li> <li>(optional) <code>reply</code> defines where the result of each branch is sent to when    the branch does not have its own <code>reply</code> object.</li> </ol>"},{"location":"eventing/flows/parallel/#parallel-status","title":"Parallel Status","text":"<p>Parallel has three parts for the Status:</p> <ol> <li><code>conditions</code> which details the overall status of the Parallel object</li> <li><code>ingressChannelStatus</code> and <code>branchesStatuses</code> which convey the status of    underlying <code>Channel</code> and <code>Subscription</code> resource that are created as part of    this Parallel.</li> <li><code>address</code> which is exposed so that Parallel can be used where Addressable can    be used. Sending to this address will target the <code>Channel</code> which is fronting    this Parallel (same as <code>ingressChannelStatus</code>).</li> </ol>"},{"location":"eventing/flows/parallel/#examples","title":"Examples","text":"<p>Learn how to use Parallel by following the code samples.</p>"},{"location":"eventing/flows/sequence/","title":"Sequence","text":"<p>Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates <code>Channel</code>s and <code>Subscription</code>s under the hood.</p> <p>Info</p> <p>Sequence needs \"hairpin\" traffic. Please verify that your pod can reach itself via the service IP. If the \"hairpin\" traffic is not available, you can reach out to your cluster administrator since its a cluster level (typically CNI) setting.</p>"},{"location":"eventing/flows/sequence/#usage","title":"Usage","text":""},{"location":"eventing/flows/sequence/#sequence-spec","title":"Sequence Spec","text":"<p>Sequence has three parts for the Spec:</p> <ol> <li><code>Steps</code> which defines the in-order list of <code>Subscriber</code>s, aka, which    functions are executed in the listed order. These are specified using the    <code>messaging.v1.SubscriberSpec</code> just like you would when creating    <code>Subscription</code>. Each step should be <code>Addressable</code>.</li> <li><code>ChannelTemplate</code> defines the Template which will be used to create    <code>Channel</code>s between the steps.</li> <li><code>Reply</code> (Optional) Reference to where the results of the final step in the    sequence are sent to.</li> </ol>"},{"location":"eventing/flows/sequence/#sequence-status","title":"Sequence Status","text":"<p>Sequence has four parts for the Status:</p> <ol> <li>Conditions which detail the overall Status of the Sequence object</li> <li>ChannelStatuses which convey the Status of underlying <code>Channel</code> resources    that are created as part of this Sequence. It is an array and each Status    corresponds to the Step number, so the first entry in the array is the Status    of the <code>Channel</code> before the first Step.</li> <li>SubscriptionStatuses which convey the Status of underlying <code>Subscription</code>    resources that are created as part of this Sequence. It is an array and each    Status corresponds to the Step number, so the first entry in the array is the    <code>Subscription</code> which is created to wire the first channel to the first step    in the <code>Steps</code> array.</li> <li>AddressStatus which is exposed so that Sequence can be used where Addressable    can be used. Sending to this address will target the <code>Channel</code> which is    fronting the first Step in the Sequence.</li> </ol>"},{"location":"eventing/flows/sequence/#examples","title":"Examples","text":"<p>For each of the following examples, you use a <code>PingSource</code> as the source of events.</p> <p>We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage.</p>"},{"location":"eventing/flows/sequence/#sequence-with-no-reply","title":"Sequence with no reply","text":"<p>For the first example, we'll use a 3 Step <code>Sequence</code> that is wired directly into the <code>PingSource</code>. Each of the steps simply tacks on \"- Handled by \", for example the first Step in the <code>Sequence</code> will take the incoming message and append \"- Handled by 0\" to the incoming message. <p>See Sequence with no reply (terminal last Step).</p>"},{"location":"eventing/flows/sequence/#sequence-with-reply","title":"Sequence with reply","text":"<p>For the next example, we'll use the same 3 Step <code>Sequence</code> that is wired directly into the <code>PingSource</code>. Each of the steps simply tacks on \"- Handled by \", for example the first Step in the <code>Sequence</code> will take the incoming message and append \"- Handled by 0\" to the incoming message. <p>The only difference is that we'll use the <code>Subscriber.Spec.Reply</code> field to wire the output of the last Step to an event display pod.</p> <p>See Sequence with reply (last Step produces output).</p>"},{"location":"eventing/flows/sequence/#chaining-sequences-together","title":"Chaining Sequences together","text":"<p>For the next example, we'll use the same 3 Step <code>Sequence</code> that is wired directly into the <code>PingSource</code>. Each of the steps simply tacks on \"- Handled by \", for example the first Step in the <code>Sequence</code> will take the incoming message and append \"- Handled by 0\" to the incoming message. <p>The only difference is that we'll use the <code>Subscriber.Spec.Reply</code> field to wire the output of the last Step to another <code>Sequence</code> that does the same message modifications as the first pipeline (with different steps however).</p> <p>See Chaining Sequences together.</p>"},{"location":"eventing/flows/sequence/#using-sequence-with-brokertrigger-model","title":"Using Sequence with Broker/Trigger model","text":"<p>You can also create a Trigger which targets <code>Sequence</code>. This time we'll wire <code>PingSource</code> to send events to a <code>Broker</code> and then we'll have the <code>Sequence</code> emit the resulting Events back into the Broker so that the results of the <code>Sequence</code> can be observed by other Triggers.</p> <p>See Using Sequence with Broker/Trigger model.</p>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/","title":"Sequence wired to event-display","text":"<p>We are going to create the following logical configuration. We create a PingSource, feeding events to a <code>Sequence</code>, then taking the output of that <code>Sequence</code> and displaying the resulting output.</p> <p></p> <p>The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go.</p>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#prerequisites","title":"Prerequisites","text":"<p>For this example, we'll assume you have set up an <code>InMemoryChannel</code> as well as Knative Serving (for our functions). The examples use <code>default</code> namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this.</p> <p>If you want to use different type of <code>Channel</code>, you will have to modify the <code>Sequence.Spec.ChannelTemplate</code> to create the appropriate Channel resources.</p>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#setup","title":"Setup","text":""},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-knative-services","title":"Create the Knative Services","text":"<p>Change <code>default</code> in the following command to create the steps in the namespace where you want resources created:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: first\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 0\"\n\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: second\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 1\"\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: third\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 2\"\n---\n</code></pre> <pre><code>kubectl -n default create -f ./steps.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-sequence","title":"Create the Sequence","text":"<p>The <code>sequence.yaml</code> file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel.</p> <pre><code>apiVersion: flows.knative.dev/v1\nkind: Sequence\nmetadata:\n  name: sequence\nspec:\n  channelTemplate:\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n  steps:\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: first\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: second\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: third\n  reply:\n    ref:\n      kind: Service\n      apiVersion: serving.knative.dev/v1\n      name: event-display\n</code></pre> <p>Change <code>default</code> in the following command to create the <code>Sequence</code> in the namespace where you want the resources to be created:</p> <pre><code>kubectl -n default create -f ./sequence.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-service-displaying-the-events-created-by-sequence","title":"Create the Service displaying the events created by Sequence","text":"<pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: event-display\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> <p>Change <code>default</code> in the following command to create the <code>Sequence</code> in the namespace where you want your resources to be created:</p> <pre><code>kubectl -n default create -f ./event-display.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-pingsource-targeting-the-sequence","title":"Create the PingSource targeting the Sequence","text":"<p>This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: ping-source\nspec:\n  schedule: \"*/2 * * * *\"\n  contentType: \"application/json\"\n  data: '{\"message\": \"Hello world!\"}'\n  sink:\n    ref:\n      apiVersion: flows.knative.dev/v1\n      kind: Sequence\n      name: sequence\n</code></pre> <pre><code>kubectl -n default create -f ./ping-source.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#inspecting-the-results","title":"Inspecting the results","text":"<p>You can now see the final output by inspecting the logs of the event-display pods.</p> <pre><code>kubectl -n default get pods\n</code></pre> <p>Wait a bit and then look at the logs for the event-display pod:</p> <pre><code>kubectl -n default logs -l serving.knative.dev/service=event-display -c user-container --tail=-1\n\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: samples.http.mode3\n  source: /apis/v1/namespaces/default/pingsources/ping-source\n  id: e8fa7906-ab62-4e61-9c13-a9406e2130a9\n  time: 2020-03-02T20:52:00.0004957Z\n  datacontenttype: application/json\nExtensions,\n  knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local\n  traceparent: 00-6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00\nData,\n  {\n    \"id\": 0,\n    \"message\": \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\"\n  }\n</code></pre> <p>And you can see that the initial PingSource message <code>(\"Hello World!\")</code> has been appended to it by each of the steps in the Sequence.</p>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/","title":"Sequence wired to another Sequence","text":"<p>We are going to create the following logical configuration. We create a PingSource, feeding events to a <code>Sequence</code>, then taking the output of that <code>Sequence</code> and sending it to a second <code>Sequence</code> and finally displaying the resulting output.</p> <p></p> <p>The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go.</p>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#prerequisites","title":"Prerequisites","text":"<p>For this example, we'll assume you have set up an <code>InMemoryChannel</code> as well as Knative Serving (for our functions). The examples use <code>default</code> namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this.</p> <p>If you want to use different type of <code>Channel</code>, you will have to modify the <code>Sequence.Spec.ChannelTemplate</code> to create the appropriate Channel resources.</p>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#setup","title":"Setup","text":""},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-knative-services","title":"Create the Knative Services","text":"<p>Change <code>default</code> in the following command to create the steps in the namespace where you want resources created:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: first\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 0\"\n\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: second\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 1\"\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: third\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 2\"\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: fourth\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 3\"\n\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: fifth\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 4\"\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: sixth\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 5\"\n---\n</code></pre> <pre><code>kubectl -n default create -f ./steps.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-first-sequence","title":"Create the first Sequence","text":"<p>The <code>sequence1.yaml</code> file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel.</p> <pre><code>apiVersion: flows.knative.dev/v1\nkind: Sequence\nmetadata:\n  name: first-sequence\nspec:\n  channelTemplate:\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n  steps:\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: first\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: second\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: third\n  reply:\n    ref:\n      kind: Sequence\n      apiVersion: flows.knative.dev/v1\n      name: second-sequence\n</code></pre> <p>Change <code>default</code> in the following command to create the <code>Sequence</code> in the namespace where you want your resources created:</p> <pre><code>kubectl -n default create -f ./sequence1.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-second-sequence","title":"Create the second Sequence","text":"<p>The <code>sequence2.yaml</code> file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel.</p> <pre><code>apiVersion: flows.knative.dev/v1\nkind: Sequence\nmetadata:\n  name: second-sequence\nspec:\n  channelTemplate:\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n  steps:\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: fourth\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: fifth\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: sixth\n  reply:\n    ref:\n      kind: Service\n      apiVersion: serving.knative.dev/v1\n      name: event-display\n</code></pre> <pre><code>kubectl -n default create -f ./sequence2.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-service-displaying-the-events-created-by-sequence","title":"Create the Service displaying the events created by Sequence","text":"<pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: event-display\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> <p>Change <code>default</code> in the following command to create the <code>Sequence</code> in the namespace where you want your resources created:</p> <pre><code>kubectl -n default create -f ./event-display.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-pingsource-targeting-the-first-sequence","title":"Create the PingSource targeting the first Sequence","text":"<p>This will create a PingSource which will send a CloudEvent with <code>{\"message\": \"Hello world!\"}</code> as the data payload every 2 minutes.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: ping-source\nspec:\n  schedule: \"*/2 * * * *\"\n  contentType: \"application/json\"\n  data: '{\"message\": \"Hello world!\"}'\n  sink:\n    ref:\n      apiVersion: flows.knative.dev/v1\n      kind: Sequence\n      name: first-sequence\n</code></pre> <pre><code>kubectl -n default create -f ./ping-source.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#inspecting-the-results","title":"Inspecting the results","text":"<p>You can now see the final output by inspecting the logs of the event-display pods.</p> <pre><code>kubectl -n default get pods\n</code></pre> <p>Then look at the logs for the event-display pod:</p> <pre><code>kubectl -n default logs -l serving.knative.dev/service=event-display -c user-container --tail=-1\n\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.sources.ping\n  source: /apis/v1/namespaces/default/pingsources/ping-source\n  id: 29d531df-78d8-4d11-9ffd-ba24045241a9\n  time: 2020-03-02T21:18:00.0011708Z\n  datacontenttype: application/json\nExtensions,\n  knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local\n  traceparent: 00-e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00\nData,\n  {\n    \"id\": 0,\n    \"message\": \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\"\n  }\n</code></pre> <p>And you can see that the initial PingSource message <code>(\"Hello World!\")</code> has been appended to it by each of the steps in the Sequence.</p>"},{"location":"eventing/flows/sequence/sequence-terminal/","title":"Sequence terminal","text":"<p>We are going to create the following logical configuration. We create a PingSource, feeding events to a <code>Sequence</code>. Sequence can then do either external work, or out of band create additional events.</p> <p></p> <p>The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go.</p>"},{"location":"eventing/flows/sequence/sequence-terminal/#prerequisites","title":"Prerequisites","text":"<p>For this example, we'll assume you have set up an <code>InMemoryChannel</code> as well as Knative Serving (for our functions). The examples use <code>default</code> namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this.</p> <p>If you want to use different type of <code>Channel</code>, you will have to modify the <code>Sequence.Spec.ChannelTemplate</code> to create the appropriate Channel resources.</p>"},{"location":"eventing/flows/sequence/sequence-terminal/#setup","title":"Setup","text":""},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-knative-services","title":"Create the Knative Services","text":"<p>First create the 3 steps that will be referenced in the Steps.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: first\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 0\"\n\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: second\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 1\"\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: third\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 2\"\n---\n</code></pre> <pre><code>kubectl -n default create -f ./steps.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-sequence","title":"Create the Sequence","text":"<p>The <code>sequence.yaml</code> file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel.</p> <pre><code>apiVersion: flows.knative.dev/v1\nkind: Sequence\nmetadata:\n  name: sequence\nspec:\n  channelTemplate:\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n  steps:\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: first\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: second\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: third\n</code></pre> <p>Change <code>default</code> in the following command to create the <code>Sequence</code> in the namespace where you want the resources to be created:</p> <pre><code>kubectl -n default create -f ./sequence.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-pingsource-targeting-the-sequence","title":"Create the PingSource targeting the Sequence","text":"<p>This will create a PingSource which will send a CloudEvent with <code>{\"message\": \"Hello world!\"}</code> as the data payload every 2 minutes.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: ping-source\nspec:\n  schedule: \"*/2 * * * *\"\n  contentType: \"application/json\"\n  data: '{\"message\": \"Hello world!\"}'\n  sink:\n    ref:\n      apiVersion: flows.knative.dev/v1\n      kind: Sequence\n      name: sequence\n</code></pre> <pre><code>kubectl -n default create -f ./ping-source.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-terminal/#inspecting-the-results","title":"Inspecting the results","text":"<p>You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the <code>PingSource</code> to emit every 2 minutes, it might take some time for the events to show up in the logs.</p> <pre><code>kubectl -n default get pods\n</code></pre> <p>Let's look at the logs for the first <code>Step</code> in the <code>Sequence</code>:</p> <p><pre><code>kubectl -n default logs -l serving.knative.dev/service=first -c user-container --tail=-1\n\n2020/03/02 21:28:00 listening on 8080, appending \" - Handled by 0\" to events\n2020/03/02 21:28:01 Received a new event:\n2020/03/02 21:28:01 [2020-03-02T21:28:00.0010247Z] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: &amp;{Sequence:0 Message:Hello world!}\n2020/03/02 21:28:01 Transform the event to:\n2020/03/02 21:28:01 [2020-03-02T21:28:00.0010247Z] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: &amp;{Sequence:0 Message:Hello world! - Handled by 0}\n</code></pre> And you can see that the initial PingSource message <code>(\"Hello World!\")</code> has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :)</p> <p>Then we can look at the output of the second Step in the <code>Sequence</code>:</p> <p><pre><code>kubectl -n default logs -l serving.knative.dev/service=second -c user-container --tail=-1\n\n2020/03/02 21:28:02 listening on 8080, appending \" - Handled by 1\" to events\n2020/03/02 21:28:02 Received a new event:\n2020/03/02 21:28:02 [2020-03-02T21:28:00.0010247Z] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: &amp;{Sequence:0 Message:Hello world! - Handled by 0}\n2020/03/02 21:28:02 Transform the event to:\n2020/03/02 21:28:02 [2020-03-02T21:28:00.0010247Z] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: &amp;{Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1}\n</code></pre> And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\"</p> <p>Then we can look at the output of the last Step in the <code>Sequence</code>:</p> <pre><code>kubectl -n default logs -l serving.knative.dev/service=third -c user-container --tail=-1\n\n2020/03/02 21:28:03 listening on 8080, appending \" - Handled by 2\" to events\n2020/03/02 21:28:03 Received a new event:\n2020/03/02 21:28:03 [2020-03-02T21:28:00.0010247Z] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: &amp;{Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1}\n2020/03/02 21:28:03 Transform the event to:\n2020/03/02 21:28:03 [2020-03-02T21:28:00.0010247Z] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: &amp;{Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2}\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/","title":"Using Sequence with Broker and Trigger","text":"<p>We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a <code>Filter</code> that wires those events into a <code>Sequence</code> consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events.</p>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#prerequisites","title":"Prerequisites","text":"<ul> <li>Knative Serving</li> <li><code>InMemoryChannel</code></li> </ul> <p>Note</p> <p>The examples use the <code>default</code> namespace.</p> <p>If you want to use different type of <code>Channel</code>, you will have to modify the <code>Sequence.Spec.ChannelTemplate</code> to create the appropriate Channel resources.</p> <p></p> <p>The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go.</p>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#setup","title":"Setup","text":""},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#creating-the-broker","title":"Creating the Broker","text":"<ol> <li> <p>To create the cluster default Broker type, copy the following YAML into a file:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n name: default\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-knative-services","title":"Create the Knative Services","text":"<pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: first\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 0\"\n\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: second\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 1\"\n---\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: third\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/appender\n          env:\n            - name: MESSAGE\n              value: \" - Handled by 2\"\n            - name: TYPE\n              value: \"samples.http.mod3\"\n---\n</code></pre> <p>Change <code>default</code> in the following command to create the services in the namespace where you have configured your broker:</p> <pre><code>kubectl -n default create -f ./steps.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-sequence","title":"Create the Sequence","text":"<p>The <code>sequence.yaml</code> file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel.</p> <p>Also, change the spec.reply.name to point to your <code>Broker</code></p> <pre><code>apiVersion: flows.knative.dev/v1\nkind: Sequence\nmetadata:\n  name: sequence\nspec:\n  channelTemplate:\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n  steps:\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: first\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: second\n    - ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: third\n  reply:\n    ref:\n      kind: Broker\n      apiVersion: eventing.knative.dev/v1\n      name: default\n</code></pre> <p>Change <code>default</code> in the following command to create the sequence in the namespace where you have configured your broker:</p> <pre><code>kubectl -n default create -f ./sequence.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-pingsource-targeting-the-broker","title":"Create the PingSource targeting the Broker","text":"<p>This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: ping-source\nspec:\n  schedule: \"*/2 * * * *\"\n  contentType: \"application/json\"\n  data: '{\"message\": \"Hello world!\"}'\n  sink:\n    ref:\n      apiVersion: eventing.knative.dev/v1\n      kind: Broker\n      name: default\n</code></pre> <p>Change <code>default</code> in the following command to create the PingSource in the namespace where you have configured your broker and sequence:</p> <pre><code>kubectl -n default create -f ./ping-source.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-trigger-targeting-the-sequence","title":"Create the Trigger targeting the Sequence","text":"<pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: sequence-trigger\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: dev.knative.sources.ping\n  subscriber:\n    ref:\n      apiVersion: flows.knative.dev/v1\n      kind: Sequence\n      name: sequence\n</code></pre> <p>Change <code>default</code> in the following command to create the trigger in the namespace where you have configured your broker and sequence:</p> <pre><code>kubectl -n default create -f ./trigger.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-service-and-trigger-displaying-the-events-created-by-sequence","title":"Create the Service and Trigger displaying the events created by Sequence","text":"<pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: sequence-display\nspec:\n  template:\n    spec:\n      containers:\n        - image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n---\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: display-trigger\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: samples.http.mod3\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: sequence-display\n---\n</code></pre> <p>Change <code>default</code> in the following command to create the service and trigger in the namespace where you have configured your broker:</p> <pre><code>kubectl -n default create -f ./display-trigger.yaml\n</code></pre>"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#inspecting-the-results","title":"Inspecting the results","text":"<p>You can now see the final output by inspecting the logs of the sequence-display pods.</p> <pre><code>kubectl -n default get pods\n</code></pre> <p>View the logs for the <code>sequence-display</code> pod:</p> <pre><code>kubectl -n default logs -l serving.knative.dev/service=sequence-display -c user-container --tail=-1\n\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: samples.http.mod3\n  source: /apis/v1/namespaces/default/pingsources/ping-source\n  id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2\n  time: 2020-03-03T14:56:00.000652027Z\n  datacontenttype: application/json\nExtensions,\n  knativearrivaltime: 2020-03-03T14:56:00.018390608Z\n  knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local; default-kne-trigger-kn-channel.default.svc.cluster.local\n  traceparent: 00-e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00\nData,\n  {\n    \"id\": 0,\n    \"message\": \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\"\n  }\n</code></pre> <p>And you can see that the initial PingSource message <code>{\"Hello World!\"}</code> has been appended to it by each of the steps in the Sequence.</p>"},{"location":"eventing/observability/logging/collecting-logs/","title":"Logging","text":"<p>You can use Fluent Bit, a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving, which automatically deletes pods and associated logs when they are no longer needed.</p> <p>Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders.</p>"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-logging-components","title":"Setting up logging components","text":"<p>Setting up log collection requires two steps:</p> <ol> <li>Running a log forwarding DaemonSet on each node.</li> <li>Running a collector somewhere in the cluster.</li> </ol> <p>Tip</p> <p>In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath.</p>"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-the-collector","title":"Setting up the collector","text":"<p>The <code>fluent-bit-collector.yaml</code> file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called <code>logging</code>.</p> <p>Important</p> <p>Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready.</p> <p></p>"},{"location":"eventing/observability/logging/collecting-logs/#procedure","title":"Procedure","text":"<ol> <li> <p>Apply the configuration by entering the command:</p> <p><pre><code>kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml\n</code></pre> The default configuration will classify logs into:</p> <ul> <li>Knative services, or pods with an <code>app=Knative</code> label.</li> <li>Non-Knative apps.</li> </ul> <p>Note</p> <p>Logs default to logging with the pod name; this can be changed by updating the <code>log-collector-config</code> ConfigMap before or after installation.</p> <p>Warning</p> <p>After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it.</p> </li> <li> <p>To access the logs through your web browser, enter the command:</p> <pre><code>kubectl port-forward --namespace logging service/log-collector 8080:80\n</code></pre> </li> <li> <p>Navigate to <code>http://localhost:8080/</code>.</p> </li> <li> <p>Optional: You can open a shell in the <code>nginx</code> pod and search the logs using Unix tools, by entering the command:</p> <pre><code>kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0\n</code></pre> </li> </ol>"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-the-forwarders","title":"Setting up the forwarders","text":"<p>See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default.</p> <p>When you create a ConfigMap during the installation steps, you must:</p> <ul> <li>Replace the ElasticSearch configuration with the <code>fluent-bit-configmap.yaml</code>, or</li> <li> <p>Add the following block to the ConfigMap, and update the <code>@INCLUDE output-elasticsearch.conf</code> to be <code>@INCLUDE output-forward.conf</code>:</p> <pre><code>output-forward.conf: |\n  [OUTPUT]\n      Name            forward\n      Host            log-collector.logging\n      Port            24224\n      Require_ack_response  True\n</code></pre> </li> </ul>"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-a-local-collector","title":"Setting up a local collector","text":"<p>Warning</p> <p>This procedure describes a development environment setup and is not suitable for production use.</p> <p>If you are using a local Kubernetes cluster for development, you can create a <code>hostPath</code> PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools.</p> <p>The <code>PersistentVolumeClaim</code> will look similar to the following:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: shared-logs\n  labels:\n    app: logs-collector\nspec:\n  accessModes:\n    - \"ReadWriteOnce\"\n  storageClassName: manual\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: logs-log-collector-0\n    namespace: logging\n  capacity:\n    storage: 5Gi\n  hostPath:\n    path: &lt;see below&gt;\n</code></pre> <p>Note</p> <p>The <code>hostPath</code> will vary based on your Kubernetes software and host operating system.</p> <p>You must update the StatefulSet <code>volumeClaimTemplates</code> to reference the <code>shared-logs</code> volume, as shown in the following example:</p> <pre><code>volumeClaimTemplates:\n  metadata:\n    name: logs\n  spec:\n    accessModes: [\"ReadWriteOnce\"]\n    volumeName: shared-logs\n</code></pre>"},{"location":"eventing/observability/logging/collecting-logs/#kind","title":"Kind","text":"<p>When creating your cluster, you must use a <code>kind-config.yaml</code> and specify <code>extraMounts</code> for each node, as shown in the following example:</p> <pre><code>apiversion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n  - role: control-plane\n    extraMounts:\n      - hostPath: ./logs\n        containerPath: /shared/logs\n  - role: worker\n    extraMounts:\n      - hostPath: ./logs\n        containerPath: /shared/logs\n</code></pre> <p>You can then use <code>/shared/logs</code> as the <code>spec.hostPath.path</code> in your PersistentVolume. Note that the directory path <code>./logs</code> is relative to the directory that the Kind cluster was created in.</p>"},{"location":"eventing/observability/logging/collecting-logs/#docker-desktop","title":"Docker Desktop","text":"<p>Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems:</p> Host OS <code>hostPath</code> Mac OS <code>/Users/${USER}</code> Windows <code>/run/desktop/mnt/host/c/Users/${USER}/</code> Linux <code>/home/${USER}</code>"},{"location":"eventing/observability/logging/collecting-logs/#minikube","title":"Minikube","text":"<p>Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes.</p> <p>The following command mounts the <code>logs</code> directory inside the current directory onto <code>/mnt/logs</code> in the VM:</p> <pre><code>minikube mount ./logs:/mnt/logs\n</code></pre> <p>You must also reference <code>/mnt/logs</code> as the <code>hostPath.path</code> in the PersistentVolume.</p>"},{"location":"eventing/observability/logging/config-logging/","title":"Configuring Log Settings","text":"<p>Log configuration for all Knative components is managed through the <code>config-logging</code> ConfigMap in the corresponding namespace. For example, Serving components are configured through <code>config-logging</code> in the <code>knative-serving</code> namespace and Eventing components are configured through <code>config-logging</code> in the <code>knative-eventing</code> namespace, etc.</p> <p>Knative components use the zap logging library; options are documented in more detail in that project.</p> <p>In addition to <code>zap-logger-config</code>, which is a general key that applies to all components in that namespace, the <code>config-logging</code> ConfigMap supports overriding the log level for individual components.</p> ConfigMap key Description <code>zap-logger-config</code> A JSON object container for a zap logger configuration. Key fields are highlighted below. <code>zap-logger-config.level</code> The default logging level for components. Messages at or above this severity level will be logged. <code>zap-logger-config.encoding</code> The log encoding format for component logs (defaults to JSON). <code>zap-logger-config.encoderConfig</code> A <code>zap</code> EncoderConfig used to customize record contents. <code>loglevel.&lt;component&gt;</code> Overrides logging level for the given component only. Messages at or above this severity level will be logged. <p>Log levels supported by Zap are:</p> <ul> <li><code>debug</code> - fine-grained debugging</li> <li><code>info</code> - normal logging</li> <li><code>warn</code> - unexpected but non-critical errors</li> <li><code>error</code> - critical errors; unexpected during normal operation</li> <li><code>dpanic</code> - in debug mode, trigger a panic (crash)</li> <li><code>panic</code> - trigger a panic (crash)</li> <li><code>fatal</code> - immediately exit with exit status 1 (failure)</li> </ul>"},{"location":"eventing/observability/metrics/collecting-metrics/","title":"Collecting Metrics in Knative","text":"<p>Knative supports different popular tools for collecting metrics:</p> <ul> <li>Prometheus</li> <li>OpenTelemetry Collector</li> </ul> <p>Grafana dashboards are available for metrics collected directly with Prometheus.</p> <p>You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry.</p> <p>Warning</p> <p>You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove <code>metrics.backend-destination</code> and <code>metrics.request-metrics-backend-destination</code> keys from the config-observability Configmap to enable Prometheus metrics.</p>"},{"location":"eventing/observability/metrics/collecting-metrics/#about-the-prometheus-stack","title":"About the Prometheus Stack","text":"<p>Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used.</p> <p>Grafana is an open-source platform for data analytics and visualization, enabling users to create customizable dashboards for monitoring and analyzing metrics from various data sources.</p> <p>Prometheus Stack is a preconfigured collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules, combined to provide end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. The stack includes by default some Prometheus packages and Grafana.</p>"},{"location":"eventing/observability/metrics/collecting-metrics/#setting-up-the-prometheus-stack","title":"Setting up the Prometheus Stack","text":"<ol> <li> <p>Install the Prometheus Stack by using Helm:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml\n# values.yaml contains at minimum the configuration below\n</code></pre> <p>Caution</p> <p>You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. <pre><code>kube-state-metrics:\n  metricLabelsAllowlist:\n    - pods=[*]\n    - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance]\nprometheus:\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n    podMonitorSelectorNilUsesHelmValues: false\n</code></pre></p> </li> <li> <p>Apply the ServiceMonitors/PodMonitors to collect metrics from Knative.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml\n</code></pre> </li> </ol>"},{"location":"eventing/observability/metrics/collecting-metrics/#access-the-prometheus-instance-locally","title":"Access the Prometheus instance locally","text":"<p>By default, the Prometheus instance is only exposed on a private service named <code>prometheus-kube-prometheus-prometheus</code>.</p> <p>To access the console in your web browser:</p> <ol> <li> <p>Enter the command:</p> <pre><code>kubectl port-forward -n default svc/prometheus-kube-prometheus-prometheus 9090:9090\n</code></pre> </li> <li> <p>Access the console in your browser via <code>http://localhost:9090</code>.</p> </li> </ol>"},{"location":"eventing/observability/metrics/collecting-metrics/#access-the-grafana-instance-locally","title":"Access the Grafana instance locally","text":"<p>By default, the Grafana instance is only exposed on a private service named <code>prometheus-grafana</code>.</p> <p>To access the dashboards in your web browser:</p> <ol> <li> <p>Enter the command:</p> <pre><code>kubectl port-forward -n default svc/prometheus-grafana 3000:80\n</code></pre> </li> <li> <p>Access the dashboards in your browser via <code>http://localhost:3000</code>.</p> </li> <li> <p>Use the default credentials to login:</p> <pre><code>username: admin\npassword: prom-operator\n</code></pre> </li> </ol>"},{"location":"eventing/observability/metrics/collecting-metrics/#import-grafana-dashboards","title":"Import Grafana dashboards","text":"<ol> <li> <p>Grafana dashboards can be imported from the <code>monitoring</code> repository.</p> </li> <li> <p>If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/grafana/dashboards.yaml\n</code></pre> <p>Caution</p> <p>You will need to ensure that the helm chart has following values configured, otherwise the dashboards loading will not work. <pre><code>grafana:\n  sidecar:\n    dashboards:\n      enabled: true\n      searchNamespace: ALL\n</code></pre> If you have an existing configmap and the dashboards loading doesn't work, add the <code>labelValue: true</code> attribute to the helm chart after the <code>searchNamespace: ALL</code> declaration.</p> </li> </ol>"},{"location":"eventing/observability/metrics/collecting-metrics/#about-opentelemetry","title":"About OpenTelemetry","text":"<p>OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs.</p> <p>You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components.</p> <p>OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.</p>"},{"location":"eventing/observability/metrics/collecting-metrics/#understanding-the-collector","title":"Understanding the collector","text":"<p>The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service.</p> <p>In the following example, you can configure a single collector instance using a ConfigMap and a Deployment.</p> <p>Tip</p> <p>For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator.</p> <p>Caution</p> <p>The Grafana dashboards at https://github.com/knative-extensions/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector.</p> <p></p>"},{"location":"eventing/observability/metrics/collecting-metrics/#set-up-the-collector","title":"Set up the collector","text":"<ol> <li> <p>Create a namespace for the collector to run in, by entering the following command:</p> <p><pre><code>kubectl create namespace metrics\n</code></pre> The next step uses the <code>metrics</code> namespace for creating the collector.</p> </li> <li> <p>Create a Deployment, Service, and ConfigMap for the collector by entering the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml\n</code></pre> </li> <li> <p>Update the <code>config-observability</code> ConfigMaps in the Knative Serving and    Eventing namespaces, by entering the follow command:</p> <pre><code>kubectl patch --namespace knative-serving configmap/config-observability \\\n  --type merge \\\n  --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'\nkubectl patch --namespace knative-eventing configmap/config-observability \\\n  --type merge \\\n  --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'\n</code></pre> </li> </ol>"},{"location":"eventing/observability/metrics/collecting-metrics/#verify-the-collector-setup","title":"Verify the collector setup","text":"<ol> <li> <p>You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command:</p> <pre><code>kubectl port-forward --namespace metrics deployment/otel-collector 8889\n</code></pre> </li> <li> <p>Fetch <code>http://localhost:8889/metrics</code> to see the exported metrics.</p> </li> </ol>"},{"location":"eventing/observability/metrics/eventing-metrics/","title":"Knative Eventing metrics","text":"<p>Administrators can view metrics for Knative Eventing components.</p>"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-ingress","title":"Broker - Ingress","text":"<p>Use the following metrics to debug how broker ingress performs and what events are dispatched via the ingress component.</p> <p>By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx).</p> Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_nameevent_typenamespace_nameresponse_coderesponse_code_classunique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_nameevent_typenamespace_nameresponse_coderesponse_code_classunique_name Milliseconds Stable"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-filter","title":"Broker - Filter","text":"<p>Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx).</p> Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_namecontainer_name=filter_typenamespace_nameresponse_coderesponse_code_classtrigger_nameunique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_namecontainer_namefilter_typenamespace_nameresponse_coderesponse_code_classtrigger_nameunique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_namecontainer_namefilter_typenamespace_nametrigger_nameunique_name Milliseconds Stable"},{"location":"eventing/observability/metrics/eventing-metrics/#in-memory-dispatcher","title":"In-memory Dispatcher","text":"<p>In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx).</p> Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_nameevent_type=namespace_name=response_coderesponse_code_classunique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_nameevent_typenamespace_name=response_coderesponse_code_classunique_name Milliseconds Stable <p>Note</p> <p>A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section.</p>"},{"location":"eventing/observability/metrics/eventing-metrics/#eventing-sources","title":"Eventing sources","text":"<p>Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected.</p> Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_sourceevent_typenamenamespace_nameresource_groupresponse_coderesponse_code_classresponse_errorresponse_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_sourceevent_typenamenamespace_nameresource_groupresponse_coderesponse_code_classresponse_errorresponse_timeout Dimensionless Stable"},{"location":"eventing/reference/eventing-api/","title":"Eventing API","text":"<p>This file is updated to the correct version from the eventing repo (docs/eventing-api.md) during the build.</p>"},{"location":"eventing/sinks/","title":"About sinks","text":"<p>When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources. Knative Services, Channels, and Brokers are all examples of sinks.</p> <p>Addressable objects receive and acknowledge an event delivered over HTTP to an address defined in their <code>status.address.url</code> field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface.</p> <p>Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed.</p>"},{"location":"eventing/sinks/#sink-as-a-parameter","title":"Sink as a parameter","text":"<p>Sink is used as a reference to an object that resolves to a URI to use as the sink.</p> <p>A <code>sink</code> definition supports the following fields:</p> Field Description Required or optional <code>ref</code> This points to an Addressable. Required if not using <code>uri</code> <code>ref.apiVersion</code> API version of the referent. Required if using <code>ref</code> <code>ref.kind</code> Kind of the referent. Required if using <code>ref</code> <code>ref.namespace</code> Namespace of the referent. If omitted this defaults to the object holding it. Optional <code>ref.name</code> Name of the referent. Required if using <code>ref</code> <code>uri</code> This can be an absolute URL with a non-empty scheme and non-empty host that points to the target or a relative URI. Relative URIs are resolved using the base URI retrieved from Ref. Required if not using <code>ref</code> <p>Note</p> <p>At least one of <code>ref</code> or <code>uri</code> is required. If both are specified, <code>uri</code> is resolved into the URL from the Addressable <code>ref</code> result.</p>"},{"location":"eventing/sinks/#sink-parameter-example","title":"Sink parameter example","text":"<p>Given the following YAML, if <code>ref</code> resolves into <code>\"http://mysink.default.svc.cluster.local\"</code>, then <code>uri</code> is added to this resulting in <code>\"http://mysink.default.svc.cluster.local/extra/path\"</code>.</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: bind-heartbeat\nspec:\n  ...\n  sink:\n    ref:\n      apiVersion: v1\n      kind: Service\n      namespace: default\n      name: mysink\n    uri: /extra/path\n</code></pre> <p>Contract</p> <p>This results in the <code>K_SINK</code> environment variable being set on the <code>subject</code> as <code>\"http://mysink.default.svc.cluster.local/extra/path\"</code>.</p>"},{"location":"eventing/sinks/#using-custom-resources-as-sinks","title":"Using custom resources as sinks","text":"<p>To use a Kubernetes custom resource (CR) as a sink for events, you must:</p> <ol> <li> <p>Make the CR Addressable. You must ensure that the CR contains a <code>status.address.url</code>. For more information, see the spec for Addressable resources.</p> </li> <li> <p>Create an Addressable-resolver ClusterRole to obtain the necessary RBAC rules for the sink to receive events.</p> <p>For example, you can create a <code>kafkasinks-addressable-resolver</code> ClusterRole to allow <code>get</code>, <code>list</code>, and <code>watch</code> access to KafkaSink objects and statuses:</p> <pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: kafkasinks-addressable-resolver\n  labels:\n    kafka.eventing.knative.dev/release: devel\n    duck.knative.dev/addressable: \"true\"\n# Do not use this role directly. These rules will be added to the \"addressable-resolver\" role.\nrules:\n  - apiGroups:\n      - eventing.knative.dev\n    resources:\n      - kafkasinks\n      - kafkasinks/status\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre> </li> </ol>"},{"location":"eventing/sinks/#filtering-events-sent-to-sinks-by-using-triggers","title":"Filtering events sent to sinks by using Triggers","text":"<p>You can connect a Trigger to a sink, so that events are filtered before they are sent to the sink. A sink that is connected to a Trigger is configured as a <code>subscriber</code> in the Trigger resource spec.</p> <p>For example:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: &lt;trigger-name&gt;\nspec:\n...\n  subscriber:\n    ref:\n      apiVersion: eventing.knative.dev/v1alpha1\n      kind: KafkaSink\n      name: &lt;kafka-sink-name&gt;\n</code></pre> <p>Where;</p> <ul> <li><code>&lt;trigger-name&gt;</code> is the name of the Trigger being connected to the sink.</li> <li><code>&lt;kafka-sink-name&gt;</code> is the name of a KafkaSink object.</li> </ul>"},{"location":"eventing/sinks/#specifying-sinks-using-the-kn-cli-sink-flag","title":"Specifying sinks using the kn CLI --sink flag","text":"<p>When you create an event-producing CR by using the Knative (<code>kn</code>) CLI, you can specify a sink where events are sent to from that resource, by using the <code>--sink</code> flag.</p> <p>The following example creates a SinkBinding that uses a Service, <code>http://event-display.svc.cluster.local</code>, as the sink:</p> <pre><code>kn source binding create bind-heartbeat \\\n  --namespace sinkbinding-example \\\n  --subject \"Job:batch/v1:app=heartbeat-cron\" \\\n  --sink http://event-display.svc.cluster.local \\\n  --ce-override \"sink=bound\"\n</code></pre> <p>The <code>svc</code> in <code>http://event-display.svc.cluster.local</code> determines that the sink is a Knative Service. Other default sink prefixes include Channel and Broker.</p> <p>Tip</p> <p>You can configure which resources can be used with the <code>--sink</code> flag for <code>kn</code> CLI commands by customizing <code>kn</code>.</p>"},{"location":"eventing/sinks/#supported-third-party-sink-types","title":"Supported third-party sink types","text":"Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream"},{"location":"eventing/sinks/kafka-sink/","title":"Knative Sink for Apache Kafka","text":"<p>The <code>KafkaSink</code> is an Apache Kafka-native Sink implementation persisting the incoming CloudEvent to a configurable Apache Kafka Topic. This page shows how to install and configure the Knative <code>KafkaSink</code>.</p>"},{"location":"eventing/sinks/kafka-sink/#prerequisites","title":"Prerequisites","text":"<p>You must have access to a Kubernetes cluster with Knative Eventing installed.</p>"},{"location":"eventing/sinks/kafka-sink/#installation","title":"Installation","text":"<ol> <li> <p>Install the Kafka controller:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> <li> <p>Install the KafkaSink data plane:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml\n</code></pre> </li> <li> <p>Verify that <code>kafka-controller</code> and <code>kafka-sink-receiver</code> Deployments are running:</p> <pre><code>kubectl get deployments.apps -n knative-eventing\n</code></pre> <p>Example output:</p> <pre><code>NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\neventing-controller            1/1     1            1           10s\neventing-webhook               1/1     1            1           9s\nkafka-controller               1/1     1            1           3s\nkafka-sink-receiver            1/1     1            1           5s\n</code></pre> </li> </ol>"},{"location":"eventing/sinks/kafka-sink/#kafkasink-example","title":"KafkaSink example","text":"<p>A KafkaSink object looks similar to the following:</p> <pre><code>apiVersion: eventing.knative.dev/v1alpha1\nkind: KafkaSink\nmetadata:\n  name: my-kafka-sink\n  namespace: default\nspec:\n  topic: mytopic\n  bootstrapServers:\n   - my-cluster-kafka-bootstrap.kafka:9092\n</code></pre>"},{"location":"eventing/sinks/kafka-sink/#output-topic-content-mode","title":"Output Topic Content Mode","text":"<p>The CloudEvent specification defines 2 modes to transport a CloudEvent: structured and binary.</p> <p>A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body.</p> <p>The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols.</p> <p>A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data.</p> <p>The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort.</p> <p>A KafkaSink object with a specified <code>contentMode</code> looks similar to the following:</p> <pre><code>apiVersion: eventing.knative.dev/v1alpha1\nkind: KafkaSink\nmetadata:\n  name: my-kafka-sink\n  namespace: default\nspec:\n  topic: mytopic\n  bootstrapServers:\n   - my-cluster-kafka-bootstrap.kafka:9092\n\n  # CloudEvent content mode of Kafka messages sent to the topic.\n  # Possible values:\n  # - structured\n  # - binary\n  #\n  # default: binary.\n  #\n  # CloudEvent spec references:\n  # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md#message\n  # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#33-structured-content-mode\n  # - https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/kafka-protocol-binding.md#32-binary-content-mode\n  contentMode: binary # or structured\n</code></pre>"},{"location":"eventing/sinks/kafka-sink/#security","title":"Security","text":"<p>Knative supports the following Apache Kafka security features:</p> <ul> <li>Authentication using <code>SASL</code> without encryption</li> <li>Authentication using <code>SASL</code> and encryption using <code>SSL</code></li> <li>Authentication and encryption using <code>SSL</code></li> <li>Encryption using <code>SSL</code> without client authentication</li> </ul>"},{"location":"eventing/sinks/kafka-sink/#enabling-security-features","title":"Enabling security features","text":"<p>To enable security features, in the KafkaSink spec, you can reference a secret:</p> <pre><code>apiVersion: eventing.knative.dev/v1alpha1\nkind: KafkaSink\nmetadata:\n   name: my-kafka-sink\n   namespace: default\nspec:\n   topic: mytopic\n   bootstrapServers:\n      - my-cluster-kafka-bootstrap.kafka:9092\n   auth:\n     secret:\n       ref:\n         name: my_secret\n</code></pre> <p>Note</p> <p>The secret <code>my_secret</code> must exist in the same namespace of the KafkaSink. Certificates and keys must be in <code>PEM</code> format._</p>"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl","title":"Authentication using SASL","text":"<p>Knative supports the following SASL mechanisms:</p> <ul> <li><code>PLAIN</code></li> <li><code>SCRAM-SHA-256</code></li> <li><code>SCRAM-SHA-512</code></li> </ul> <p>To use a specific SASL mechanism replace <code>&lt;sasl_mechanism&gt;</code> with the mechanism of your choice.</p>"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-without-encryption","title":"Authentication using SASL without encryption","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SASL_PLAINTEXT \\\n  --from-literal=sasl.mechanism=&lt;sasl_mechanism&gt; \\\n  --from-literal=user=&lt;my_user&gt; \\\n  --from-literal=password=&lt;my_password&gt;\n</code></pre>"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-and-encryption-using-ssl","title":"Authentication using SASL and encryption using SSL","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SASL_SSL \\\n  --from-literal=sasl.mechanism=&lt;sasl_mechanism&gt; \\\n  --from-file=ca.crt=caroot.pem \\\n  --from-literal=user=&lt;my_user&gt; \\\n  --from-literal=password=&lt;my_password&gt;\n</code></pre>"},{"location":"eventing/sinks/kafka-sink/#encryption-using-ssl-without-client-authentication","title":"Encryption using SSL without client authentication","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SSL \\\n  --from-file=ca.crt=&lt;my_caroot.pem_file_path&gt; \\\n  --from-literal=user.skip=true\n</code></pre>"},{"location":"eventing/sinks/kafka-sink/#authentication-and-encryption-using-ssl","title":"Authentication and encryption using SSL","text":"<pre><code>kubectl create secret --namespace &lt;namespace&gt; generic &lt;my_secret&gt; \\\n  --from-literal=protocol=SSL \\\n  --from-file=ca.crt=&lt;my_caroot.pem_file_path&gt; \\\n  --from-file=user.crt=&lt;my_cert.pem_file_path&gt; \\\n  --from-file=user.key=&lt;my_key.pem_file_path&gt;\n</code></pre> <p>Note</p> <p>The <code>ca.crt</code> can be omitted to enable fallback and use the system's root CA set.</p>"},{"location":"eventing/sinks/kafka-sink/#kafka-producer-configurations","title":"Kafka Producer configurations","text":"<p>A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the <code>config-kafka-sink-data-plane</code> ConfigMap in the <code>knative-eventing</code> namespace.</p> <p>Documentation for the settings available in this ConfigMap is available on the Apache Kafka website, in particular, Producer configurations.</p>"},{"location":"eventing/sinks/kafka-sink/#enable-debug-logging-for-data-plane-components","title":"Enable debug logging for data plane components","text":"<p>To enable debug logging for data plane components change the logging level to <code>DEBUG</code> in the <code>kafka-config-logging</code> ConfigMap.</p> <ol> <li> <p>Create the <code>kafka-config-logging</code> ConfigMap as a YAML file that contains the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-config-logging\n  namespace: knative-eventing\ndata:\n  config.xml: |\n    &lt;configuration&gt;\n      &lt;appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/&gt;\n      &lt;/appender&gt;\n      &lt;root level=\"DEBUG\"&gt;\n        &lt;appender-ref ref=\"jsonConsoleAppender\"/&gt;\n      &lt;/root&gt;\n    &lt;/configuration&gt;\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> <li> <p>Restart the <code>kafka-sink-receiver</code>:</p> <pre><code>kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver\n</code></pre> </li> </ol>"},{"location":"eventing/sources/","title":"Event sources","text":"<p>An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink. A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source.</p> <p>Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR.</p> <p>All Sources are part of the <code>sources</code> category.</p> knkubectl <p>You can list existing event sources on your cluster by entering the kn command:</p> <pre><code>kn source list\n</code></pre> <p>You can list existing event sources on your cluster by entering the command:</p> <pre><code>kubectl get sources\n</code></pre> <p>Note</p> <p>Event Sources that import events from other messaging technologies such as Kafka or RabbitMQ are not responsible for setting Optional Attributes such as the <code>datacontenttype</code>. This is a responsibility of the original event producer; the Source just appends attributes if they exist.</p>"},{"location":"eventing/sources/#knative-sources","title":"Knative Sources","text":"Name Status Maintainer Description APIServerSource Stable Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. Apache CouchDB Alpha Knative Brings Apache CouchDB messages into Knative. Apache Kafka Stable Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. CephSource Beta Knative The Ceph source converts bucket notifications from Ceph format into CloudEvents format, and inject them into Knative. Conversion logic follow the one described for AWS S3 bucket notifications. ContainerSource Stable Knative The ContainerSource instantiates container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a <code>spec.template</code> with at least a container image specified, the ContainerSource keeps a Pod running with the specified image(s). <code>K_SINK</code> (destination address) and <code>KE_CE_OVERRIDES</code> (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub Beta Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types. See the GitHub Source example for more details. GitLab Beta Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types, listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. KogitoSource Alpha Knative An implementation of the Kogito Runtime custom resource managed by the Kogito Operator. PingSource Stable Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Stable Knative Brings RabbitMQ messages into Knative. RedisSource Alpha Knative Brings Redis Stream into Knative. SinkBinding Stable Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting <code>K_SINK</code> (destination address) and <code>K_CE_OVERRIDES</code> (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a <code>spec.template</code> that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details."},{"location":"eventing/sources/#third-party-sources","title":"Third-Party Sources","text":"Name Status Maintainer Description Amazon CloudWatch Stable TriggerMesh Collects metrics from Amazon CloudWatch. (installation) (example) Amazon CloudWatch Logs Stable TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. (installation) (example) AWS CodeCommit Stable TriggerMesh Registers for events emitted by an AWS CodeCommit source code repository. (installation) (example) Amazon Cognito Identity Stable TriggerMesh Registers for events from Amazon Cognito identity pools. (installation) (example) Amazon Cognito User Stable TriggerMesh Registers for events from Amazon Cognito user pools. (installation) (example) Amazon DynamoDB Stable TriggerMesh Reads records from an Amazon DynamoDB stream. (installation) (example) Amazon Kinesis Stable TriggerMesh Reads records from an Amazon Kinesis stream. (installation) (example) Amazon S3 Stable TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. (installation) (example) Amazon SNS Stable TriggerMesh Subscribes to messages from an Amazon SNS topic. (installation) (example) Amazon SQS Stable TriggerMesh Consumes messages from an Amazon SQS queue. (installation) (example) Apache Camel Stable Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Azure Activity Logs Stable TriggerMesh Capture activity logs from Azure Activity Logs. (installation) (example) Azure Blob Storage Stable TriggerMesh Subscribes to events from an Azure Blob Storage account. (installation) (example) Azure Event Grid Stable TriggerMesh Retrieves events from Azure Event Grid. (installation) (example) Azure Event Hubs Stable TriggerMesh Consumes events from Azure Event Hubs. (installation) (example) Azure IoT Hub Stable TriggerMesh Consumes event from Azure IoT Hub. (installation) (example) Azure Queue Storage Stable TriggerMesh Retrieves messages from Azure Queue Storage. (installation) (example) Azure Service Bus Queues Stable TriggerMesh Consumes messages from an Azure Service Bus queue. (installation) (example) Azure Service Bus Topics Stable TriggerMesh Subscribes to messages from an Azure Service Bus topic. (installation) (example) Debezium Alpha Debezium Consume database changes as CloudEvents in Knative. (knative configuration) Direktiv Alpha Direktiv Receive events from Direktiv. DockerHubSource Alpha None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. Google Cloud Audit Logs Stable TriggerMesh Captures audit logs from Google Cloud Audit Logs. (installation) (example) Google Cloud Billing Stable TriggerMesh Captures budget notifications from Google Cloud Billing. (installation) (example) Google Cloud Pub/Sub Stable TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. (installation) (example) Google Cloud Source Repositories Stable TriggerMesh Consumes events from Google Cloud Source Repositories. (installation) (example) Google Cloud Storage Stable TriggerMesh Captures change notifications from a Google Cloud Storage bucket. (installation) (example) HTTP Poller Stable TriggerMesh Periodically pulls events from an HTTP/S URL. (installation) (example) Oracle Cloud Infrastructure Stable TriggerMesh Retrieves metrics from Oracle Cloud Infrastructure. (installation) (example) Salesforce Stable TriggerMesh Consumes events from a Salesforce channel. (installation) (example) Slack Stable TriggerMesh Subscribes to events from Slack. (installation) (example) Twilio Supported TriggerMesh Receive events from Twilio. (installation) (example) VMware Alpha VMware Brings vSphere events into Knative. Webhook Stable TriggerMesh Ingest events from a webhook using HTTP. (installation) (example) Zendesk Stable TriggerMesh Subscribes to events from Zendesk. (installation) (example)"},{"location":"eventing/sources/#additional-resources","title":"Additional resources","text":"<ul> <li>If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker.</li> <li>For more information about using <code>kn</code> Source related commands, see the <code>kn source</code> reference documentation.</li> </ul>"},{"location":"eventing/sources/apiserversource/","title":"About ApiServerSource","text":"<p>The API server source is a Knative Eventing Kubernetes custom resource that listens for events emitted by the Kubernetes API server (eg. pod creation, deployment updates, etc...) and forwards them as CloudEvents to a sink.</p> <p>The API server source is part of the core Knative Eventing component, and is provided by default when Knative Eventing is installed. Multiple instances of an ApiServerSource object can be created by users.</p>"},{"location":"eventing/sources/apiserversource/getting-started/","title":"Creating an ApiServerSource object","text":"<p>This topic describes how to create an ApiServerSource object.</p>"},{"location":"eventing/sources/apiserversource/getting-started/#before-you-begin","title":"Before you begin","text":"<p>Before you can create an ApiServerSource object:</p> <ul> <li>You must have Knative Eventing installed on your cluster.</li> <li>You must install the <code>kubectl</code> CLI tool.</li> <li>Optional: If you want to use the <code>kn</code> commands, install the <code>kn</code> tool.</li> </ul>"},{"location":"eventing/sources/apiserversource/getting-started/#create-an-apiserversource-object","title":"Create an ApiServerSource object","text":"<ol> <li> <p>Optional: Create a namespace for the API server source instance by running the command:</p> <p><pre><code>kubectl create namespace &lt;namespace&gt;\n</code></pre> Where <code>&lt;namespace&gt;</code> is the name of the namespace that you want to create.</p> <p>Note</p> <p>Creating a namespace for your ApiServerSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your <code>default</code> namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources.</p> </li> <li> <p>Create a ServiceAccount:</p> <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: &lt;service-account&gt;\n  namespace: &lt;namespace&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;service-account&gt;</code> is the name of the ServiceAccount that you want to create.</li> <li><code>&lt;namespace&gt;</code> is the namespace that you created in step 1 earlier.</li> </ul> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>Create a Role:</p> <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: &lt;role&gt;\n  namespace: &lt;namespace&gt;\nrules:\n  &lt;rules&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;role&gt;</code> is the name of the Role that you want to create.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</li> <li> <p><code>&lt;rules&gt;</code> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the <code>events</code> resource, use the following set of permissions: <pre><code>- apiGroups:\n  - \"\"\n  resources:\n  - events\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre></p> <p>Note</p> <p>The only required verbs are <code>get</code>, <code>list</code> and <code>watch</code>.</p> </li> </ul> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>Create a RoleBinding:</p> <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: &lt;role-binding&gt;\n  namespace: &lt;namespace&gt;\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: &lt;role&gt;\nsubjects:\n- kind: ServiceAccount\n  name: &lt;service-account&gt;\n  namespace: &lt;namespace&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;role-binding&gt;</code> is the name of the RoleBinding that you want to create.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</li> <li><code>&lt;role&gt;</code> is the name of the Role that you created in step 3 earlier.</li> <li><code>&lt;service-account&gt;</code> is the name of the ServiceAccount that you created in step 2 earlier.</li> </ul> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log:</p> <ol> <li> <p>Copy the YAML below into a file:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: event-display\n  namespace: &lt;namespace&gt;\nspec:\n  replicas: 1\n  selector:\n    matchLabels: &amp;labels\n      app: event-display\n  template:\n    metadata:\n      labels: *labels\n    spec:\n      containers:\n        - name: event-display\n          image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: event-display\n  namespace: &lt;namespace&gt;\nspec:\n  selector:\n    app: event-display\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>Where <code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 above.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>Create the ApiServerSource object:</p> knYAML <ul> <li> <p>To create the ApiServerSource, run the command:</p> <p><pre><code>kn source apiserver create &lt;apiserversource&gt; \\\n  --namespace &lt;namespace&gt; \\\n  --mode \"Resource\" \\\n  --resource \"Event:v1\" \\\n  --service-account &lt;service-account&gt; \\\n  --sink &lt;sink-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;apiserversource&gt;</code> is the name of the source that you want to create.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</li> <li><code>&lt;service-account&gt;</code> is the name of the ServiceAccount that you created in step 2 earlier.</li> <li><code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>http://event-display.pingsource-example.svc.cluster.local</code>.</li> </ul> <p>For a list of available options, see the Knative client documentation.</p> </li> </ul> <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: &lt;apiserversource-name&gt;\n namespace: &lt;namespace&gt;\nspec:\n serviceAccountName: &lt;service-account&gt;\n mode: &lt;event-mode&gt;\n resources:\n   - apiVersion: v1\n     kind: Event\n sink:\n   ref:\n     apiVersion: v1\n     kind: &lt;sink-kind&gt;\n     name: &lt;sink-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;apiserversource-name&gt;</code> is the name of the source that you want to create.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</li> <li><code>&lt;service-account&gt;</code> is the name of the ServiceAccount that you created in step 2 earlier.</li> <li><code>&lt;event-mode&gt;</code> is either <code>Resource</code> or <code>Reference</code>. If set to <code>Resource</code>, the event payload contains the entire resource that the event is for. If set to <code>Reference</code>, the event payload only contains a reference to the resource that the event is for. The default is <code>Reference</code>.</li> <li><code>&lt;sink-kind&gt;</code> is any supported Addressable object that you want to use as a sink, for example, a <code>Service</code> or <code>Deployment</code>.</li> <li><code>&lt;sink-name&gt;</code> is the name of your sink.</li> </ul> <p>For more information about the fields you can configure for the ApiServerSource object, see ApiServerSource reference.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> </ol>"},{"location":"eventing/sources/apiserversource/getting-started/#verify-the-apiserversource-object","title":"Verify the ApiServerSource object","text":"<ol> <li> <p>Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command:</p> <p><pre><code>kubectl run busybox --image=busybox --namespace=&lt;namespace&gt; --restart=Never -- ls\n</code></pre> Where <code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</p> </li> <li> <p>Delete the test Pod by running the command:</p> <p><pre><code>kubectl --namespace=&lt;namespace&gt; delete pod busybox\n</code></pre> Where <code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</p> </li> <li> <p>View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command:</p> <p><pre><code>kubectl logs --namespace=&lt;namespace&gt; -l app=&lt;sink&gt; --tail=100\n</code></pre> Where:</p> <ul> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</li> <li><code>&lt;sink&gt;</code> is the name of the PodSpecable object that you used as a sink in step 5 earlier.</li> </ul> <p>Example log output:</p> <pre><code>\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.apiserver.resource.update\n  source: https://10.96.0.1:443\n  subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50\n  id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0\n  time: 2020-07-28T19:14:54.719501054Z\n  datacontenttype: application/json\nExtensions,\n  kind: Event\n  name: busybox.1626008649e617e3\n  namespace: apiserversource-example\nData,\n  {\n    \"apiVersion\": \"v1\",\n    \"count\": 1,\n    \"eventTime\": null,\n    \"firstTimestamp\": \"2020-07-28T19:14:54Z\",\n    \"involvedObject\": {\n      \"apiVersion\": \"v1\",\n      \"fieldPath\": \"spec.containers{busybox}\",\n      \"kind\": \"Pod\",\n      \"name\": \"busybox\",\n      \"namespace\": \"apiserversource-example\",\n      \"resourceVersion\": \"28987493\",\n      \"uid\": \"1efb342a-737b-11e9-a6c5-42010a8a00ed\"\n    },\n    \"kind\": \"Event\",\n    \"lastTimestamp\": \"2020-07-28T19:14:54Z\",\n    \"message\": \"Started container\",\n    \"metadata\": {\n      \"creationTimestamp\": \"2020-07-28T19:14:54Z\",\n      \"name\": \"busybox.1626008649e617e3\",\n      \"namespace\": \"default\",\n      \"resourceVersion\": \"506088\",\n    \"selfLink\": \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\",\n      \"uid\": \"2005af47-737b-11e9-a6c5-42010a8a00ed\"\n    },\n    \"reason\": \"Started\",\n    \"reportingComponent\": \"\",\n    \"reportingInstance\": \"\",\n    \"source\": {\n      \"component\": \"kubelet\",\n      \"host\": \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\"\n    },\n    \"type\": \"Normal\"\n  }\n</code></pre> </li> </ol>"},{"location":"eventing/sources/apiserversource/getting-started/#delete-the-apiserversource-object","title":"Delete the ApiServerSource object","text":"<p>To remove the ApiServerSource object and all of the related resources:</p> <ul> <li> <p>Delete the namespace by running the command:</p> <p><pre><code>kubectl delete namespace &lt;namespace&gt;\n</code></pre> Where <code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 earlier.</p> </li> </ul>"},{"location":"eventing/sources/apiserversource/reference/","title":"ApiServerSource reference","text":"<p>This topic provides reference information about the configurable fields for the ApiServerSource object.</p>"},{"location":"eventing/sources/apiserversource/reference/#apiserversource","title":"ApiServerSource","text":"<p>An ApiServerSource definition supports the following fields:</p> Field Description Required or optional <code>apiVersion</code> Specifies the API version, for example <code>sources.knative.dev/v1</code>. Required <code>kind</code> Identifies this resource object as an ApiServerSource object. Required <code>metadata</code> Specifies metadata that uniquely identifies the ApiServerSource object. For example, a <code>name</code>. Required <code>spec</code> Specifies the configuration information for this ApiServerSource object. Required <code>spec.resources</code> The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required <code>spec.mode</code> EventMode controls the format of the event. Set to <code>Reference</code> to send a <code>dataref</code> event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to <code>Resource</code> to have the full resource lifecycle event in the payload. Defaults to <code>Reference</code>. Optional <code>spec.owner</code> ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional <code>spec.serviceAccountName</code> The name of the ServiceAccount to use to run this source. Defaults to <code>default</code> if not set. Optional <code>spec.sink</code> A reference to an object that resolves to a URI to use as the sink. Required <code>spec.ceOverrides</code> Defines overrides to control the output format and modifications to the event sent to the sink. Optional <code>spec.namespaceSelector</code> Specifies a label selector to track multiple namespaces. If unspecified, the namespace of the ApiServerSource will be tracked. Optional"},{"location":"eventing/sources/apiserversource/reference/#resources-parameter","title":"Resources parameter","text":"<p>The <code>resources</code> parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter.</p> <p>A <code>resources</code> definition supports the following fields:</p> Field Description Required or optional <code>apiVersion</code> API version of the resource to watch. Required <code>kind</code> Kind of the resource to watch. Required <code>selector</code> LabelSelector filters this source to objects to those resources pass the label selector.  Optional <code>selector.matchExpressions</code> A list of label selector requirements. The requirements are ANDed. Use one of <code>matchExpressions</code> or <code>matchLabels</code> <code>selector.matchExpressions.key</code> The label key that the selector applies to. Required if using <code>matchExpressions</code> <code>selector.matchExpressions.operator</code> Represents a key's relationship to a set of values. Valid operators are <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code>. Required if using <code>matchExpressions</code> <code>selector.matchExpressions.values</code> An array of string values. If <code>operator</code> is <code>In</code> or <code>NotIn</code>, the values array must be non-empty. If <code>operator</code> is <code>Exists</code> or <code>DoesNotExist</code>, the values array must be empty. This array is replaced during a strategic merge patch. Required if using <code>matchExpressions</code> <code>selector.matchLabels</code> A map of key-value pairs. Each key-value pair in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>, where the key field is <code>matchLabels.&lt;key&gt;</code>, the <code>operator</code> is <code>In</code>, and the <code>values</code> array contains only \"matchLabels.\". The requirements are ANDed. Use one of <code>matchExpressions</code> or <code>matchLabels</code>"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter","title":"Example: Resources parameter","text":"<p>Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n  name: &lt;apiserversource&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  # ...\n  resources:\n    - apiVersion: v1\n      kind: Pod\n    - apiVersion: apps/v1\n      kind: Deployment\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter-using-matchexpressions","title":"Example: Resources parameter using matchExpressions","text":"<p>Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label <code>app=myapp</code> or <code>app=yourapp</code>:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n  name: &lt;apiserversource&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  # ...\n  resources:\n    - apiVersion: v1\n      kind: Pod\n      selector:\n        matchExpressions:\n          - key: app\n            operator: In\n            values:\n              - myapp\n              - yourapp\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter-using-matchlabels","title":"Example: Resources parameter using matchLabels","text":"<p>Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label <code>app=myapp</code>:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n  name: &lt;apiserversource&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  # ...\n  resources:\n    - apiVersion: v1\n      kind: Pod\n      selector:\n        matchLabels:\n          app: myapp\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#serviceaccountname-parameter","title":"ServiceAccountName parameter","text":"<p>ServiceAccountName is a reference to a Kubernetes service account.</p> <p>To track the lifecycle events of the specified <code>resources</code>, you must assign the proper permissions to the ApiServerSource object.</p>"},{"location":"eventing/sources/apiserversource/reference/#example-tracking-pods","title":"Example: tracking Pods","text":"<p>The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace <code>apiserversource-example</code> for the ApiServerSource.</p> <p>Example ServiceAccount:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: test-service-account\n  namespace: apiserversource-example\n</code></pre> <p>Example Role with permission to get, list and watch Pod resources:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: test-role\nrules:\n  - apiGroups:\n    - \"\"\n    resources:\n    - pods\n    verbs:\n    - get\n    - list\n    - watch\n</code></pre> <p>Example RoleBinding:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: test-role-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: test-role\nsubjects:\n  - kind: ServiceAccount\n    name: test-service-account\n    namespace: apiserversource-example\n</code></pre> <p>Example ApiServerSource using <code>test-service-account</code>:</p> <pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: test-apiserversource\n namespace: apiserversource-example\nspec:\n  # ...\n  serviceAccountName: test-service-account\n  ...\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#owner-parameter","title":"Owner parameter","text":"<p>ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter.</p> <p>An <code>owner</code> definition supports the following fields:</p> Field Description Required or optional <code>apiVersion</code> API version of the resource to watch. Required <code>kind</code> Kind of the resource to watch. Required"},{"location":"eventing/sources/apiserversource/reference/#example-owner-parameter","title":"Example: Owner parameter","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: &lt;apiserversource&gt;\n namespace: &lt;namespace&gt;\nspec:\n  ...\n  owner:\n    apiVersion: apps/v1\n    kind: Deployment\n  ...\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#namespaceselector-parameter","title":"NamespaceSelector parameter","text":"<p>The NamespaceSelector is an optional label selector that can be utilized to target more than one namespace. If the selector is unset, the namespace of the ApiServerSource will be tracked.</p> <p>A <code>namespaceSelector</code> supports the following fields:</p> Field Description Required or optional <code>matchExpressions</code> A list of label selector requirements. The requirements are ANDed. Use one of <code>matchExpressions</code> or <code>matchLabels</code> <code>matchExpressions.key</code> The label key that the selector applies to. Required if using <code>matchExpressions</code> <code>matchExpressions.operator</code> Represents a key's relationship to a set of values. Valid operators are <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code>. Required if using <code>matchExpressions</code> <code>matchExpressions.values</code> An array of string values. If <code>operator</code> is <code>In</code> or <code>NotIn</code>, the values array must be non-empty. If <code>operator</code> is <code>Exists</code> or <code>DoesNotExist</code>, the values array must be empty. This array is replaced during a strategic merge patch. Required if using <code>matchExpressions</code> <code>matchLabels</code> A map of key-value pairs. Each key-value pair in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>, where the key field is <code>matchLabels.&lt;key&gt;</code>, the <code>operator</code> is <code>In</code>, and the <code>values</code> array contains only \"matchLabels.\". The requirements are ANDed. Use one of <code>matchExpressions</code> or <code>matchLabels</code>"},{"location":"eventing/sources/apiserversource/reference/#example-target-multiple-namespaces-with-matchexpressions","title":"Example: Target multiple namespaces with matchExpressions","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: &lt;apiserversource&gt;\n namespace: &lt;namespace&gt;\nspec:\n  ...\n  namespaceSelector:\n    matchExpressions:\n      - key: environment\n        operator: In\n        values:\n          - production\n          - development\n  ...\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#example-target-multiple-namespaces-with-matchlabels","title":"Example: Target multiple namespaces with matchLabels","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: &lt;apiserversource&gt;\n namespace: &lt;namespace&gt;\nspec:\n  ...\n  namespaceSelector:\n    matchLabels:\n      environment: production\n  ...\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#example-target-all-namespaces-with-an-empty-selector","title":"Example: Target all namespaces with an empty selector","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: &lt;apiserversource&gt;\n namespace: &lt;namespace&gt;\nspec:\n  ...\n  namespaceSelector: {}\n  ...\n</code></pre>"},{"location":"eventing/sources/apiserversource/reference/#cloudevent-overrides","title":"CloudEvent Overrides","text":"<p>CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink.</p> <p>A <code>ceOverrides</code> definition supports the following fields:</p> Field Description Required or optional <code>extensions</code> Specifies which attributes are added or overridden on the outbound event. Each <code>extensions</code> key-value pair is set independently on the event as an attribute extension. Optional <p>Note</p> <p>Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the <code>type</code> attribute.</p>"},{"location":"eventing/sources/apiserversource/reference/#example-cloudevent-overrides","title":"Example: CloudEvent Overrides","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: ApiServerSource\nmetadata:\n name: &lt;apiserversource&gt;\n namespace: &lt;namespace&gt;\nspec:\n  ...\n  ceOverrides:\n    extensions:\n      extra: this is an extra attribute\n      additional: 42\n</code></pre> <p>Contract</p> <p>This results in the <code>K_CE_OVERRIDES</code> environment variable being set on the sink container as follows:</p> <pre><code>{ \"extensions\": { \"extra\": \"this is an extra attribute\", \"additional\": \"42\" } }\n</code></pre>"},{"location":"eventing/sources/kafka-source/","title":"Knative Source for Apache Kafka","text":"<p>The <code>KafkaSource</code> reads messages stored in existing Apache Kafka topics, and sends those messages as CloudEvents through HTTP to its configured <code>sink</code>. The <code>KafkaSource</code> preserves the order of the messages stored in the topic partitions. It does this by waiting for a successful response from the <code>sink</code> before it delivers the next message in the same partition.</p>"},{"location":"eventing/sources/kafka-source/#install-the-kafkasource-controller","title":"Install the KafkaSource controller","text":"<ol> <li> <p>Install the <code>KafkaSource</code> controller by entering the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> <li> <p>Install the Kafka Source data plane by entering the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml\n</code></pre> </li> <li> <p>Verify that <code>kafka-controller</code> and <code>kafka-source-dispatcher</code> are running,    by entering the following command:</p> <pre><code>kubectl get deployments.apps,statefulsets.apps -n knative-eventing\n</code></pre> <p>Example output: <pre><code>NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kafka-controller               1/1     1            1           3s\n\nNAME                                       READY   AGE\nstatefulset.apps/kafka-source-dispatcher   1/1     3s\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/kafka-source/#optional-create-a-kafka-topic","title":"Optional: Create a Kafka topic","text":"<p>Note</p> <p>The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool.</p> <p>If you are using Strimzi:</p> <ol> <li> <p>Create a <code>KafkaTopic</code> YAML file:</p> <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n  name: knative-demo-topic\n  namespace: kafka\n  labels:\n    strimzi.io/cluster: my-cluster\nspec:\n  partitions: 3\n  replicas: 1\n  config:\n    retention.ms: 7200000\n    segment.bytes: 1073741824\n</code></pre> </li> <li> <p>Deploy the <code>KafkaTopic</code> YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of your <code>KafkaTopic</code> YAML file.</p> <p>Example output: <pre><code>kafkatopic.kafka.strimzi.io/knative-demo-topic created\n</code></pre></p> </li> <li> <p>Ensure that the <code>KafkaTopic</code> is running by running the command:</p> <pre><code>kubectl -n kafka get kafkatopics.kafka.strimzi.io\n</code></pre> <p>Example output: <pre><code>NAME                 CLUSTER      PARTITIONS   REPLICATION FACTOR\nknative-demo-topic   my-cluster   3            1\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/kafka-source/#create-a-service","title":"Create a Service","text":"<ol> <li> <p>Create the <code>event-display</code> Service as a YAML file:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: event-display\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - # This corresponds to\n          # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go\n          image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> <p>Example output: <pre><code>service.serving.knative.dev/event-display created\n</code></pre></p> </li> <li> <p>Ensure that the Service Pod is running, by running the command:</p> <pre><code>kubectl get pods\n</code></pre> <p>The Pod name is prefixed with <code>event-display</code>: <pre><code>NAME                                            READY     STATUS    RESTARTS   AGE\nevent-display-00001-deployment-5d5df6c7-gv2j4   2/2       Running   0          72s\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/kafka-source/#kafka-event-source","title":"Kafka event source","text":"<ol> <li> <p>Modify <code>source/event-source.yaml</code> accordingly with bootstrap servers, topics, and so on:</p> <pre><code>apiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\n  name: kafka-source\nspec:\n  consumerGroup: knative-group\n  bootstrapServers:\n  - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace\n  topics:\n  - knative-demo-topic\n  sink:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: event-display\n</code></pre> </li> <li> <p>Deploy the event source:</p> <pre><code>kubectl apply -f event-source.yaml\n</code></pre> <p>Example output: <pre><code>kafkasource.sources.knative.dev/kafka-source created\n</code></pre></p> </li> <li> <p>Verify that the KafkaSource is ready:</p> <pre><code>kubectl get kafkasource kafka-source\n</code></pre> <p>Example output: <pre><code>NAME           TOPICS                   BOOTSTRAPSERVERS                            READY   REASON   AGE\nkafka-source   [\"knative-demo-topic\"]   [\"my-cluster-kafka-bootstrap.kafka:9092\"]   True             26h\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/kafka-source/#scaling","title":"Scaling","text":"<p>To schedule more or fewer consumers, a KafkaSource can be scaled, and they can be allocated to different dispatcher pods. The kafkasource status displays such allocation under the status.placements key.</p> <p>You can scale a KafkaSource with kubectl by using the following notation:</p> <pre><code>kubectl scale kafkasource -n &lt;ns&gt; &lt;kafkasource-name&gt; --replicas=&lt;number-of-replicas&gt; # e.g. 12 replicas for a topic with 12 partitions\n</code></pre> <p>Alternatively, if you are using a GitOps approach, you can add the <code>consumers</code> key as shown in the example below and commit it to your repository:</p> <pre><code>    apiVersion: sources.knative.dev/v1beta1\n    kind: KafkaSource\n    metadata:\n      name: kafka-source\n    spec:\n      consumerGroup: knative-group\n      bootstrapServers:\n      - my-cluster-kafka-bootstrap.kafka:9092 \n      consumers: 12    # Number of replicas\n      topics:\n      - knative-demo-topic\n      sink:\n        ref:\n          apiVersion: serving.knative.dev/v1\n          kind: Service\n          name: event-display\n</code></pre>"},{"location":"eventing/sources/kafka-source/#automatic-scaling-with-keda","title":"Automatic Scaling with KEDA","text":"<p>Kafka Sources and Brokers for Apache Kafka have (Alpha) support for serverless scaling with KEDA, including scale to zero. If you want Knative and KEDA to scale your Kafka source for you, you must install KEDA, and then enable the feature flag.</p> <p>To enable the feature flag, you need to create or modify the <code>config-kafka-features</code> configmap in the <code>knative-eventing</code> namespace. You can create the file as below:</p> <pre><code>    apiVersion: v1\n    kind: Configmap\n    metadata:\n      name: config-kafka-features\n      namespace: knative-eventing\n    data:\n      controller-autoscaler-keda: \"enabled\"\n</code></pre> <p>From there, apply the configmap into your cluster and assuming that KEDA is also installed your Kafka Sources will scale for you! For more information on other values you can add to the <code>config-kafka-features</code> configmap, read about the Kafka Broker features.</p>"},{"location":"eventing/sources/kafka-source/#verify","title":"Verify","text":"<ol> <li> <p>Produce a message (<code>{\"msg\": \"This is a test!\"}</code>) to the Apache Kafka topic as in the following example:</p> <pre><code>kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic\n</code></pre> <p>Tip</p> <p>If you don't see a command prompt, try pressing Enter.</p> </li> <li> <p>Verify that the Service received the message from the event source:</p> <pre><code>kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container\n</code></pre> <p>Example output: <pre><code>\u2601\ufe0f cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.kafka.event\n  source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic\n  subject: partition:0#564\n  id: partition:0/offset:564\n  time: 2020-02-10T18:10:23.861866615Z\n  datacontenttype: application/json\nExtensions,\n  key:\nData,\n    {\n      \"msg\": \"This is a test!\"\n    }\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/kafka-source/#handling-delivery-failures","title":"Handling Delivery Failures","text":"<p>The <code>KafkaSource</code> implements the <code>Delivery</code> Specificiation, allowing you to configure event delivery parameters for it, which are applied in cases where an event fails to be delivered:</p> <pre><code>    apiVersion: sources.knative.dev/v1beta1\n    kind: KafkaSource\n    metadata:\n      name: kafka-source\n    spec:\n      consumerGroup: knative-group\n      bootstrapServers:\n      - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace\n      topics:\n      - knative-demo-topic\n      delivery:\n        deadLetterSink:\n          ref:\n            apiVersion: serving.knative.dev/v1\n            kind: Service\n            name: example-sink\n        backoffDelay: &lt;duration&gt;\n        backoffPolicy: &lt;policy-type&gt;\n        retry: &lt;integer&gt;\n      sink:\n        ref:\n          apiVersion: serving.knative.dev/v1\n          kind: Service\n          name: event-display\n</code></pre> <p>The <code>delivery</code> API is discussed in the Handling Delivery Failure chapter.</p>"},{"location":"eventing/sources/kafka-source/#optional-specify-the-key-deserializer","title":"Optional: Specify the key deserializer","text":"<p>When <code>KafkaSource</code> receives a message from Kafka, it dumps the key in the Event extension called <code>Key</code> and dumps Kafka message headers in the extensions starting with <code>kafkaheader</code>.</p> <p>You can specify the key deserializer among four types:</p> <ul> <li><code>string</code> (default) for UTF-8 encoded strings</li> <li><code>int</code> for 32-bit &amp; 64-bit signed integers</li> <li><code>float</code> for 32-bit &amp; 64-bit floating points</li> <li><code>byte-array</code> for a Base64 encoded byte array</li> </ul> <p>To specify the key deserializer, add the label <code>kafkasources.sources.knative.dev/key-type</code> to the <code>KafkaSource</code> definition, as shown in the following example:</p> <pre><code>apiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\nname: kafka-source\nlabels:\n  kafkasources.sources.knative.dev/key-type: int\nspec:\nconsumerGroup: knative-group\nbootstrapServers:\n- my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace\ntopics:\n- knative-demo-topic\nsink:\n  ref:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: event-display\n</code></pre>"},{"location":"eventing/sources/kafka-source/#optional-specify-the-initial-offset","title":"Optional: Specify the initial offset","text":"<p>By default the <code>KafkaSource</code> starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to <code>earliest</code>, for example:</p> <pre><code>apiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\n  name: kafka-source\nspec:\nconsumerGroup: knative-group\ninitialOffset: earliest\nbootstrapServers:\n- my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace\ntopics:\n- knative-demo-topic\nsink:\n  ref:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: event-display\n</code></pre> <p>Note</p> <p>The valid values for <code>initialOffset</code> are <code>earliest</code> and <code>latest</code>. Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group.</p>"},{"location":"eventing/sources/kafka-source/#connecting-to-a-tls-enabled-kafka-broker","title":"Connecting to a TLS-enabled Kafka Broker","text":"<p>The KafkaSource supports TLS and SASL authentication methods. To enable TLS authentication, you must have the following files:</p> <ul> <li>CA Certificate</li> <li>Client Certificate and Key</li> </ul> <p>KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM.</p> <ol> <li> <p>Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands:</p> <pre><code>kubectl create secret generic cacert --from-file=caroot.pem\n</code></pre> <pre><code>kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem\n</code></pre> </li> <li> <p>Apply the KafkaSource. Modify the <code>bootstrapServers</code> and <code>topics</code> fields accordingly.</p> <pre><code>apiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\n name: kafka-source-with-tls\nspec:\n net:\n   tls:\n     enable: true\n     cert:\n       secretKeyRef:\n         key: tls.crt\n         name: kafka-secret\n     key:\n       secretKeyRef:\n         key: tls.key\n         name: kafka-secret\n     caCert:\n       secretKeyRef:\n         key: caroot.pem\n         name: cacert\n consumerGroup: knative-group\n bootstrapServers:\n - my-secure-kafka-bootstrap.kafka:443\n topics:\n - knative-demo-topic\n sink:\n   ref:\n     apiVersion: serving.knative.dev/v1\n     kind: Service\n     name: event-display\n</code></pre> </li> </ol>"},{"location":"eventing/sources/kafka-source/#enabling-sasl-for-kafkasources","title":"Enabling SASL for KafkaSources","text":"<p>Simple Authentication and Security Layer (SASL) is used by Apache Kafka for authentication. If you use SASL authentication on your cluster, users must provide credentials to Knative for communicating with the Kafka cluster, otherwise events cannot be produced or consumed.</p>"},{"location":"eventing/sources/kafka-source/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have access to a Kafka cluster that has Simple Authentication and Security Layer (SASL).</li> </ul>"},{"location":"eventing/sources/kafka-source/#procedure","title":"Procedure","text":"<ol> <li> <p>Create a secret that uses the Kafka cluster's SASL information, by running the following commands:</p> <pre><code>STRIMZI_CRT=$(kubectl -n kafka get secret example-cluster-cluster-ca-cert --template='{{index.data \"ca.crt\"}}' | base64 --decode )\n</code></pre> <pre><code>SASL_PASSWD=$(kubectl -n kafka get secret example-user --template='{{index.data \"password\"}}' | base64 --decode )\n</code></pre> <pre><code>kubectl create secret -n default generic &lt;secret_name&gt; \\\n    --from-literal=ca.crt=\"$STRIMZI_CRT\" \\\n    --from-literal=password=\"$SASL_PASSWD\" \\\n    --from-literal=saslType=\"SCRAM-SHA-512\" \\\n    --from-literal=user=\"example-user\"\n</code></pre> </li> <li> <p>Create or modify a KafkaSource so that it contains the following spec options:</p> <pre><code>apiVersion: sources.knative.dev/v1beta1\nkind: KafkaSource\nmetadata:\n  name: example-source\nspec:\n...\n  net:\n    sasl:\n      enable: true\n      user:\n        secretKeyRef:\n          name: &lt;secret_name&gt;\n          key: user\n      password:\n        secretKeyRef:\n          name: &lt;secret_name&gt;\n          key: password\n      type:\n        secretKeyRef:\n          name: &lt;secret_name&gt;\n          key: saslType\n    tls:\n      enable: true\n      caCert:\n        secretKeyRef:\n          name: &lt;secret_name&gt;\n          key: ca.crt\n...\n</code></pre> <p>Where <code>&lt;secret_name&gt;</code> is the name of the secret generated in the previous step.</p> </li> </ol>"},{"location":"eventing/sources/kafka-source/#clean-up-steps","title":"Clean up steps","text":"<ol> <li> <p>Delete the Kafka event source:</p> <pre><code>kubectl delete -f source/source.yaml kafkasource.sources.knative.dev\n</code></pre> <p>Example output: <pre><code>\"kafka-source\" deleted\n</code></pre></p> </li> <li> <p>Delete the <code>event-display</code> Service:</p> <pre><code>kubectl delete -f source/event-display.yaml service.serving.knative.dev\n</code></pre> <p>Example output: <pre><code>\"event-display\" deleted\n</code></pre></p> </li> <li> <p>Optional: Remove the Apache Kafka Topic</p> <pre><code>kubectl delete -f kafka-topic.yaml\n</code></pre> <p>Example output: <pre><code>kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/ping-source/","title":"Creating a PingSource object","text":"<p>This topic describes how to create a PingSource object.</p> <p>A PingSource is an event source that produces events with a fixed payload on a specified cron schedule.</p> <p>The following example shows how you can configure a PingSource as an event source that sends events every minute to a Knative service named <code>event-display</code> that is used as a sink. If you have an existing sink, you can replace the examples with your own values.</p>"},{"location":"eventing/sources/ping-source/#before-you-begin","title":"Before you begin","text":"<p>To create a PingSource:</p> <ul> <li>You must install Knative Eventing. The PingSource event source type is enabled by default when you install Knative Eventing.</li> <li>You can use either <code>kubectl</code> or <code>kn</code> commands to create components such as a sink and PingSource.</li> <li>You can use either <code>kubectl</code> or <code>kail</code> for logging during the verification step in this procedure.</li> </ul>"},{"location":"eventing/sources/ping-source/#create-a-pingsource-object","title":"Create a PingSource object","text":"<ol> <li> <p>Optional: Create a namespace for your PingSource by running the command:</p> <pre><code>kubectl create namespace &lt;namespace&gt;\n</code></pre> <p>Where <code>&lt;namespace&gt;</code> is the namespace that you want your PingSource to use. For example, <code>pingsource-example</code>.</p> <p>Note</p> <p>Creating a namespace for your PingSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your <code>default</code> namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources.</p> </li> <li> <p>Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log:</p> <ol> <li> <p>Copy the YAML below into a file:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: event-display\n  namespace: &lt;namespace&gt;\nspec:\n  replicas: 1\n  selector:\n    matchLabels: &amp;labels\n      app: event-display\n  template:\n    metadata:\n      labels: *labels\n    spec:\n      containers:\n        - name: event-display\n          image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: event-display\n  namespace: &lt;namespace&gt;\nspec:\n  selector:\n    app: event-display\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>Where <code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 above.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>Create the PingSource object.</p> <p>Note</p> <p>The data you want to send must be represented as text in the PingSource YAML file. Events that send binary data cannot be directly serialized in YAML. However, you can send binary data that is base64 encoded by using <code>dataBase64</code> in place of <code>data</code> in the PingSource spec.</p> <p>Use one of the following options:</p> knkn: binary dataYAMLYAML: binary data <ul> <li> <p>To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML, run the command:</p> <p><pre><code>kn source ping create &lt;pingsource-name&gt; \\\n  --namespace &lt;namespace&gt; \\\n  --schedule \"&lt;cron-schedule&gt;\" \\\n  --data '&lt;data&gt;' \\\n  --sink &lt;sink-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;pingsource-name&gt;</code> is the name of the PingSource that you want to create, for example, <code>test-ping-source</code>.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 above.</li> <li><code>&lt;cron-schedule&gt;</code> is a cron expression for the schedule for the PingSource to send events, for example, <code>*/1 * * * *</code> sends an event every minute. Both standard and Quartz Scheduler cron formats are supported, with the latter supporting a seconds field.</li> <li><code>&lt;data&gt;</code> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as <code>{\"message\": \"Hello world!\"}</code>.</li> <li><code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>http://event-display.pingsource-example.svc.cluster.local</code>.</li> </ul> <p>For a list of available options, see the Knative client documentation.</p> </li> </ul> <ul> <li> <p>To create a PingSource that sends binary data, run the command:</p> <p><pre><code>kn source ping create &lt;pingsource-name&gt; \\\n  --namespace &lt;namespace&gt; \\\n  --schedule \"&lt;cron-schedule&gt;\" \\\n  --data '&lt;base64-data&gt;' \\\n  --encoding 'base64' \\\n  --sink &lt;sink-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;pingsource-name&gt;</code> is the name of the PingSource that you want to create, for example, <code>test-ping-source</code>.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 above.</li> <li><code>&lt;cron-schedule&gt;</code> is a cron expression for the schedule for the PingSource to send events, for example, <code>*/1 * * * *</code> sends an event every minute. Both standard and Quartz Scheduler cron formats are supported, with the latter supporting a seconds field.</li> <li><code>&lt;base64-data&gt;</code> is the base64 encoded binary data that you want to send, for example, <code>ZGF0YQ==</code>.</li> <li><code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>http://event-display.pingsource-example.svc.cluster.local</code>.</li> </ul> <p>For a list of available options, see the Knative client documentation.</p> </li> </ul> <ul> <li> <p>To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML:</p> <ol> <li> <p>Create a YAML file using the template below:</p> <p><pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: &lt;pingsource-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  schedule: \"&lt;cron-schedule&gt;\"\n  contentType: \"&lt;content-type&gt;\"\n  data: '&lt;data&gt;'\n  sink:\n    ref:\n      apiVersion: v1\n      kind: &lt;sink-kind&gt;\n      name: &lt;sink-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;pingsource-name&gt;</code> is the name of the PingSource that you want to create, for example, <code>test-ping-source</code>.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 above.</li> <li><code>&lt;cron-schedule&gt;</code> is a cron expression for the schedule for the PingSource to send events, for example, <code>*/1 * * * *</code> sends an event every minute. Both standard and Quartz Scheduler cron formats are supported, with the latter supporting a seconds field.</li> <li><code>&lt;content-type&gt;</code> is the media type of the data you want to send, for example, <code>application/json</code>.</li> <li><code>&lt;data&gt;</code> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as <code>{\"message\": \"Hello world!\"}</code>.</li> <li><code>&lt;sink-kind&gt;</code> is any supported Addressable object that you want to use as a sink, for example, a <code>Service</code> or <code>Deployment</code>.</li> <li><code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>event-display</code>.</li> </ul> <p>For more information about the fields you can configure for the PingSource object, see PingSource reference.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> </ul> <ul> <li> <p>To create a PingSource that sends binary data:</p> <ol> <li> <p>Create a YAML file using the template below:</p> <p><pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: &lt;pingsource-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  schedule: \"&lt;cron-schedule&gt;\"\n  contentType: \"&lt;content-type&gt;\"\n  dataBase64: \"&lt;base64-data&gt;\"\n  sink:\n    ref:\n      apiVersion: v1\n      kind: &lt;sink-kind&gt;\n      name: &lt;sink-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;pingsource-name&gt;</code> is the name of the PingSource that you want to create, for example, <code>test-ping-source-binary</code>.</li> <li><code>&lt;namespace&gt;</code> is the name of the namespace that you created in step 1 above.</li> <li><code>&lt;cron-schedule&gt;</code> is a cron expression for the schedule for the PingSource to send events, for example, <code>*/1 * * * *</code> sends an event every minute. Both standard and Quartz Scheduler cron formats are supported, with the latter supporting a seconds field.</li> <li><code>&lt;content-type&gt;</code> is the media type of the data you want to send, for example, <code>application/json</code>.</li> <li><code>&lt;base64-data&gt;</code> is the base64 encoded binary data that you want to send, for example, <code>ZGF0YQ==</code>.</li> <li><code>&lt;sink-kind&gt;</code> is any supported Addressable object that you want to use as a sink, for example, a Kubernetes Service.</li> <li><code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>event-display</code>.</li> </ul> <p>For more information about the fields you can configure for the PingSource object, see PingSource reference.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> </ul> </li> </ol>"},{"location":"eventing/sources/ping-source/#verify-the-pingsource-object","title":"Verify the PingSource object","text":"<ol> <li> <p>View the logs for the <code>event-display</code> event consumer by running the command:</p> kubectlkail <pre><code>kubectl -n pingsource-example logs -l app=event-display --tail=100\n</code></pre> <pre><code>kail -l serving.knative.dev/service=event-display -c user-container --since=10m\n</code></pre> </li> <li> <p>Verify that the output returns the properties of the events that your PingSource sent to your sink. In the example below, the command has returned the <code>Attributes</code> and <code>Data</code> properties of the events that the PingSource sent to the <code>event-display</code> Service:</p> <pre><code>\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.sources.ping\n  source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source\n  id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8\n  time: 2021-03-25T19:41:00.444508332Z\n  datacontenttype: application/json\nData,\n  {\n    \"message\": \"Hello world!\"\n  }\n</code></pre> </li> </ol>"},{"location":"eventing/sources/ping-source/#delete-the-pingsource-object","title":"Delete the PingSource object","text":"<p>You can either delete the PingSource and all related resources, or delete the resources individually:</p> <ul> <li> <p>To remove the PingSource object and all of the related resources, delete the namespace by running the command:</p> <p><pre><code>kubectl delete namespace &lt;namespace&gt;\n</code></pre> Where <code>&lt;namespace&gt;</code> is the namespace that contains the PingSource object.</p> </li> <li> <p>To delete the PingSource instance only, run the command:</p> knkubectl <p><pre><code>kn source ping delete &lt;pingsource-name&gt;\n</code></pre> Where <code>&lt;pingsource-name&gt;</code> is the name of the PingSource you want to delete, for example, <code>test-ping-source</code>.</p> <p><pre><code>kubectl delete pingsources.sources.knative.dev &lt;pingsource-name&gt;\n</code></pre> Where <code>&lt;pingsource-name&gt;</code> is the name of the PingSource you want to delete, for example, <code>test-ping-source</code>.</p> </li> <li> <p>To delete the sink only, run the command:</p> knkubectl <p><pre><code>kn service delete &lt;sink-name&gt;\n</code></pre> Where <code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>event-display</code>.</p> <p><pre><code>kubectl delete service.serving.knative.dev &lt;sink-name&gt;\n</code></pre> Where <code>&lt;sink-name&gt;</code> is the name of your sink, for example, <code>event-display</code>.</p> </li> </ul>"},{"location":"eventing/sources/ping-source/reference/","title":"PingSource reference","text":"<p>This topic provides reference information about the configurable fields for the PingSource object.</p>"},{"location":"eventing/sources/ping-source/reference/#pingsource","title":"PingSource","text":"<p>A PingSource definition supports the following fields:</p> Field Description Required or optional <code>apiVersion</code> Specifies the API version, for example <code>sources.knative.dev/v1</code>. Required <code>kind</code> Identifies this resource object as a PingSource object. Required <code>metadata</code> Specifies metadata that uniquely identifies the PingSource object. For example, a <code>name</code>. Required <code>spec</code> Specifies the configuration information for this PingSource object. Required <code>spec.contentType</code> The media type of <code>data</code> or <code>dataBase64</code>. Default is empty. Optional <code>spec.data</code> The data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with <code>dataBase64</code>. Required if not sending base64 encoded data <code>spec.dataBase64</code> A base64-encoded string of the actual event's body posted to the sink. Default is empty. Mutually exclusive with <code>data</code>. Required if sending base64 encoded data <code>spec.schedule</code> Specifies the cron schedule. Defaults to <code>* * * * *</code>. Optional <code>spec.sink</code> A reference to an object that resolves to a URI to use as the sink. Required <code>spec.timezone</code> Modifies the actual time relative to the specified timezone. Defaults to the system time zone.  See the list of valid tz database time zones on Wikipedia. For general information about time zones, see the IANA website. Optional <code>spec.ceOverrides</code> Defines overrides to control the output format and modifications to the event sent to the sink. Optional <code>status</code> Defines the observed state of PingSource. Optional <code>status.observedGeneration</code> The 'Generation' of the Service that was last processed by the controller. Optional <code>status.conditions</code> The latest available observations of a resource's current state. Optional <code>status.sinkUri</code> The current active sink URI that has been configured for the Source. Optional"},{"location":"eventing/sources/ping-source/reference/#cloudevent-overrides","title":"CloudEvent Overrides","text":"<p>CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink.</p> <p>A <code>ceOverrides</code> definition supports the following fields:</p> Field Description Required or optional <code>extensions</code> Specifies which attributes are added or overridden on the outbound event. Each <code>extensions</code> key-value pair is set independently on the event as an attribute extension. Optional <p>Note</p> <p>Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the <code>type</code> attribute.</p>"},{"location":"eventing/sources/ping-source/reference/#example-cloudevent-overrides","title":"Example: CloudEvent Overrides","text":"<pre><code>apiVersion: sources.knative.dev/v1\nkind: PingSource\nmetadata:\n  name: test-heartbeats\nspec:\n  ...\n  ceOverrides:\n    extensions:\n      extra: this is an extra attribute\n      additional: 42\n</code></pre> <p>Contract</p> <p>This results in the <code>K_CE_OVERRIDES</code> environment variable being set on the <code>subject</code> as follows:  <pre><code>{ \"extensions\": { \"extra\": \"this is an extra attribute\", \"additional\": \"42\" } }\n</code></pre></p>"},{"location":"eventing/sources/rabbitmq-source/","title":"Creating a RabbitMQSource","text":"<p>This topic describes how to create a RabbitMQSource.</p>"},{"location":"eventing/sources/rabbitmq-source/#prerequisites","title":"Prerequisites","text":"<ol> <li>You have installed Knative Eventing</li> <li>You have installed CertManager v1.5.4 - easiest integration with RabbitMQ Messaging Topology Operator</li> <li>You have installed RabbitMQ Messaging Topology Operator - our recommendation is latest release with CertManager</li> <li>A working RabbitMQ Instance, we recommend to create one Using the RabbitMQ Cluster Operator. For more information about configuring the <code>RabbitmqCluster</code> CRD, see the RabbitMQ website</li> </ol>"},{"location":"eventing/sources/rabbitmq-source/#install-the-rabbitmq-controller","title":"Install the RabbitMQ controller","text":"<ol> <li> <p>Install the RabbitMQSource controller by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-source.yaml\n</code></pre> </li> <li> <p>Verify that <code>rabbitmq-controller-manager</code> and <code>rabbitmq-webhook</code> are running:</p> <pre><code>kubectl get deployments.apps -n knative-sources\n</code></pre> <p>Example output:</p> <pre><code>NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\nrabbitmq-controller-manager     1/1     1            1           3s\nrabbitmq-webhook                1/1     1            1           4s\n</code></pre> </li> </ol>"},{"location":"eventing/sources/rabbitmq-source/#create-a-service","title":"Create a Service","text":"<ol> <li> <p>Create the <code>event-display</code> Service as a YAML file:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: event-display\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - # This corresponds to\n          # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go\n          image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> <p>Example output: <pre><code>service.serving.knative.dev/event-display created\n</code></pre></p> </li> <li> <p>Ensure that the Service Pod is running, by running the command:</p> <pre><code>kubectl get pods\n</code></pre> <p>The Pod name is prefixed with <code>event-display</code>: <pre><code>NAME                                            READY     STATUS    RESTARTS   AGE\nevent-display-00001-deployment-5d5df6c7-gv2j4   2/2       Running   0          72s\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/rabbitmq-source/#create-a-rabbitmqsource-object","title":"Create a RabbitMQSource object","text":"<ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: sources.knative.dev/v1alpha1\nkind: RabbitmqSource\nmetadata:\n  name: &lt;source-name&gt;\nspec:\n  rabbitmqClusterReference:\n    # Configure name if a RabbitMQ Cluster Operator is being used.\n    name: &lt;cluster-name&gt;\n    # Configure connectionSecret if an external RabbitMQ cluster is being used.\n    connectionSecret:\n      name: rabbitmq-secret-credentials\n  rabbitmqResourcesConfig:\n    parallelism: 10\n    exchangeName: \"eventing-rabbitmq-source\"\n    queueName: \"eventing-rabbitmq-source\"\n  delivery:\n    retry: 5\n    backoffPolicy: \"linear\"\n    backoffDelay: \"PT1S\"\n  sink:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: event-display\n</code></pre> Where:</p> <ul> <li><code>&lt;source-name&gt;</code> is the name you want for your RabbitMQSource object.</li> <li><code>&lt;cluster-name&gt;</code> is the name of the RabbitMQ cluster you created earlier.</li> </ul> <p>Note</p> <p>You cannot set <code>name</code> and <code>connectionSecret</code> at the same time, since <code>name</code> is for a RabbitMQ Cluster Operator instance running in the same cluster as the Source, and <code>connectionSecret</code> is for an external RabbitMQ server.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/sources/rabbitmq-source/#verify","title":"Verify","text":"<p>Check the event-display Service to see if it is receiving events. It might take a while for the Source to start sending events to the Sink.</p> <pre><code>  kubectl -l='serving.knative.dev/service=event-display' logs -c user-container\n  \u2601\ufe0f  cloudevents.Event\n  Context Attributes,\n    specversion: 1.0\n    type: dev.knative.rabbitmq.event\n    source: /apis/v1/namespaces/default/rabbitmqsources/&lt;source-name&gt;\n    subject: f147099d-c64d-41f7-b8eb-a2e53b228349\n    id: f147099d-c64d-41f7-b8eb-a2e53b228349\n    time: 2021-12-16T20:11:39.052276498Z\n    datacontenttype: application/json\n  Data,\n    {\n      ...\n      Random Data\n      ...\n    }\n</code></pre>"},{"location":"eventing/sources/rabbitmq-source/#cleanup","title":"Cleanup","text":"<ol> <li> <p>Delete the RabbitMQSource:</p> <pre><code>kubectl delete -f &lt;source-yaml-filename&gt;\n</code></pre> </li> <li> <p>Delete the RabbitMQ credentials secret:</p> <pre><code>kubectl delete -f &lt;secret-yaml-filename&gt;\n</code></pre> </li> <li> <p>Delete the event display Service:</p> <pre><code>kubectl delete -f &lt;service-yaml-filename&gt;\n</code></pre> </li> </ol>"},{"location":"eventing/sources/rabbitmq-source/#additional-information","title":"Additional information","text":"<ul> <li>For more samples visit the <code>eventing-rabbitmq</code> Github repository samples directory</li> <li>To report a bug or request a feature, open an issue in the <code>eventing-rabbitmq</code> Github repository.</li> </ul>"},{"location":"eventing/sources/redis/","title":"About RedisStreamSource","text":"<p>The RedisStreamSource reads messages from Redis Streams and sends them as CloudEvents to the referenced Sink.</p>"},{"location":"eventing/sources/redis/getting-started/","title":"Creating a RedisStreamSource","text":"<p>This topic describes how to create a <code>RedisStreamSource</code> object.</p>"},{"location":"eventing/sources/redis/getting-started/#install-the-redisstreamsource-add-on","title":"Install the RedisStreamSource add-on","text":"<p><code>RedisStreamSource</code> is a Knative Eventing add-on.</p> <ol> <li> <p>Install RedisStreamSource by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-redis/latest/redis-source.yaml\n</code></pre> </li> <li> <p>Verify that <code>redis-controller-manager</code>is running:</p> <pre><code>kubectl get deployments.apps -n knative-sources\n</code></pre> <p>Example output:</p> <pre><code>NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\nredis-controller-manager        1/1     1            1           3s\n</code></pre> </li> </ol>"},{"location":"eventing/sources/redis/getting-started/#create-a-service","title":"Create a Service","text":"<ol> <li> <p>Create the <code>event-display</code> Service as a YAML file:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: event-display\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - # This corresponds to\n          # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go\n          image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> <p>Example output: <pre><code>service.serving.knative.dev/event-display created\n</code></pre></p> </li> <li> <p>Ensure that the Service Pod is running, by running the command:</p> <pre><code>kubectl get pods\n</code></pre> <p>The Pod name is prefixed with <code>event-display</code>: <pre><code>NAME                                            READY     STATUS    RESTARTS   AGE\nevent-display-00001-deployment-5d5df6c7-gv2j4   2/2       Running   0          72s\n</code></pre></p> </li> </ol>"},{"location":"eventing/sources/redis/getting-started/#create-a-redisstreamsource-object","title":"Create a RedisStreamSource object","text":"<ol> <li>Create the <code>RedisStreamSource</code> object using the YAML template below:</li> </ol> <pre><code>apiVersion: sources.knative.dev/v1alpha1\nkind: RedisStreamSource\nmetadata:\n  name: &lt;redis-stream-source&gt;\nspec:\n  address: &lt;redis-uri&gt;\n  stream: &lt;redis-stream-name&gt;\n  group: &lt;consumer-group-name&gt;\n  sink: &lt;sink&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;redis-stream-source&gt;</code> is the name of your source. (required)</li> <li><code>&lt;redis-uri&gt;</code> is the Redis URI. See the Redis documentation for more information. (required)</li> <li><code>&lt;redis-stream-name&gt;</code> is the name of the Redis stream. (required)</li> <li><code>&lt;consumer-group-name&gt;</code> is the name of the Redis consumer group. When left empty a group    is automatically created for this source, and deleted when this source is deleted. (optional)</li> <li> <p><code>&lt;sink&gt;</code> is where to send events. (required)</p> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ul>"},{"location":"eventing/sources/redis/getting-started/#verify-the-redisstreamsource-object","title":"Verify the RedisStreamSource object","text":"<ol> <li> <p>View the logs for the <code>event-display</code> event consumer by running the command:</p> <pre><code>kubectl logs -l app=event-display --tail=100\n</code></pre> <p>Sample output:</p> <pre><code>\u2601\ufe0f  cloudevents.Event\nValidation: valid\nContext Attributes,\n  specversion: 1.0\n  type: dev.knative.sources.redisstream\n  source: /mystream\n  id: 1597775814718-0\n  time: 2020-08-18T18:36:54.719802342Z\n  datacontenttype: application/json\nData,\n  [\n    \"fruit\",\n    \"banana\"\n    \"color\",\n    \"yellow\"\n  ]\n</code></pre> </li> </ol>"},{"location":"eventing/sources/redis/getting-started/#delete-the-redisstreamsource-object","title":"Delete the RedisStreamSource object","text":"<ul> <li> <p>Delete the <code>RedisStreamSource</code> object:</p> <pre><code>kubectl delete -f &lt;filename&gt;\n</code></pre> </li> </ul>"},{"location":"eventing/sources/redis/getting-started/#additional-information","title":"Additional information","text":"<ul> <li>For more information about Redis Stream source, see the <code>eventing-redis</code> Github repository</li> </ul>"},{"location":"eventing/sugar/","title":"Knative Eventing Sugar Controller","text":"<p>Knative Eventing Sugar Controller will react to configured labels to produce or control eventing resources in a cluster or namespace. This allows cluster operators and developers to focus on creating fewer resources, and the underlying eventing infrastructure is created on-demand, and cleaned up when no longer needed.</p>"},{"location":"eventing/sugar/#installing","title":"Installing","text":"<p>The Sugar Controller is <code>disabled</code> by default and can be enabled by configuring <code>config-sugar</code> ConfigMap. See below for a simple example and Configure Sugar Controller for more details.</p>"},{"location":"eventing/sugar/#automatic-broker-creation","title":"Automatic Broker Creation","text":"<p>One way to create a Broker is to manually apply a resource to a cluster using the default settings:</p> <ol> <li> <p>Copy the following YAML into a file:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n  name: default\n  namespace: default\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>There might be cases where automated Broker creation is desirable, such as on namespace creation, or on Trigger creation. The Sugar controller enables those use-cases. The following sample configuration of the <code>sugar-config</code> ConfigMap enables Sugar Controller for select Namespaces &amp; all Triggers.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: config-sugar\nnamespace: knative-eventing\nlabels:\n    eventing.knative.dev/release: devel\ndata:\n  # Specify a label selector to selectively apply sugaring to certain namespaces\n  namespace-selector: |\n    matchExpressions:\n    - key: \"my.custom.injection.key\"\n      operator: \"In\"\n      values: [\"enabled\"]\n  # Use an empty object to enable for all triggers\n  trigger-selector: |\n    {}\n</code></pre> <ul> <li>When a Namespace is created with label <code>my.custom.injection.key: enabled</code> , the Sugar controller will create a Broker named \"default\" in that   namespace.</li> <li>When a Trigger is created, the Sugar controller will create a Broker named \"default\" in the   Trigger's namespace.</li> </ul> <p>When a Broker is deleted and but the referenced label selectors are in use, the Sugar Controller will automatically recreate a default Broker.</p>"},{"location":"eventing/sugar/#namespace-examples","title":"Namespace Examples","text":"<p>Creating a \"default\" Broker when creating a Namespace:</p> <ol> <li> <p>Copy the following YAML into a file:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: example\n  labels:\n    my.custom.injection.key: enabled\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To automatically create a Broker after a namespace exists, label the Namespace:</p> <pre><code>kubectl label namespace default my.custom.injection.key=enabled\n</code></pre> <p>If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing.</p>"},{"location":"eventing/sugar/#trigger-example","title":"Trigger Example","text":"<p>Create a \"default\" Broker in the Trigger's Namespace when creating a Trigger:</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: hello-sugar\n  namespace: hello\nspec:\n  broker: default\n  subscriber:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: event-display\nEOF\n</code></pre> <p>This will make a Broker called \"default\" in the Namespace \"hello\", and attempt to send events to the \"event-display\" service.</p> <p>If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing and the Trigger will not own the existing Broker.</p>"},{"location":"eventing/triggers/","title":"Using Triggers","text":"<p>A trigger represents a desire to subscribe to events from a specific broker.</p> <p>The <code>subscriber</code> value must be a Destination.</p>"},{"location":"eventing/triggers/#example-triggers","title":"Example Triggers","text":"<p>The following trigger receives all the events from the <code>default</code> broker and delivers them to the Knative Serving service <code>my-service</code>:</p> <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\nspec:\n  broker: default\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: my-service\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>The following trigger receives all the events from the <code>default</code> broker and delivers them to the custom path <code>/my-custom-path</code> for the Kubernetes service <code>my-service</code>:</p> <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\nspec:\n  broker: default\n  subscriber:\n    ref:\n      apiVersion: v1\n      kind: Service\n      name: my-service\n    uri: /my-custom-path\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/triggers/#trigger-filtering","title":"Trigger filtering","text":"<p>Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the trigger to filter it. Note that we only support exact matching on string values.</p>"},{"location":"eventing/triggers/#example","title":"Example","text":"<p>This example filters events from the <code>default</code> broker that are of type <code>dev.knative.foo.bar</code> and have the extension <code>myextension</code> with the value <code>my-extension-value</code>.</p> <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: dev.knative.foo.bar\n      myextension: my-extension-value\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: my-service\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"eventing/triggers/#new-trigger-filters","title":"New trigger filters","text":"<p>If you need more powerful filtering options, you can use the new trigger filters.</p>"},{"location":"eventing/triggers/#trigger-annotations","title":"Trigger annotations","text":"<p>You can modify a Trigger's behavior by setting the following two annotations:</p> <ul> <li><code>eventing.knative.dev/injection</code>: if set to <code>enabled</code>, Eventing automatically creates a Broker for a Trigger if it doesn't exist. The Broker is created in the namespace where the Trigger is created. This annotation only works if you have the Sugar Controller enabled, which is optional and not enabled by default.</li> <li><code>knative.dev/dependency</code>: this annotation is used to mark the sources that the Trigger depends on. If one of the dependencies is not ready, the Trigger will not be ready.</li> </ul> <p>The following YAML is an example of a Trigger with a dependency: <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: my-service-trigger\n  annotations:\n    knative.dev/dependency: '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}'\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: dev.knative.foo.bar\n      myextension: my-extension-value\n    subscriber:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: my-service\n</code></pre></p>"},{"location":"eventing/troubleshooting/","title":"Debugging Knative Eventing","text":"<p>This is an evolving document on how to debug a non-working Knative Eventing setup.</p>"},{"location":"eventing/troubleshooting/#audience","title":"Audience","text":"<p>This document is intended for people that are familiar with the object model of Knative Eventing. You don't need to be an expert, but do need to know roughly how things fit together.</p>"},{"location":"eventing/troubleshooting/#prerequisites","title":"Prerequisites","text":"<ol> <li>Setup Knative Eventing and an Eventing-contrib resource.</li> </ol>"},{"location":"eventing/troubleshooting/#example","title":"Example","text":"<p>This guide uses an example consisting of an event source that sends events to a function.</p> <p></p> <p>See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml:</p> <pre><code>kubectl apply --filename example.yaml\n</code></pre>"},{"location":"eventing/troubleshooting/#triggering-events","title":"Triggering Events","text":"<p>Knative events will occur whenever a Kubernetes <code>Event</code> occurs in the <code>knative-debug</code> namespace. We can cause this to occur with the following commands:</p> <pre><code>kubectl --namespace knative-debug run to-be-deleted --image=image-that-doesnt-exist --restart=Never\n# 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event.\nsleep 5\nkubectl --namespace knative-debug delete pod to-be-deleted\n</code></pre> <p>Then we can see the Kubernetes <code>Event</code>s (note that these are not Knative events!):</p> <pre><code>kubectl --namespace knative-debug get events\n</code></pre> <p>This should produce output along the lines of:</p> <pre><code>LAST SEEN   FIRST SEEN   COUNT     NAME                             KIND      SUBOBJECT                        TYPE      REASON                   SOURCE                                         MESSAGE\n20s         20s          1         to-be-deleted.157aadb9f376fc4e   Pod                                        Normal    Scheduled                default-scheduler                              Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2\n</code></pre>"},{"location":"eventing/troubleshooting/#where-are-my-events","title":"Where are my events?","text":"<p>You've applied example.yaml and you are inspecting <code>fn</code>'s logs:</p> <pre><code>kubectl --namespace knative-debug logs -l app=fn -c user-container\n</code></pre> <p>But you don't see any events arrive. Where is the problem?</p>"},{"location":"eventing/troubleshooting/#check-created-resources","title":"Check created resources","text":"<p>The first thing to check are all the created resources, do their statuses contain <code>ready</code> true?</p> <p>We will attempt to determine why from the most basic pieces out:</p> <ol> <li><code>fn</code> - The <code>Deployment</code> has no dependencies inside Knative.</li> <li><code>svc</code> - The <code>Service</code> has no dependencies inside Knative.</li> <li><code>chan</code> - The <code>Channel</code> depends on its backing <code>channel implementation</code> and    somewhat depends on <code>sub</code>.</li> <li><code>src</code> - The <code>Source</code> depends on <code>chan</code>.</li> <li><code>sub</code> - The <code>Subscription</code> depends on both <code>chan</code> and <code>svc</code>.</li> </ol>"},{"location":"eventing/troubleshooting/#fn","title":"<code>fn</code>","text":"<pre><code>kubectl --namespace knative-debug get deployment fn -o jsonpath='{.status.availableReplicas}'\n</code></pre> <p>We want to see <code>1</code>. If you don't, then you need to debug the <code>Deployment</code>. Is there anything obviously wrong mentioned in the <code>status</code>?</p> <pre><code>kubectl --namespace knative-debug get deployment fn --output yaml\n</code></pre> <p>If it is not obvious what is wrong, then you need to debug the <code>Deployment</code>, which is out of scope of this document.</p> <p>Verify that the <code>Pod</code> is <code>Ready</code>:</p> <pre><code>kubectl --namespace knative-debug get pod -l app=fn -o jsonpath='{.items[*].status.conditions[?(@.type == \"Ready\")].status}'\n</code></pre> <p>This should return <code>True</code>. If it doesn't, then try to debug the <code>Deployment</code> using the Kubernetes Application Debugging guide.</p>"},{"location":"eventing/troubleshooting/#svc","title":"<code>svc</code>","text":"<pre><code>kubectl --namespace knative-debug get service svc\n</code></pre> <p>We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml.</p> <p>Verify it points at the expected pod.</p> <pre><code>svcLabels=$(kubectl --namespace knative-debug get service svc -o go-template='{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' )\nkubectl --namespace knative-debug get pods -l $svcLabels\n</code></pre> <p>This should return a single Pod, which if you inspect is the one generated by <code>fn</code>.</p>"},{"location":"eventing/troubleshooting/#chan","title":"<code>chan</code>","text":"<p><code>chan</code> uses the <code>in-memory-channel</code>. This is a very basic channel and has few failure modes that will be exhibited in <code>chan</code>'s <code>status</code>.</p> <pre><code>kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath='{.status.conditions[?(@.type == \"Ready\")].status}'\n</code></pre> <p>This should return <code>True</code>. If it doesn't, get the full resource:</p> <pre><code>kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml\n</code></pre> <p>If <code>status</code> is completely missing, it implies that something is wrong with the <code>in-memory-channel</code> controller. See Channel Controller.</p> <p>Next verify that <code>chan</code> is addressable:</p> <pre><code>kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath='{.status.address.hostname}'\n</code></pre> <p>This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller.</p> <p>We will verify that the two resources that the <code>chan</code> creates exist and are <code>Ready</code>.</p>"},{"location":"eventing/troubleshooting/#service","title":"<code>Service</code>","text":"<p><code>chan</code> creates a K8s <code>Service</code>.</p> <pre><code>kubectl --namespace knative-debug get service -l messaging.knative.dev/role=in-memory-channel\n</code></pre> <p>It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that <code>src</code> can send events to it. If it doesn't exist, it implies that something went wrong during <code>chan</code> reconciliation. See Channel Controller.</p>"},{"location":"eventing/troubleshooting/#src","title":"<code>src</code>","text":"<p><code>src</code> is a <code>ApiServerSource</code>.</p> <p>First we will verify that <code>src</code> is writing to <code>chan</code>.</p> <pre><code>kubectl --namespace knative-debug get apiserversource src -o jsonpath='{.spec.sink}'\n</code></pre> <p>Which should return <code>map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan]</code>. If it doesn't, then <code>src</code> was setup incorrectly and its <code>spec</code> needs to be fixed. Fixing should be as simple as updating its <code>spec</code> to have the correct <code>sink</code> (see example.yaml).</p> <p>Now that we know <code>src</code> is sending to <code>chan</code>, let's verify that it is <code>Ready</code>.</p> <pre><code>kubectl --namespace knative-debug get apiserversource src -o jsonpath='{.status.conditions[?(.type == \"Ready\")].status}'\n</code></pre>"},{"location":"eventing/troubleshooting/#sub","title":"<code>sub</code>","text":"<p><code>sub</code> is a <code>Subscription</code> from <code>chan</code> to <code>fn</code>.</p> <p>Verify that <code>sub</code> is <code>Ready</code>:</p> <pre><code>kubectl --namespace knative-debug get subscription sub -o jsonpath='{.status.conditions[?(.type == \"Ready\")].status}'\n</code></pre> <p>This should return <code>True</code>. If it doesn't then, look at all the status entries.</p> <pre><code>kubectl --namespace knative-debug get subscription sub --output yaml\n</code></pre>"},{"location":"eventing/troubleshooting/#controllers","title":"Controllers","text":"<p>Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs.</p> <p>Note</p> <p>The Kubernetes Deployment Controller, which controls <code>fn</code>, is out of scope for this document.</p>"},{"location":"eventing/troubleshooting/#service-controller","title":"Service Controller","text":"<p>The Kubernetes Service Controller, controlling <code>svc</code>, is out of scope for this document.</p>"},{"location":"eventing/troubleshooting/#channel-controller","title":"Channel Controller","text":"<p>There is not a single <code>Channel</code> Controller. Instead, there is one Controller for each Channel CRD. <code>chan</code> uses the <code>InMemoryChannel</code> <code>Channel CRD</code>, whose Controller is:</p> <pre><code>kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel=in-memory-channel,messaging.knative.dev/role=controller --output yaml\n</code></pre> <p>See its logs with:</p> <pre><code>kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel=in-memory-channel,messaging.knative.dev/role=controller\n</code></pre> <p>Pay particular attention to any lines that have a logging level of <code>warning</code> or <code>error</code>.</p>"},{"location":"eventing/troubleshooting/#source-controller","title":"Source Controller","text":"<p>Each Source will have its own Controller. <code>src</code> is a <code>ApiServerSource</code>, so its Controller is:</p> <pre><code>kubectl --namespace knative-eventing get pod -l app=sources-controller\n</code></pre> <p>This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller.</p>"},{"location":"eventing/troubleshooting/#apiserversource-controller","title":"ApiServerSource Controller","text":"<p>The <code>ApiServerSource</code> Controller is run in the same binary as some other Source Controllers from Eventing. It is:</p> <pre><code>kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName=src,eventing.knative.dev/source=apiserver-source-controller\n</code></pre> <p>View its logs with:</p> <pre><code>kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName=src,eventing.knative.dev/source=apiserver-source-controller\n</code></pre> <p>Pay particular attention to any lines that have a logging level of <code>warning</code> or <code>error</code>.</p>"},{"location":"eventing/troubleshooting/#subscription-controller","title":"Subscription Controller","text":"<p>The <code>Subscription</code> Controller controls <code>sub</code>. It attempts to resolve the addresses that a <code>Channel</code> should send events to, and once resolved, inject those into the <code>Channel</code>'s <code>spec.subscribable</code>.</p> <pre><code>kubectl --namespace knative-eventing get pod -l app=eventing-controller\n</code></pre> <p>View its logs with:</p> <pre><code>kubectl --namespace knative-eventing logs -l app=eventing-controller\n</code></pre> <p>Pay particular attention to any lines that have a logging level of <code>warning</code> or <code>error</code>.</p>"},{"location":"eventing/troubleshooting/#data-plane","title":"Data Plane","text":"<p>The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane.</p> <p>The Knative event takes the following path:</p> <ol> <li> <p>Event is generated by <code>src</code>.</p> </li> <li> <p>In this case, it is caused by having a Kubernetes <code>Event</code> trigger it, but      as far as Knative is concerned, the <code>Source</code> is generating the event denovo      (from nothing).</p> </li> <li> <p><code>src</code> is POSTing the event to <code>chan</code>'s address,    <code>http://chan-kn-channel.knative-debug.svc.cluster.local</code>.</p> </li> <li> <p>The Channel Dispatcher receives the request and introspects the Host header    to determine which <code>Channel</code> it corresponds to. It sees that it corresponds    to <code>knative-debug/chan</code> so forwards the request to the subscribers defined in    <code>sub</code>, in particular <code>svc</code>, which is backed by <code>fn</code>.</p> </li> <li> <p><code>fn</code> receives the request and logs it.</p> </li> </ol> <p>We will investigate components in the order in which events should travel.</p>"},{"location":"eventing/troubleshooting/#channel-dispatcher","title":"Channel Dispatcher","text":"<p>The Channel Dispatcher is the component that receives POSTs pushing events into <code>Channel</code>s and then POSTs to subscribers of those <code>Channel</code>s when an event is received. For the <code>in-memory-channel</code> used in this example, there is a single binary that handles both the receiving and dispatching sides for all <code>in-memory-channel</code> <code>Channel</code>s.</p> <p>First we will inspect the Dispatcher's logs to see if it is anything obvious:</p> <pre><code>kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel=in-memory-channel,messaging.knative.dev/role=dispatcher -c dispatcher\n</code></pre> <p>Ideally we will see lines like:</p> <pre><code>{\"level\":\"info\",\"ts\":\"2019-08-16T13:50:55.424Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"provisioners/message_receiver.go:147\",\"msg\":\"Request mapped to channel: knative-debug/chan-kn-channel\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\"}\n{\"level\":\"info\",\"ts\":\"2019-08-16T13:50:55.425Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"provisioners/message_dispatcher.go:112\",\"msg\":\"Dispatching message to http://svc.knative-debug.svc.cluster.local/\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\"}\n{\"level\":\"info\",\"ts\":\"2019-08-16T13:50:55.981Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"provisioners/message_receiver.go:140\",\"msg\":\"Received request for chan-kn-channel.knative-debug.svc.cluster.local\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\"}\n</code></pre> <p>Which shows that the request is being received and then sent to <code>svc</code>, which is returning a 2XX response code (likely 200, 202, or 204).</p> <p>However if we see something like:</p> <pre><code>{\"level\":\"info\",\"ts\":\"2019-08-16T16:10:16.859Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"provisioners/message_receiver.go:140\",\"msg\":\"Received request for chan-kn-channel.knative-debug.svc.cluster.local\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\"}\n{\"level\":\"info\",\"ts\":\"2019-08-16T16:10:16.859Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"provisioners/message_receiver.go:147\",\"msg\":\"Request mapped to channel: knative-debug/chan-kn-channel\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\"}\n{\"level\":\"info\",\"ts\":\"2019-08-16T16:10:16.859Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"provisioners/message_dispatcher.go:112\",\"msg\":\"Dispatching message to http://svc.knative-debug.svc.cluster.local/\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\"}\n{\"level\":\"error\",\"ts\":\"2019-08-16T16:10:38.169Z\",\"logger\":\"inmemorychannel-dispatcher.in-memory-channel-dispatcher\",\"caller\":\"fanout/fanout_handler.go:121\",\"msg\":\"Fanout had an error\",\"knative.dev/controller\":\"in-memory-channel-dispatcher\",\"error\":\"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\",\"stacktrace\":\"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\"}\n</code></pre> <p>Then we know there was a problem posting to <code>http://svc.knative-debug.svc.cluster.local/</code>.</p>"},{"location":"functions/","title":"Knative Functions overview","text":"<p>Knative Functions provides a simple programming model for using functions on Knative, without requiring in-depth knowledge of Knative, Kubernetes, containers, or dockerfiles.</p> <p>Knative Functions enables you to easily create, build, and deploy stateless, event-driven functions as Knative Services by using the <code>func</code> CLI.</p> <p>When you build or run a function, an Open Container Initiative (OCI) format container image is generated automatically for you, and is stored in a container registry. Each time you update your code and then run or deploy it, the container image is also updated.</p> <p>You can create functions and manage function workflows by using the <code>func</code> CLI, or by using the <code>kn func</code> plugin for the Knative CLI.</p>"},{"location":"functions/#function-templates","title":"Function templates","text":"<p>Knative Functions provides templates that can be used to create basic functions, by initiating a function project boilerplate when you run a <code>create</code> command.</p> <p>Templates allow you to choose the language and invocation format for your function. The following templates are available with both CloudEvent and HTTP invocation formats:</p> <ul> <li>Node.js</li> <li>Python</li> <li>Go</li> <li>Quarkus</li> <li>Rust</li> <li>Spring Boot</li> <li>TypeScript</li> </ul>"},{"location":"functions/#language-packs","title":"Language packs","text":"<p>Functions can be written in any language supported by the available language packs.</p>"},{"location":"functions/#getting-started-with-functions","title":"Getting started with functions","text":"<p>Before you can use Knative Functions, you must have access to a Knative development environment. To set up a development environment, you can follow the Knative Quickstart tutorial.</p>"},{"location":"functions/building-functions/","title":"Building functions","text":"<p>Building a function creates an OCI container image for your function that can be pushed to a container registry. It does not run or deploy the function, which can be useful if you want to build a container image for your function locally, but do not want to automatically run the function or deploy it to a cluster, for example, in a testing scenario.</p>"},{"location":"functions/building-functions/#local-builds","title":"Local builds","text":"<p>You can build a container image for your function locally without deploying it to a cluster, by using the <code>build</code> command.</p>"},{"location":"functions/building-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a Docker daemon on your local machine. This is already provided if you have used the Quickstart installation.</li> </ul>"},{"location":"functions/building-functions/#procedure","title":"Procedure","text":"<p>The <code>build</code> command uses the project name and the image registry name to construct a fully qualified container image name for the function. If the function project has not previously been built, you are prompted to provide an image registry.</p> funckn func <p>To build the function, run the following command:</p> <pre><code>func build\n</code></pre> <p>To build the function, run the following command:</p> <pre><code>kn func build\n</code></pre> <p>Note</p> <p>The coordinates for the image registry can be configured through an environment variable (<code>FUNC_REGISTRY</code>) as well.</p>"},{"location":"functions/building-functions/#on-cluster-builds","title":"On-cluster Builds","text":"<p>If you do not have a local Docker daemon running, or you are using a CI/CD pipeline, you might want to build your function on the cluster instead of using a local build. You can create an on-cluster build by using the <code>func deploy --remote</code> command.</p> <p>Note</p> <p>If you're doing a direct-upload deployment (i.e. the source code is on your local machine instead of a git repo), you can create an on-cluster build without the need of specifying the Git URL, but if you already specified a Git URL before, you'll need to specify the flag as empty, using the command <code>func deploy --remote --git-url=\"\"</code></p>"},{"location":"functions/building-functions/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>The function must exist in a Git repository.</li> <li>You must configure your cluster to use Tekton Pipelines. See the on-cluster build documentation.</li> </ul>"},{"location":"functions/building-functions/#procedure_1","title":"Procedure","text":"<p>When running the command for the first time, you must specify the Git URL for the function:</p> funckn func <pre><code>func deploy --remote --registry &lt;registry&gt; --git-url &lt;git-url&gt; -p hello\n</code></pre> <pre><code>kn func deploy --remote --registry &lt;registry&gt; --git-url &lt;git-url&gt; -p hello\n</code></pre> <p>After you have specified the Git URL for your function once, you can omit it in subsequent commands.</p>"},{"location":"functions/creating-functions/","title":"Creating functions","text":"<p>After you have installed Knative Functions, you can create a function project by using the <code>func</code> CLI or the <code>kn func</code> plugin:</p> <code>func</code> CLI<code>kn func</code> plugin <pre><code>func create -l &lt;language&gt; &lt;function-name&gt;\n</code></pre> <p>Example:</p> <pre><code>func create -l go hello\n</code></pre> <pre><code>kn func create -l &lt;language&gt; &lt;function-name&gt;\n</code></pre> <p>Example:</p> <pre><code>kn func create -l go hello\n</code></pre> <p>Expected output</p> <pre><code>Created go function in hello\n</code></pre> <p>For more information about options for function <code>create</code> commands, see the func create documentation.</p>"},{"location":"functions/deploying-functions/","title":"Deploying functions","text":"<p>Deploying a function creates an OCI container image for your function, and pushes this container image to your image registry. The function is deployed to the cluster as a Knative Service. Redeploying a function updates the container image and resulting Service that is running on your cluster. Functions that have been deployed to a cluster are accessible on the cluster just like any other Knative Service.</p>"},{"location":"functions/deploying-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>You have a Docker daemon on your local machine. This is already provided if you have used the Quickstart installation.</p> </li> <li> <p>You have access to a container registry and are able to push images to this registry. Note that some image registries set newly pushed images to private by default. If you are deploying a function for the first time, you may need to ensure that your images are set to public.</p> </li> </ul>"},{"location":"functions/deploying-functions/#procedure","title":"Procedure","text":"<p>The <code>deploy</code> command uses the function project name as the Knative Service name. When the function is built, the project name and the image registry name are used to construct a fully qualified image name for the function.</p> funckn func <p>Deploy the function by running the command inside the project directory:</p> <pre><code>func deploy --registry &lt;registry&gt;\n</code></pre> <p>Deploy the function by running the command inside the project directory:</p> <pre><code>kn func deploy --registry &lt;registry&gt;\n</code></pre> <p>Expected output</p> <pre><code>    \ud83d\ude4c Function image built: &lt;registry&gt;/hello:latest\n    \u2705 Function deployed in namespace \"default\" and exposed at URL:\n    http://hello.default.127.0.0.1.sslip.io\n</code></pre> <p>You can verify that your function has been successfully deployed by using the <code>invoke</code> command and observing the output:</p> funckn func <pre><code>func invoke\n</code></pre> <pre><code>kn func invoke\n</code></pre> <p>Expected output</p> <pre><code>Received response\nPOST / HTTP/1.1 hello.default.127.0.0.1.sslip.io\n  User-Agent: Go-http-client/1.1\n  Content-Length: 25\n  Accept-Encoding: gzip\n  Content-Type: application/json\n  K-Proxy-Request: activator\n  X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c\n  Forwarded: for=10.244.0.15;proto=http\n  X-Forwarded-For: 10.244.0.15, 10.244.0.9\n  X-Forwarded-Proto: http\nBody:\n</code></pre>"},{"location":"functions/install-func/","title":"Installing Knative Functions","text":"<p>You can install Knative Functions either by using the standalone <code>func</code> CLI, or by installing the <code>kn func</code> plugin that is available for the Knative <code>kn</code> CLI.</p>"},{"location":"functions/install-func/#installing-the-func-cli","title":"Installing the <code>func</code> CLI","text":"HomebrewExecutable binaryGoContainer image <p>To install <code>func</code> using Homebrew, run the following commands:</p> <pre><code>brew tap knative-extensions/kn-plugins\n</code></pre> <pre><code>brew install func\n</code></pre> <p>If you have already installed the <code>kn</code> CLI by using Homebrew, the <code>func</code> CLI is automatically recognized as a plugin to <code>kn</code>, and can be referenced as <code>kn func</code> or <code>func</code> interchangeably.</p> <p>Note</p> <p>Use <code>brew upgrade</code> instead if you are upgrading from a previous version.</p> <p>You can install <code>func</code> by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>func</code> release page.</p> </li> <li> <p>Rename the binary to <code>func</code> and make it executable by running the following commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; func\n</code></pre> <pre><code>chmod +x func\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>func_darwin_amd64</code> or <code>func_linux_amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your PATH by running the command:</p> <pre><code>mv func /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the CLI is working by running the command:</p> <pre><code>func version\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>func</code> client repository and navigate to the <code>func</code> directory:</p> <pre><code>git clone https://github.com/knative/func.git func\n</code></pre> <pre><code>cd func/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>make\n</code></pre> </li> <li> <p>Move <code>func</code> into your system path, and verify that <code>func</code> commands are working properly. For example:</p> <pre><code>func version\n</code></pre> </li> </ol> <p>Run <code>func</code> from a container image. For example:</p> <pre><code>docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc\n</code></pre> <p>Links to images are available here:</p> <ul> <li>Latest release</li> </ul> <p>Note</p> <p>Running <code>func</code> from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use <code>func</code>.</p>"},{"location":"functions/install-func/#installing-the-kn-func-cli-plugin","title":"Installing the <code>kn func</code> CLI plugin","text":"kn plugin <p>You can install Knative Functions as a <code>kn</code> CLI plugin, by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>func</code> release page.</p> </li> <li> <p>Rename the binary to <code>kn-func</code>, and make it executable by running the following commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; kn-func\n</code></pre> <pre><code>chmod +x kn-func\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>func_darwin_amd64</code> or <code>func_linux_amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your PATH by running the command:</p> <pre><code>mv kn-func /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the CLI is working by running the command:</p> <pre><code>kn func version\n</code></pre> </li> </ol>"},{"location":"functions/invoking-functions/","title":"Invoking functions","text":"<p>You can use the <code>func invoke</code> command to send a test request to invoke a function either locally or on your Knative cluster.</p> <p>This command can be used to test that a function is working and able to receive HTTP requests and CloudEvents correctly.</p> <p>If your function is running locally, <code>func invoke</code> sends a test request to the local instance.</p> <p>You can use the <code>func invoke</code> command to send test data to your function with the <code>--data</code> flag, as well as other options to simulate different types of requests. See the func invoke documentation for more information.</p>"},{"location":"functions/language-packs/","title":"Language packs","text":"<p>Language packs can be used to extend Knative Functions to support additional runtimes, function signatures, operating systems, and installed tooling for functions. Language Packs are distributed through Git repositories or as a directory on a disc.</p> <p>For more information see the language pack documentation.</p>"},{"location":"functions/language-packs/#using-external-git-repositories","title":"Using external Git repositories","text":"<p>When creating a new function, a Git repository can be specified as the source for the template files. The Knative Extensions maintains a set of example templates which can be used during project creation.</p> <p>For example, you can run the following command to use the <code>metacontroller</code> template for Node.js:</p> <pre><code>func create myfunc -l nodejs -t metacontroller --repository https://github.com/knative-extensions/func-tastic\n</code></pre>"},{"location":"functions/language-packs/#installing-language-packs-locally","title":"Installing language packs locally","text":"<p>Language packs can be installed locally by using the <code>func repository</code> command.</p> <p>For example, to add the Knative Extensions example templates, you can run the following command:</p> <pre><code>func repository add knative https://github.com/knative-extensions/func-tastic\n</code></pre> <p>After the Knative Extensions example templates are installed, you can use the <code>metacontroller</code> template by specifying the <code>knative</code> prefix in the <code>create</code> command:</p> <pre><code>func create -t knative/metacontroller -l nodejs my-controller-function\n</code></pre>"},{"location":"functions/running-functions/","title":"Running functions","text":"<p>Running a function creates an OCI container image for your function before running the function in your local environment, but does not deploy the function to a cluster. This can be useful if you want to run your function locally for a testing scenario.</p>"},{"location":"functions/running-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a Docker daemon on your local machine. This is already provided if you have used the Quickstart installation.</li> </ul>"},{"location":"functions/running-functions/#procedure","title":"Procedure","text":"<p>The <code>run</code> command builds an image for your function if required, and runs this image locally, instead of deploying it on a cluster.</p> func <p>Run the function locally by running the command inside the project directory. If you have not yet built the function you will need to provide the <code>--registry</code> flag:</p> <pre><code>cd hello\n</code></pre> <pre><code>func run [--registry &lt;registry&gt;]\n</code></pre> <p>Note</p> <p>The coordinates for the image registry can be configured through an environment variable (<code>FUNC_REGISTRY</code>) as well.</p> <p>Using this command also builds the function if necessary.</p> <p>You can force a rebuild of the image by running the command:</p> <pre><code>func run --build\n</code></pre> <p>It is also possible to disable the build, by running the command:</p> <pre><code>func run --build=false\n</code></pre> kn func <p>Run the function locally, by running the command inside the project directory:</p> <pre><code>cd hello\n</code></pre> <pre><code>kn func run\n</code></pre> <p>Using this command also builds the function if necessary.</p> <p>You can force a rebuild of the image by running the command:</p> <pre><code>kn func run --build\n</code></pre> <p>It is also possible to disable the build, by running the command:</p> <pre><code>kn func run --build=false\n</code></pre> <p>You can verify that your function has been successfully run by using the <code>invoke</code> command and observing the output:</p> funckn func <pre><code>func invoke\n</code></pre> <pre><code>kn func invoke\n</code></pre> <p>Expected output</p> <pre><code>Received response\nPOST / HTTP/1.1 hello.default.127.0.0.1.sslip.io\n  User-Agent: Go-http-client/1.1\n  Content-Length: 25\n  Accept-Encoding: gzip\n  Content-Type: application/json\n  K-Proxy-Request: activator\n  X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c\n  Forwarded: for=10.244.0.15;proto=http\n  X-Forwarded-For: 10.244.0.15, 10.244.0.9\n  X-Forwarded-Proto: http\nBody:\n</code></pre>"},{"location":"functions/subscribing-functions/","title":"Subscribe functions to CloudEvents","text":""},{"location":"functions/subscribing-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li>Knative Eventing installed on the cluster</li> </ul>"},{"location":"functions/subscribing-functions/#procedure","title":"Procedure","text":"<p>The <code>subscribe</code> command will connect the function to a set of events, matching a series of filters for Cloud Event metadata and a Knative Broker as the source of events, from where they are consumed.</p> funckn func <p>To subscribe the function to events for a given broker, run the following command:</p> <pre><code>func subscribe --filter type=com.example --filter extension=my-extension-value --source my-broker\n</code></pre> <p>To subscribe the function to events for the default broker, run the following command:</p> <pre><code>func subscribe --filter type=com.example --filter extension=my-extension-value\n</code></pre> <p>To subscribe the function to events for a given broker, run the following command:</p> <pre><code>kn func subscribe --filter type=com.example --filter extension=my-extension-value --source my-broker\n</code></pre> <p>To subscribe the function to events for the default broker, run the following command:</p> <pre><code>kn func subscribe --filter type=com.example --filter extension=my-extension-value\n</code></pre>"},{"location":"functions/subscribing-functions/#deployment-with-triggers","title":"Deployment with Triggers","text":"<p>When invoking <code>func deploy</code> the CLI will create Knative Triggers for the function.</p> funckn func <p>Deploy the function with Triggers by running the command inside the project directory:</p> <pre><code>func deploy\n</code></pre> <p>Deploy the function with Triggers by running the command inside the project directory:</p> <pre><code>kn func deploy\n</code></pre> <p>Expected output</p> <pre><code>    \ud83d\ude4c Function image built: &lt;registry&gt;/hello:latest\n    \ud83c\udfaf Creating Triggers on the cluster\n    \u2705 Function deployed in namespace \"default\" and exposed at URL:\n    http://hello.default.127.0.0.1.sslip.io\n</code></pre>"},{"location":"getting-started/","title":"Welcome to the Knative Quickstart tutorial","text":"<p>Following this Quickstart tutorial provides you with a simplified, local Knative installation by using the Knative <code>quickstart</code> plugin.</p> <p>You can use this simple Knative deployment to try out commonly used features of Knative Serving and Knative Eventing.</p> <p>We recommend that you complete the topics in this tutorial in order.</p>"},{"location":"getting-started/#before-you-begin","title":"Before you begin","text":"<p>Warning</p> <p>Knative <code>quickstart</code> environments are for experimentation use only. For a production ready installation, see the YAML-based installation or the Knative Operator installation.</p> <p>Before you can get started with a Knative <code>quickstart</code> deployment you must install:</p> <ul> <li> <p>kind (Kubernetes in Docker) or minikube to enable you to run a local Kubernetes cluster with Docker container nodes.</p> </li> <li> <p>The Kubernetes CLI (<code>kubectl</code>) to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs.</p> </li> <li> <p>The Knative CLI (<code>kn</code>). For instructions, see the next section.</p> </li> <li> <p>You need to have a minimum of 3\u00a0CPUs and 3\u00a0GB of RAM available for the cluster to be created.</p> </li> </ul> <p>Tip</p> <p>Hit . (period) on your keyboard to move forward in the tutorial. Use , (comma) to go back at any time.</p>"},{"location":"getting-started/about-knative-functions/","title":"About Knative Functions","text":"<p>Knative Functions provides a simple programming model for using functions on Knative, without requiring in-depth knowledge of Knative, Kubernetes, containers, or dockerfiles.</p> <p>Knative Functions enables you to easily create, build, and deploy stateless, event-driven functions as Knative Services by using the <code>func</code> CLI.</p> <p>When you build or run a function, an Open Container Initiative (OCI) format container image is generated automatically for you, and is stored in a container registry. Each time you update your code and then run or deploy it, the container image is also updated.</p> <p>You can create functions and manage function workflows by using the <code>func</code> CLI, or by using the <code>kn func</code> plugin for the Knative CLI.</p>"},{"location":"getting-started/build-run-deploy-func/","title":"Building, running, or deploying functions","text":"<p>After you have created a function project, you can build, run, or deploy your function, depending on your use case.</p>"},{"location":"getting-started/build-run-deploy-func/#running-a-function","title":"Running a function","text":"<p>Running a function creates an OCI container image for your function before running the function in your local environment, but does not deploy the function to a cluster. This can be useful if you want to run your function locally for a testing scenario.</p>"},{"location":"getting-started/build-run-deploy-func/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a Docker daemon on your local machine. This is already provided if you have used the Quickstart installation.</li> </ul>"},{"location":"getting-started/build-run-deploy-func/#procedure","title":"Procedure","text":"<p>The <code>run</code> command builds an image for your function if required, and runs this image locally, instead of deploying it on a cluster.</p> func <p>Run the function locally by running the command inside the project directory. If you have not yet built the function you will need to provide the <code>--registry</code> flag:</p> <pre><code>cd hello\n</code></pre> <pre><code>func run [--registry &lt;registry&gt;]\n</code></pre> <p>Note</p> <p>The coordinates for the image registry can be configured through an environment variable (<code>FUNC_REGISTRY</code>) as well.</p> <p>Using this command also builds the function if necessary.</p> <p>You can force a rebuild of the image by running the command:</p> <pre><code>func run --build\n</code></pre> <p>It is also possible to disable the build, by running the command:</p> <pre><code>func run --build=false\n</code></pre> kn func <p>Run the function locally, by running the command inside the project directory:</p> <pre><code>cd hello\n</code></pre> <pre><code>kn func run\n</code></pre> <p>Using this command also builds the function if necessary.</p> <p>You can force a rebuild of the image by running the command:</p> <pre><code>kn func run --build\n</code></pre> <p>It is also possible to disable the build, by running the command:</p> <pre><code>kn func run --build=false\n</code></pre> <p>You can verify that your function has been successfully run by using the <code>invoke</code> command and observing the output:</p> funckn func <pre><code>func invoke\n</code></pre> <pre><code>kn func invoke\n</code></pre> <p>Expected output</p> <pre><code>Received response\nPOST / HTTP/1.1 hello.default.127.0.0.1.sslip.io\n  User-Agent: Go-http-client/1.1\n  Content-Length: 25\n  Accept-Encoding: gzip\n  Content-Type: application/json\n  K-Proxy-Request: activator\n  X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c\n  Forwarded: for=10.244.0.15;proto=http\n  X-Forwarded-For: 10.244.0.15, 10.244.0.9\n  X-Forwarded-Proto: http\nBody:\n</code></pre>"},{"location":"getting-started/build-run-deploy-func/#deploying-a-function","title":"Deploying a function","text":"<p>Deploying a function creates an OCI container image for your function, and pushes this container image to your image registry. The function is deployed to the cluster as a Knative Service. Redeploying a function updates the container image and resulting Service that is running on your cluster. Functions that have been deployed to a cluster are accessible on the cluster just like any other Knative Service.</p>"},{"location":"getting-started/build-run-deploy-func/#prerequisites_1","title":"Prerequisites","text":"<ul> <li> <p>You have a Docker daemon on your local machine. This is already provided if you have used the Quickstart installation.</p> </li> <li> <p>You have access to a container registry and are able to push images to this registry. Note that some image registries set newly pushed images to private by default. If you are deploying a function for the first time, you may need to ensure that your images are set to public.</p> </li> </ul>"},{"location":"getting-started/build-run-deploy-func/#procedure_1","title":"Procedure","text":"<p>The <code>deploy</code> command uses the function project name as the Knative Service name. When the function is built, the project name and the image registry name are used to construct a fully qualified image name for the function.</p> funckn func <p>Deploy the function by running the command inside the project directory:</p> <pre><code>func deploy --registry &lt;registry&gt;\n</code></pre> <p>Deploy the function by running the command inside the project directory:</p> <pre><code>kn func deploy --registry &lt;registry&gt;\n</code></pre> <p>Expected output</p> <pre><code>    \ud83d\ude4c Function image built: &lt;registry&gt;/hello:latest\n    \u2705 Function deployed in namespace \"default\" and exposed at URL:\n    http://hello.default.127.0.0.1.sslip.io\n</code></pre> <p>You can verify that your function has been successfully deployed by using the <code>invoke</code> command and observing the output:</p> funckn func <pre><code>func invoke\n</code></pre> <pre><code>kn func invoke\n</code></pre> <p>Expected output</p> <pre><code>Received response\nPOST / HTTP/1.1 hello.default.127.0.0.1.sslip.io\n  User-Agent: Go-http-client/1.1\n  Content-Length: 25\n  Accept-Encoding: gzip\n  Content-Type: application/json\n  K-Proxy-Request: activator\n  X-Request-Id: 9e351834-0542-4f32-9928-3a5d6aece30c\n  Forwarded: for=10.244.0.15;proto=http\n  X-Forwarded-For: 10.244.0.15, 10.244.0.9\n  X-Forwarded-Proto: http\nBody:\n</code></pre>"},{"location":"getting-started/build-run-deploy-func/#building-a-function","title":"Building a function","text":"<p>Building a function creates an OCI container image for your function that can be pushed to a container registry. It does not run or deploy the function, which can be useful if you want to build a container image for your function locally, but do not want to automatically run the function or deploy it to a cluster, for example, in a testing scenario.</p>"},{"location":"getting-started/build-run-deploy-func/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>You have a Docker daemon on your local machine. This is already provided if you have used the Quickstart installation.</li> </ul>"},{"location":"getting-started/build-run-deploy-func/#procedure_2","title":"Procedure","text":"<p>The <code>build</code> command uses the project name and the image registry name to construct a fully qualified container image name for the function. If the function project has not previously been built, you are prompted to provide an image registry.</p> funckn func <p>To build the function, run the following command:</p> <pre><code>func build\n</code></pre> <p>To build the function, run the following command:</p> <pre><code>kn func build\n</code></pre> <p>Note</p> <p>The coordinates for the image registry can be configured through an environment variable (<code>FUNC_REGISTRY</code>) as well.</p>"},{"location":"getting-started/clean-up/","title":"Clean Up","text":"<p>We recommend that you delete the cluster used for this tutorial to free up resources on your local machine.</p> <p>If you want to continue experimenting with Knative after deleting the cluster, you can reinstall Knative on a new cluster using the <code>quickstart</code> plugin again.</p>"},{"location":"getting-started/clean-up/#delete-the-cluster","title":"Delete the Cluster","text":"kindminikube <p>Delete your <code>kind</code> cluster by running the command:</p> <pre><code>kind delete clusters knative\n</code></pre> <p>Example output</p> <pre><code>Deleted clusters: [\"knative\"]\n</code></pre> <p>Delete your <code>minikube</code> cluster by running the command:</p> <pre><code>minikube delete -p knative\n</code></pre> <p>Example output</p> <pre><code>\ud83d\udd25  Deleting \"knative\" in hyperkit ...\n\ud83d\udc80  Removed all traces of the \"knative\" cluster.\n</code></pre>"},{"location":"getting-started/create-a-function/","title":"Creating a function","text":"<p>After you have installed Knative Functions, you can create a function project by using the <code>func</code> CLI or the <code>kn func</code> plugin:</p> <code>func</code> CLI<code>kn func</code> plugin <pre><code>func create -l &lt;language&gt; &lt;function-name&gt;\n</code></pre> <p>Example:</p> <pre><code>func create -l go hello\n</code></pre> <pre><code>kn func create -l &lt;language&gt; &lt;function-name&gt;\n</code></pre> <p>Example:</p> <pre><code>kn func create -l go hello\n</code></pre> <p>Expected output</p> <pre><code>Created go function in hello\n</code></pre> <p>For more information about options for function <code>create</code> commands, see the func create documentation.</p>"},{"location":"getting-started/first-autoscale/","title":"Autoscaling","text":"<p>Knative Serving provides automatic scaling, also known as autoscaling. This means that a Knative Service by default scales down to zero running pods when it is not in use.</p>"},{"location":"getting-started/first-autoscale/#list-your-knative-service","title":"List your Knative Service","text":"<p>Use the Knative (<code>kn</code>) CLI to view the URL where your Knative Service is hosted:</p> knkubectl <p>View a list of Knative Services by running the command: <pre><code>kn service list\n</code></pre></p> <p>Expected output</p> <pre><code>NAME    URL                                                LATEST        AGE   CONDITIONS   READY\nhello   http://hello.default.${LOADBALANCER_IP}.sslip.io   hello-00001   13s   3 OK / 3     True\n</code></pre> <p>View a list of Knative Services by running the command: <pre><code>kubectl get ksvc\n</code></pre></p> <p>Expected output</p> <pre><code>NAME    URL                                                LATESTCREATED   LATESTREADY   READY   REASON\nhello   http://hello.default.${LOADBALANCER_IP}.sslip.io   hello-00001     hello-00001   True\n</code></pre>"},{"location":"getting-started/first-autoscale/#access-your-knative-service","title":"Access your Knative Service","text":"<p>Access your Knative Service by opening the previous URL in your browser or by running the command:</p> <pre><code>echo \"Accessing URL $(kn service describe hello -o url)\"\ncurl \"$(kn service describe hello -o url)\"\n</code></pre> <p>Expected output</p> <pre><code>Hello World!\n</code></pre> Are you seeing <code>curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io</code>? <p>In some cases your DNS server may be set up not to resolve <code>*.sslip.io</code> addresses. If you encounter this problem, it can be fixed by using a different nameserver to resolve these addresses.</p> <p>The exact steps will differ according to your distribution. For example, with Ubuntu derived systems which use <code>systemd-resolved</code>, you can add the following entry to the <code>/etc/systemd/resolved.conf</code>:</p> <pre><code>[Resolve]\nDNS=8.8.8.8\nDomains=~sslip.io.\n</code></pre> <p>Then simply restart the service with <code>sudo service systemd-resolved restart</code>.</p> <p>For MacOS users, you can add the DNS and domain using the network settings as explained here.</p>"},{"location":"getting-started/first-autoscale/#observe-autoscaling","title":"Observe autoscaling","text":"<p>Watch the pods and see how they scale to zero after traffic stops going to the URL:</p> <pre><code>kubectl get pod -l serving.knative.dev/service=hello -w\n</code></pre> <p>Note</p> <p>It may take up to 2 minutes for your pods to scale down. Pinging your service again resets this timer.</p> <p>Expected output</p> <pre><code>NAME                                     READY   STATUS\nhello-world                              2/2     Running\nhello-world                              2/2     Terminating\nhello-world                              1/2     Terminating\nhello-world                              0/2     Terminating\n</code></pre>"},{"location":"getting-started/first-autoscale/#scale-up-your-knative-service","title":"Scale up your Knative Service","text":"<p>Rerun the Knative Service in your browser. You can see a new pod running again:</p> <p>Expected output</p> <pre><code>NAME                                     READY   STATUS\nhello-world                              0/2     Pending\nhello-world                              0/2     ContainerCreating\nhello-world                              1/2     Running\nhello-world                              2/2     Running\n</code></pre> <p>Exit the <code>kubectl watch</code> command with <code>Ctrl+c</code>.</p>"},{"location":"getting-started/first-broker/","title":"Sources, Brokers, and Triggers","text":"<p>As part of the <code>kn quickstart</code> install, an InMemoryChannel-backed Broker is installed on your kind cluster.</p> <p>Verify that the Broker is installed by running the following command:</p> <pre><code>kn broker list\n</code></pre> <p>Expected output</p> <pre><code>NAME             URL                                                                                AGE   CONDITIONS   READY   REASON\nexample-broker   http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker     5m    5 OK / 5    True\n</code></pre> <p>Warning</p> <p>InMemoryChannel-backed Brokers are for development use only and must not be used in a production deployment.</p> <p>Next, you'll take a look at a simple implementation of Sources, Brokers, Triggers and Sinks using an app called the CloudEvents Player.</p>"},{"location":"getting-started/first-service/","title":"Deploying a Knative Service","text":"<p>In this tutorial, you will deploy a \"Hello world\" Knative Service that accepts the environment variable <code>TARGET</code> and prints <code>Hello ${TARGET}!</code>.</p> knYAML <p>Deploy the Service by running the command:</p> <pre><code>kn service create hello \\\n--image ghcr.io/knative/helloworld-go:latest \\\n--port 8080 \\\n--env TARGET=World\n</code></pre> <p>Expected output</p> <p><pre><code>Service hello created to latest revision 'hello-0001' is available at URL:\nhttp://hello.default.${LOADBALANCER_IP}.sslip.io\n</code></pre> The value of <code>${LOADBALANCER_IP}</code> above depends on your type of cluster, for <code>kind</code> it will be <code>127.0.0.1</code> for <code>minikube</code> depends on the local tunnel.</p> <ol> <li> <p>Copy the following YAML into a file named <code>hello.yaml</code>:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: hello\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          ports:\n            - containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"World\"\n</code></pre> </li> <li> <p>Deploy the Knative Service by running the command:</p> <pre><code>kubectl apply -f hello.yaml\n</code></pre> <p>Expected output</p> <pre><code>service.serving.knative.dev/hello created\n</code></pre> </li> </ol>"},{"location":"getting-started/first-source/","title":"Using a Knative Service as a source","text":"<p>In this tutorial, you will use the CloudEvents Player app to showcase the core concepts of Knative Eventing. By the end of this tutorial, you should have an architecture that looks like this:</p> <p></p> <p>The above image is Figure 6.6 from Knative in Action.</p>"},{"location":"getting-started/first-source/#creating-your-first-source","title":"Creating your first source","text":"<p>The CloudEvents Player acts as a Source for CloudEvents by intaking the name of the Broker as an environment variable, <code>BROKER_NAME</code>. If the Broker is located in a different namespace it is possible to also set the <code>BROKER_NAMESPACE</code> environment variable. Alternatively, you can just use the <code>BROKER_URI</code>.</p> <p>You will send CloudEvents to the Broker through the CloudEvents Player application.</p> <p>Create the CloudEvents Player Service:</p> knYAML <p>Run the command: <pre><code>kn service create cloudevents-player \\\n--image quay.io/ruben/cloudevents-player:latest\n</code></pre></p> <p>Expected output</p> <pre><code>Service 'cloudevents-player' created to latest revision 'cloudevents-player-00001' is available at URL:\nhttp://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io\n</code></pre> <ol> <li> <p>Copy the following YAML into a file named <code>cloudevents-player.yaml</code>:     <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: cloudevents-player\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/min-scale: \"1\"\n    spec:\n      containers:\n        - image: quay.io/ruben/cloudevents-player:latest\n</code></pre></p> </li> <li> <p>Apply the YAML file by running the command:     <pre><code>kubectl apply -f cloudevents-player.yaml\n</code></pre></p> <p>Expected output</p> <pre><code>service.serving.knative.dev/cloudevents-player created\n</code></pre> </li> </ol> <p>The service is now running but it doesn't know where the broker is so let's create a SinkBinding between the service and the broker.</p> knYAML <p>Run the command: <pre><code>kn source binding create ce-player-binding --subject \"Service:serving.knative.dev/v1:cloudevents-player\" --sink broker:example-broker\n</code></pre></p> <p>Expected output</p> <pre><code>Sink binding 'ce-player-binding' created in namespace 'default'.\n</code></pre> <ol> <li> <p>Copy the following YAML into a file named <code>cloudevents-player-binding.yaml</code>:     <pre><code>apiVersion: sources.knative.dev/v1\nkind: SinkBinding\nmetadata:\n  name: ce-player-binding\nspec:\n  sink:\n    ref:\n      apiVersion: eventing.knative.dev/v1\n      kind: Broker\n      name: example-broker\n  subject:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: cloudevents-player\n</code></pre></p> </li> <li> <p>Apply the YAML file by running the command:     <pre><code>kubectl apply -f cloudevents-player-binding.yaml\n</code></pre></p> <p>Expected output</p> <pre><code>sinkbinding.sources.knative.dev/ce-player-binding created\n</code></pre> </li> </ol>"},{"location":"getting-started/first-source/#examining-the-cloudevents-player","title":"Examining the CloudEvents Player","text":"<p>You can use the CloudEvents Player to send and receive CloudEvents. If you open the Service URL in your browser, the Create Event form appears.</p> <p>The Service URL is <code>http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io</code>, for example, http://cloudevents-player.default.127.0.0.1.sslip.io for <code>kind</code>.</p> <p></p> What do these fields mean? Field Description <code>Event ID</code> A unique ID. Click the loop icon to generate a new one. <code>Event Type</code> An event type. <code>Event Source</code> An event source. <code>Specversion</code> Demarcates which CloudEvents spec you're using (should always be 1.0). <code>Message</code> The <code>data</code> section of the CloudEvent, a payload which is carrying the data you care to be delivered. <p>For more information on the CloudEvents Specification, check out the CloudEvents Spec.</p>"},{"location":"getting-started/first-source/#sending-an-event","title":"Sending an event","text":"<p>Try sending an event using the CloudEvents Player interface:</p> <ol> <li>Fill in the form with whatever data you want.</li> <li>Ensure your Event Source does not contain any spaces.</li> <li>Click SEND EVENT.</li> </ol> <p></p> Clicking the  shows you the CloudEvent as the Broker sees it. <p></p> Want to send events using the command line instead? <p>As an alternative to the Web form, events can also be sent/viewed using the command line.</p> <p>To post an event: <pre><code>curl -i http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Ce-Id: 123456789\" \\\n    -H \"Ce-Specversion: 1.0\" \\\n    -H \"Ce-Type: some-type\" \\\n    -H \"Ce-Source: command-line\" \\\n    -d '{\"msg\":\"Hello CloudEvents!\"}'\n</code></pre></p> <p>And to view events: <pre><code>curl http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io/messages\n</code></pre></p> <p>The  icon in the \"Status\" column implies that the event has been sent to our Broker... but where has the event gone? Well, right now, nowhere!</p> <p>A Broker is simply a receptacle for events. In order for your events to be sent anywhere, you must create a Trigger which listens for your events and places them somewhere. And, you're in luck; you'll create your first Trigger on the next page!</p>"},{"location":"getting-started/first-traffic-split/","title":"Traffic splitting","text":"<p>Traffic splitting is useful for blue/green deployments and canary deployments.</p> <p>A Revision is a snapshot-in-time of application code and configuration. A new Revision is created every time you make changes to the configuration of a Knative Service. When splitting traffic, Knative splits traffic between different Revisions of your Knative Service.</p>"},{"location":"getting-started/first-traffic-split/#creating-a-new-revision","title":"Creating a new Revision","text":"<p>Instead of <code>TARGET=World</code>, update the environment variable <code>TARGET</code> on your Knative Service to greet \"Knative\" instead.</p> knYAML <p>Deploy the updated version of your Knative Service by running the command:</p> <pre><code>kn service update hello \\\n--env TARGET=Knative\n</code></pre> <p>As before, <code>kn</code> prints out some helpful information to the CLI.</p> <p>Expected output</p> <pre><code>Service 'hello' created to latest revision 'hello-00002' is available at URL:\nhttp://hello.default.${LOADBALANCER_IP}.sslip.io\n</code></pre> <ol> <li>Edit your existing <code>hello.yaml</code> file to contain the following:     <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: hello\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          ports:\n            - containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"Knative\"\n</code></pre></li> <li> <p>Deploy the updated version of your Knative Service by running the command:     <pre><code>kubectl apply -f hello.yaml\n</code></pre></p> <p>Expected output</p> <pre><code>service.serving.knative.dev/hello configured\n</code></pre> </li> </ol> <p>Because you are updating an existing Knative Service, the URL won't change, but the new Revision has the new name <code>hello-00002</code>.</p>"},{"location":"getting-started/first-traffic-split/#accessing-the-new-revision","title":"Accessing the new Revision","text":"<p>To see the change, access the Knative Service again on your browser or use <code>curl</code> in your terminal:</p> <pre><code>echo \"Accessing URL $(kn service describe hello -o url)\"\ncurl \"$(kn service describe hello -o url)\"\n</code></pre> <p>Expected output</p> <pre><code>Hello Knative!\n</code></pre>"},{"location":"getting-started/first-traffic-split/#view-existing-revisions","title":"View existing Revisions","text":"<p>You can view a list of existing Revisions by using the Knative (<code>kn</code>) or <code>kubectl</code> CLI:</p> knkubectl <p>View a list of revisions by running the command: <pre><code>kn revisions list\n</code></pre></p> <p>Expected output</p> <pre><code>NAME            SERVICE   TRAFFIC   TAGS   GENERATION   AGE   CONDITIONS   READY   REASON\nhello-00002     hello     100%             2            30s   3 OK / 4     True\nhello-00001     hello                      1            5m    3 OK / 4     True\n</code></pre> <p>View a list of Revisions by running the command: <pre><code>kubectl get revisions\n</code></pre></p> <p>Expected output</p> <pre><code>NAME          CONFIG NAME   K8S SERVICE NAME   GENERATION   READY   REASON   ACTUAL REPLICAS   DESIRED REPLICAS\nhello-00001   hello                            1            True             0                 0\nhello-00002   hello                            2            True             0                 0\n</code></pre> <p>When running the <code>kn</code> command, the relevant column is <code>TRAFFIC</code>. You can see that 100% of traffic is going to the latest Revision, <code>hello-00002</code>, which is on the row with the highest <code>GENERATION</code>. 0% of traffic is going to the Revision <code>hello-00001</code>.</p> <p>When you create a new Revision of a Knative Service, Knative defaults to directing 100% of traffic to this latest Revision. You can change this default behavior by specifying how much traffic you want each Revision to receive.</p>"},{"location":"getting-started/first-traffic-split/#splitting-traffic-between-revisions","title":"Splitting traffic between Revisions","text":"<p>Split the traffic between the two Revisions:</p> knYAML <p>Run the command: <pre><code>kn service update hello \\\n--traffic hello-00001=50 \\\n--traffic @latest=50\n</code></pre></p> <ol> <li>Add the <code>traffic</code> section to the bottom of your existing <code>hello.yaml</code> file:     <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: hello\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          ports:\n            - containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"Knative\"\n  traffic:\n  - latestRevision: true\n    percent: 50\n  - latestRevision: false\n    percent: 50\n    revisionName: hello-00001\n</code></pre></li> <li>Apply the YAML by running the command:     <pre><code>kubectl apply -f hello.yaml\n</code></pre></li> </ol> <p>Info</p> <p><code>@latest</code> always points to the \"latest\" Revision, which in this case is <code>hello-00002</code>.</p>"},{"location":"getting-started/first-traffic-split/#verify-the-traffic-split","title":"Verify the traffic split","text":"<p>To verify that the traffic split has configured correctly, list the Revisions again by running the command:</p> <pre><code>kn revisions list\n</code></pre> <p>Expected output</p> <pre><code>NAME            SERVICE   TRAFFIC   TAGS   GENERATION   AGE   CONDITIONS   READY   REASON\nhello-00002     hello     50%              2            10m   3 OK / 4     True\nhello-00001     hello     50%              1            36m   3 OK / 4     True\n</code></pre> <p>Access your Knative Service multiple times in your browser to see the different output being served by each Revision.</p> <p>Similarly, you can access the Service URL from the terminal multiple times to see the traffic being split between the Revisions.</p> <pre><code>echo \"Accessing URL $(kn service describe hello -o url)\"\ncurl \"$(kn service describe hello -o url)\"\n</code></pre> <p>Expected output</p> <pre><code>Hello Knative!\nHello World!\nHello Knative!\nHello World!\n</code></pre>"},{"location":"getting-started/first-trigger/","title":"Using Triggers and sinks","text":"<p>In the last topic we used the CloudEvents Player as an event source to send events to the Broker. We now want the event to go from the Broker to an event sink.</p> <p>In this topic, we will use the CloudEvents Player as the sink as well as a source. This means we will be using the CloudEvents Player to both send and receive events. We will use a Trigger to listen for events in the Broker to send to the sink.</p>"},{"location":"getting-started/first-trigger/#creating-your-first-trigger","title":"Creating your first Trigger","text":"<p>Create a Trigger that listens for CloudEvents from the event source and places them into the sink, which is also the CloudEvents Player app.</p> knYAML <p>To create the Trigger, run the command: <pre><code>kn trigger create cloudevents-trigger --sink cloudevents-player  --broker example-broker\n</code></pre></p> <p>Expected output</p> <pre><code>Trigger 'cloudevents-trigger' successfully created in namespace 'default'.\n</code></pre> <ol> <li> <p>Copy the following YAML into a file named <code>ce-trigger.yaml</code>:     <pre><code>apiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: cloudevents-trigger\n  annotations:\n    knative-eventing-injection: enabled\nspec:\n  broker: example-broker\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: cloudevents-player\n</code></pre></p> </li> <li> <p>Create the Trigger by running the command:     <pre><code>kubectl apply -f ce-trigger.yaml\n</code></pre></p> <p>Expected output</p> <pre><code>trigger.eventing.knative.dev/cloudevents-trigger created\n</code></pre> </li> </ol> What CloudEvents is my Trigger listening for? <p>Because we didn't specify a <code>--filter</code> in our <code>kn</code> command, the Trigger is listening for any CloudEvents coming into the Broker.</p> <p>Expand the next note to see how to use Filters.</p> <p>Now, when we go back to the CloudEvents Player and send an event, we see that CloudEvents are both sent and received by the CloudEvents Player:</p> <p></p> <p>You may need to refresh the page to see your changes.</p> What if I want to filter on CloudEvent attributes? <p>First, delete your existing Trigger: <pre><code>  kn trigger delete cloudevents-trigger\n</code></pre> Now let's add a Trigger that listens for a certain CloudEvent Type <pre><code>  kn trigger create cloudevents-player-filter --sink cloudevents-player  --broker example-broker --filter type=some-type\n</code></pre></p> <p>If you send a CloudEvent with type <code>some-type</code>, it is reflected in the CloudEvents Player UI.  The Trigger ignores any other types.</p> <p>You can filter on any aspect of the CloudEvent you would like to.</p> <p>Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes  </p>"},{"location":"getting-started/getting-started-eventing/","title":"About Knative Eventing","text":"<p>Knative Eventing provides you with helpful tools that can be used to create event-driven applications, by easily attaching event sources, triggers, and other options to your Knative Services.</p> <p>Event-driven applications are designed to detect events as they occur, and process these events by using user-defined, event-handling procedures.</p> <p>Tip</p> <p>To find out more about event-driven architecture and Knative Eventing, check out this CNCF Session about event-driven architecture with Knative events</p> <p>After you install Knative Eventing, you can create, send, and verify events. This tutorial shows how you can use a basic workflow for managing events that uses Event Sources, Brokers, Triggers, and Sinks.</p>"},{"location":"getting-started/install-func/","title":"Installing Knative Functions","text":"<p>You can install Knative Functions either by using the standalone <code>func</code> CLI, or by installing the <code>kn func</code> plugin that is available for the Knative <code>kn</code> CLI.</p>"},{"location":"getting-started/install-func/#installing-the-func-cli","title":"Installing the <code>func</code> CLI","text":"HomebrewExecutable binaryGoContainer image <p>To install <code>func</code> using Homebrew, run the following commands:</p> <pre><code>brew tap knative-extensions/kn-plugins\n</code></pre> <pre><code>brew install func\n</code></pre> <p>If you have already installed the <code>kn</code> CLI by using Homebrew, the <code>func</code> CLI is automatically recognized as a plugin to <code>kn</code>, and can be referenced as <code>kn func</code> or <code>func</code> interchangeably.</p> <p>Note</p> <p>Use <code>brew upgrade</code> instead if you are upgrading from a previous version.</p> <p>You can install <code>func</code> by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>func</code> release page.</p> </li> <li> <p>Rename the binary to <code>func</code> and make it executable by running the following commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; func\n</code></pre> <pre><code>chmod +x func\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>func_darwin_amd64</code> or <code>func_linux_amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your PATH by running the command:</p> <pre><code>mv func /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the CLI is working by running the command:</p> <pre><code>func version\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>func</code> client repository and navigate to the <code>func</code> directory:</p> <pre><code>git clone https://github.com/knative/func.git func\n</code></pre> <pre><code>cd func/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>make\n</code></pre> </li> <li> <p>Move <code>func</code> into your system path, and verify that <code>func</code> commands are working properly. For example:</p> <pre><code>func version\n</code></pre> </li> </ol> <p>Run <code>func</code> from a container image. For example:</p> <pre><code>docker run --rm -it ghcr.io/knative/func/func create -l node -t http myfunc\n</code></pre> <p>Links to images are available here:</p> <ul> <li>Latest release</li> </ul> <p>Note</p> <p>Running <code>func</code> from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use <code>func</code>.</p>"},{"location":"getting-started/install-func/#installing-the-kn-func-cli-plugin","title":"Installing the <code>kn func</code> CLI plugin","text":"kn plugin <p>You can install Knative Functions as a <code>kn</code> CLI plugin, by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>func</code> release page.</p> </li> <li> <p>Rename the binary to <code>kn-func</code>, and make it executable by running the following commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; kn-func\n</code></pre> <pre><code>chmod +x kn-func\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>func_darwin_amd64</code> or <code>func_linux_amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your PATH by running the command:</p> <pre><code>mv kn-func /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the CLI is working by running the command:</p> <pre><code>kn func version\n</code></pre> </li> </ol>"},{"location":"getting-started/next-steps/","title":"Next Steps","text":"<p>This topic provides a list of resources to help you continue your Knative journey.</p> <p></p>"},{"location":"getting-started/next-steps/#knative-samples","title":"Knative samples","text":"<p>Try out some of the following Knative samples:</p> <ul> <li>Converting a Kubernetes Service to a Knative Service</li> <li>Knative Serving code samples</li> <li>Knative Eventing and Event source code samples</li> </ul>"},{"location":"getting-started/next-steps/#explore-the-knative-docs","title":"Explore the Knative docs","text":"<p>See the following guides for documentation specific to your use case:</p> <ul> <li>Serving Guide</li> <li>Eventing Guide</li> </ul>"},{"location":"getting-started/next-steps/#knative-books","title":"Knative books","text":"<p>Books to help you understand Knative concepts and get additional examples:</p> <ul> <li> <p>Knative in Action</p> Want a free digital copy of Knative in Action? <p>VMWare has generously donated access to a free copy of the Knative in Action eBook, get your copy here!</p> </li> <li> <p>Knative Cookbook</p> </li> </ul>"},{"location":"getting-started/next-steps/#other-knative-links","title":"Other Knative links","text":"<p>Other links to help you get started with Knative:</p> <ul> <li>Knative YouTube Channel</li> </ul>"},{"location":"getting-started/next-steps/#are-we-missing-something","title":"Are we missing something?","text":"<p>We'd love to help you along the next step of your Knative journey. If we're missing something on this page that you feel should be here, please give us feedback!</p>"},{"location":"getting-started/quickstart-install/","title":"Install Knative using quickstart","text":"<p>Following this quickstart tutorial provides you with a simplified, local Knative installation by using the Knative <code>quickstart</code> plugin.</p>"},{"location":"getting-started/quickstart-install/#before-you-begin","title":"Before you begin","text":"<p>Warning</p> <p>Knative <code>quickstart</code> environments are for experimentation use only. For a production ready installation, see the YAML-based installation or the Knative Operator installation.</p> <p>Before you can get started with a Knative <code>quickstart</code> deployment you must install:</p> <ul> <li> <p>kind (Kubernetes in Docker) or minikube to enable you to run a local Kubernetes cluster with Docker container nodes.</p> </li> <li> <p>The Kubernetes CLI (<code>kubectl</code>) to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs.</p> </li> <li> <p>The Knative CLI (<code>kn</code>). For instructions, see the next section.</p> </li> <li> <p>You need to have a minimum of 3\u00a0CPUs and 3\u00a0GB of RAM available for the cluster to be created.</p> </li> </ul>"},{"location":"getting-started/quickstart-install/#install-the-knative-cli","title":"Install the Knative CLI","text":"<p>The Knative CLI (<code>kn</code>) provides a quick and easy interface for creating Knative resources, such as Knative Services and Event Sources, without the need to create or modify YAML files directly.</p> <p>The <code>kn</code> CLI also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting.</p> Using HomebrewUsing a binaryUsing GoUsing a container image <p>Do one of the following:</p> <ul> <li> <p>To install <code>kn</code> by using Homebrew, run the command (Use <code>brew upgrade</code> instead if you are upgrading from a previous version):</p> <pre><code>brew install knative/client/kn\n</code></pre> Having issues upgrading <code>kn</code> using Homebrew? <p>If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the <code>master</code> branch was renamed to <code>main</code>. Resolve this issue by running the command:</p> <pre><code>brew uninstall kn\nbrew untap knative/client --force\nbrew install knative/client/kn\n</code></pre> </li> </ul> <p>You can install <code>kn</code> by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>kn</code> release page.</p> </li> <li> <p>Rename the binary to <code>kn</code> and make it executable by running the commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; kn\nchmod +x kn\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>kn-darwin-amd64</code> or <code>kn-linux-amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn /usr/local/bin\n</code></pre> </li> <li> <p>Verify that <code>kn</code> commands are working properly. For example:</p> <pre><code>kn version\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>kn</code> client repository:</p> <pre><code>git clone https://github.com/knative/client.git\ncd client/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>hack/build.sh -f\n</code></pre> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn /usr/local/bin\n</code></pre> </li> <li> <p>Verify that <code>kn</code> commands are working properly. For example:</p> <pre><code>kn version\n</code></pre> </li> </ol> <p>Links to images are available here:</p> <ul> <li>Latest release</li> </ul> <p>You can run <code>kn</code> from a container image. For example:</p> <pre><code>docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list\n</code></pre> <p>Note</p> <p>Running <code>kn</code> from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use <code>kn</code>.</p>"},{"location":"getting-started/quickstart-install/#install-the-knative-quickstart-plugin","title":"Install the Knative quickstart plugin","text":"<p>To get started, install the Knative <code>quickstart</code> plugin:</p> Using HomebrewUsing a binaryUsing Go <p>Do one of the following:</p> <ul> <li> <p>To install the <code>quickstart</code> plugin by using Homebrew, run the command (Use <code>brew upgrade</code> instead if you are upgrading from a previous version):</p> <pre><code>brew install knative-extensions/kn-plugins/quickstart\n</code></pre> </li> </ul> <ol> <li> <p>Download the binary for your system from the <code>quickstart</code> release page.</p> </li> <li> <p>Rename the file to remove the OS and architecture information. For example, rename <code>kn-quickstart-amd64</code> to <code>kn-quickstart</code>.</p> </li> <li> <p>Make the plugin executable. For example, <code>chmod +x kn-quickstart</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn-quickstart /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the plugin is working by running the command:</p> <pre><code>kn quickstart --help\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>kn-plugin-quickstart</code> repository:</p> <pre><code>git clone https://github.com/knative-extensions/kn-plugin-quickstart.git\ncd kn-plugin-quickstart/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>hack/build.sh\n</code></pre> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn-quickstart /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the plugin is working by running the command:</p> <pre><code>kn quickstart --help\n</code></pre> </li> </ol>"},{"location":"getting-started/quickstart-install/#run-the-knative-quickstart-plugin","title":"Run the Knative quickstart plugin","text":"<p>The <code>quickstart</code> plugin completes the following functions:</p> <ol> <li>Checks if you have the selected Kubernetes instance installed</li> <li>Creates a cluster called <code>knative</code></li> <li>Installs Knative Serving with Kourier as the default networking layer, and sslip.io as the DNS</li> <li>Installs Knative Eventing and creates an in-memory Broker and Channel implementation</li> </ol> <p>To get a local deployment of Knative, run the <code>quickstart</code> plugin:</p> Using kindUsing minikube <ol> <li> <p>Install Knative and Kubernetes using kind by running:     <pre><code>kn quickstart kind\n</code></pre></p> <p>Note</p> <p>Quickstart uses Port 80, and it will fail to install if any other services are bound on that port. If you have a service using Port 80, you'll need to stop it before using Quickstart. To check if another service is using Port 80: <pre><code>netstat -tnlp | grep 80\n</code></pre></p> </li> <li> <p>After the plugin is finished, verify you have a cluster called <code>knative</code>:</p> <pre><code>kind get clusters\n</code></pre> </li> </ol> <ol> <li> <p>Install Knative and Kubernetes in a minikube instance by running:</p> <p>Note</p> <p>The minikube cluster will be created with 3\u00a0GB of RAM. You can change to a different value not lower than 3\u00a0GB by setting a memory config in minikube. For example,  <code>minikube config set memory 4096</code> will use 4\u00a0GB of RAM.</p> <pre><code>kn quickstart minikube\n</code></pre> </li> <li> <p>The output of the previous command asked you to run minikube tunnel.    Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue:     <pre><code>minikube tunnel --profile knative\n</code></pre>     The tunnel must continue to run in a terminal window any time you are using your Knative <code>quickstart</code> environment.</p> <p>The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer.</p> <p>Note</p> <p>To terminate the tunnel process and clean up network routes, enter <code>Ctrl-C</code>. For more information about the <code>minikube tunnel</code> command, see the minikube documentation.</p> </li> <li> <p>After the plugin is finished, verify you have a cluster called <code>knative</code>:</p> <pre><code>minikube profile list\n</code></pre> </li> </ol>"},{"location":"getting-started/quickstart-install/#next-steps","title":"Next steps","text":"<p>Now that you have installed Knative, you can learn how to deploy your first Knative Service in the next topic in this tutorial.</p>"},{"location":"install/","title":"Installing Knative","text":"<p>Note</p> <p>Please also take a look at the Serving Architecture, which explains the Knative components and the general networking concept.</p> <p>You can install the Serving component, Eventing component, or both on your cluster by using one of the following deployment options:</p> <ul> <li> <p>Use the Knative Quickstart plugin to install a preconfigured, local distribution of Knative for development purposes.</p> </li> <li> <p>Use a YAML-based installation to install a production ready deployment:</p> <ul> <li>Install Knative Serving by using YAML</li> <li>Install Knative Eventing by using YAML</li> </ul> </li> <li> <p>Use the Knative Operator to install and configure a production-ready deployment.</p> </li> <li> <p>Follow the documentation for vendor-managed Knative offerings.</p> </li> </ul> <p>You can also upgrade an existing Knative installation.</p> <p>Note</p> <p>Knative installation instructions assume you are running Mac or Linux with a Bash shell.</p>"},{"location":"install/installing-cert-manager/","title":"Installing cert-manager for TLS certificates","text":"<p>Knative leverages cert-manager to request TLS certificates used for secure HTTPS connections in Knative. Installing cert-manager is required before enabling any of the Knative encryption features. Follow the steps below for the installation.</p>"},{"location":"install/installing-cert-manager/#before-you-begin","title":"Before you begin","text":"<p>You must meet the following requirements to install cert-manager for Knative:</p> <ul> <li>Knative currently supports cert-manager version <code>1.0.0</code> and higher.</li> </ul>"},{"location":"install/installing-cert-manager/#downloading-and-installing-cert-manager","title":"Downloading and installing cert-manager","text":"<p>To download and install cert-manager, follow the Installation steps from the official <code>cert-manager</code> website.</p>"},{"location":"install/installing-cert-manager/#using-cert-manager-with-knative","title":"Using cert-manager with Knative","text":"<p>Knative encryption can be configured in:</p> <ul> <li>Serving: Encryption Overview</li> </ul>"},{"location":"install/installing-istio/","title":"Installing Istio for Knative","text":"<p>This guide walks you through manually installing and customizing Istio for use with Knative.</p> <p>If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation.</p>"},{"location":"install/installing-istio/#before-you-begin","title":"Before you begin","text":"<p>You need:</p> <ul> <li>A Kubernetes cluster created.</li> <li><code>istioctl</code> installed.</li> <li>Knative Serving installed (can also be installed after the Istio).</li> </ul>"},{"location":"install/installing-istio/#supported-istio-versions","title":"Supported Istio versions","text":"<p>You can view the latest tested Istio version on the Knative Net Istio releases page.</p>"},{"location":"install/installing-istio/#installing-istio","title":"Installing Istio","text":"<p>When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Basic installation with istioctl instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs.</p>"},{"location":"install/installing-istio/#basic-installation-with-istioctl","title":"Basic installation with istioctl","text":"<ol> <li> <p>You can easily install and customize your Istio installation with <code>istioctl</code>.</p> <pre><code>istioctl install -y\n</code></pre> </li> <li> <p>To integrate Istio with Knative Serving install the Knative Istio controller by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml\n</code></pre> <p>Hint</p> <p>Make sure to also install Knative Serving and configure DNS.</p> </li> </ol>"},{"location":"install/installing-istio/#forming-a-service-mesh","title":"Forming a service mesh","text":"<p>The Istio service mesh provides a few benefits:</p> <ul> <li> <p>Allows you to turn on mutual TLS, which secures service-to-service   traffic within the cluster.</p> </li> <li> <p>Allows you to use the Istio authorization policy, controlling the access   to each Knative service based on Istio service roles.</p> </li> </ul> <p>If you want to use Istio as a service mesh, you must make sure that istio sidecars are injected to all <code>pods</code> that should be part of the service mesh. There are two ways to achieve this:</p> <ul> <li> <p>Use automatic sidecar injection and set the <code>istio-injection=enabled</code> label on all <code>namespaces</code>   that should be part of the service-mesh</p> </li> <li> <p>Use manual sidecar injection on all <code>pods</code> that should be part of the service-mesh</p> </li> </ul>"},{"location":"install/installing-istio/#using-istio-mtls-feature-with-knative","title":"Using Istio mTLS feature with Knative","text":"<p>Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment.</p> <p>Note</p> <p>It is strongly recommended to use automatic sidecar injection to avoid manually injection sidecars to all <code>pods</code> in <code>knative-serving</code>.</p> <ol> <li> <p>Enable sidecar injection on <code>knative-serving</code> system namespace.</p> <pre><code>kubectl label namespace knative-serving istio-injection=enabled\n</code></pre> </li> <li> <p>Set <code>PeerAuthentication</code> to <code>PERMISSIVE</code> on knative-serving system namespace by creating a YAML file using the following template:</p> <pre><code>apiVersion: \"security.istio.io/v1beta1\"\nkind: \"PeerAuthentication\"\nmetadata:\n  name: \"default\"\n  namespace: \"knative-serving\"\nspec:\n  mtls:\n    mode: PERMISSIVE\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"install/installing-istio/#configuring-the-installation","title":"Configuring the installation","text":""},{"location":"install/installing-istio/#updating-the-config-istio-configmap-to-use-a-non-default-local-gateway","title":"Updating the <code>config-istio</code> configmap to use a non-default local gateway","text":"<p>If you create a custom service and deployment for local gateway with a name other than <code>knative-local-gateway</code>, you need to update gateway configmap <code>config-istio</code> under the <code>knative-serving</code> namespace.</p> <ol> <li> <p>Edit the <code>config-istio</code> configmap:</p> <pre><code>kubectl edit configmap config-istio -n knative-serving\n</code></pre> </li> <li> <p>Replace the <code>local-gateways</code> field with the custom service. As an example, if you name both the service and deployment <code>custom-local-gateway</code> under the namespace <code>istio-system</code>, it should be updated to:</p> <pre><code>local-gateways: |\n  - name: knative-local-gateway\n    namespace: knative-serving\n    service: custom-local-gateway.istio-system.svc.cluster.local\n</code></pre> </li> </ol> <p>As an example, if both the custom service and deployment are labeled with <code>custom: custom-local-gateway</code>, not the default <code>istio: knative-local-gateway</code>, you must update gateway instance <code>knative-local-gateway</code> in the <code>knative-serving</code> namespace:</p> <pre><code>kubectl edit gateway knative-local-gateway -n knative-serving\n</code></pre> <p>Replace the label selector with the label of your service:</p> <pre><code>istio: knative-local-gateway\n</code></pre> <p>For the service mentioned earlier, it should be updated to:</p> <pre><code>custom: custom-local-gateway\n</code></pre> <p>If there is a change in service ports (compared to that of <code>knative-local-gateway</code>), update the port info in the gateway accordingly.</p>"},{"location":"install/installing-istio/#verifying-your-istio-installation","title":"Verifying your Istio installation","text":"<p>View the status of your Istio installation to make sure the installation was successful. You can use <code>istioctl</code> to verify the installation:</p> <pre><code>istioctl verify-install\n</code></pre>"},{"location":"install/installing-istio/#istio-resources","title":"Istio resources","text":"<ul> <li> <p>For the official Istio installation guide, see the   Istio Kubernetes Getting Started Guide.</p> </li> <li> <p>For the full list of available configs when installing Istio with <code>istioctl</code>, see   the   Istio Installation Options reference.</p> </li> </ul>"},{"location":"install/installing-istio/#clean-up-istio","title":"Clean up Istio","text":"<p>See the Uninstall Istio.</p>"},{"location":"install/installing-istio/#whats-next","title":"What's next","text":"<ul> <li>View the Knative Serving documentation.</li> <li>Try some Knative Serving code samples.</li> </ul>"},{"location":"install/knative-offerings/","title":"Knative Offerings","text":"<p>Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products.  Please check with each of these vendors for what is or is not supported.</p> <p>Here is a list of commercial Knative products (alphabetically):</p> <ul> <li>Cloud Native Runtimes for VMware Tanzu: A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster</li> <li>Control Plane Corporation: A hybrid platform that combines the services, regions and computing power of AWS, GCP, Azure and any other public or private cloud to provide developers with a flexible yet unbreakable global environment for building backend applications and services. </li> <li>Gardener: Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime.</li> <li>Google Cloud Run for Anthos: Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications.</li> <li>Google Cloud Run: A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds.</li> <li>IBM Cloud Code Engine: A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions.</li> <li>Nutanix Karbon: Extend Nutanix Karbon with serverless capabilities by installing Knative on a Karbon managed Kubernetes cluster.</li> <li>Red Hat Openshift Serverless: enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications.</li> <li>TriggerMesh: On-premises and fully-managed Knative and event-driven integration platform. With support for AWS, Azure, Google, and many more cloud and enterprise event sources and brokers. Commercial Knative support and professional services available.</li> </ul>"},{"location":"install/quickstart-install/","title":"Install Knative using quickstart","text":"<p>Following this quickstart tutorial provides you with a simplified, local Knative installation by using the Knative <code>quickstart</code> plugin.</p>"},{"location":"install/quickstart-install/#before-you-begin","title":"Before you begin","text":"<p>Warning</p> <p>Knative <code>quickstart</code> environments are for experimentation use only. For a production ready installation, see the YAML-based installation or the Knative Operator installation.</p> <p>Before you can get started with a Knative <code>quickstart</code> deployment you must install:</p> <ul> <li> <p>kind (Kubernetes in Docker) or minikube to enable you to run a local Kubernetes cluster with Docker container nodes.</p> </li> <li> <p>The Kubernetes CLI (<code>kubectl</code>) to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs.</p> </li> <li> <p>The Knative CLI (<code>kn</code>). For instructions, see the next section.</p> </li> <li> <p>You need to have a minimum of 3\u00a0CPUs and 3\u00a0GB of RAM available for the cluster to be created.</p> </li> </ul>"},{"location":"install/quickstart-install/#install-the-knative-cli","title":"Install the Knative CLI","text":"<p>The Knative CLI (<code>kn</code>) provides a quick and easy interface for creating Knative resources, such as Knative Services and Event Sources, without the need to create or modify YAML files directly.</p> <p>The <code>kn</code> CLI also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting.</p> Using HomebrewUsing a binaryUsing GoUsing a container image <p>Do one of the following:</p> <ul> <li> <p>To install <code>kn</code> by using Homebrew, run the command (Use <code>brew upgrade</code> instead if you are upgrading from a previous version):</p> <pre><code>brew install knative/client/kn\n</code></pre> Having issues upgrading <code>kn</code> using Homebrew? <p>If you are having issues upgrading using Homebrew, it might be due to a change to a CLI repository where the <code>master</code> branch was renamed to <code>main</code>. Resolve this issue by running the command:</p> <pre><code>brew uninstall kn\nbrew untap knative/client --force\nbrew install knative/client/kn\n</code></pre> </li> </ul> <p>You can install <code>kn</code> by downloading the executable binary for your system and placing it in the system path.</p> <ol> <li> <p>Download the binary for your system from the <code>kn</code> release page.</p> </li> <li> <p>Rename the binary to <code>kn</code> and make it executable by running the commands:</p> <pre><code>mv &lt;path-to-binary-file&gt; kn\nchmod +x kn\n</code></pre> <p>Where <code>&lt;path-to-binary-file&gt;</code> is the path to the binary file you downloaded in the previous step, for example, <code>kn-darwin-amd64</code> or <code>kn-linux-amd64</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn /usr/local/bin\n</code></pre> </li> <li> <p>Verify that <code>kn</code> commands are working properly. For example:</p> <pre><code>kn version\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>kn</code> client repository:</p> <pre><code>git clone https://github.com/knative/client.git\ncd client/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>hack/build.sh -f\n</code></pre> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn /usr/local/bin\n</code></pre> </li> <li> <p>Verify that <code>kn</code> commands are working properly. For example:</p> <pre><code>kn version\n</code></pre> </li> </ol> <p>Links to images are available here:</p> <ul> <li>Latest release</li> </ul> <p>You can run <code>kn</code> from a container image. For example:</p> <pre><code>docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list\n</code></pre> <p>Note</p> <p>Running <code>kn</code> from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use <code>kn</code>.</p>"},{"location":"install/quickstart-install/#install-the-knative-quickstart-plugin","title":"Install the Knative quickstart plugin","text":"<p>To get started, install the Knative <code>quickstart</code> plugin:</p> Using HomebrewUsing a binaryUsing Go <p>Do one of the following:</p> <ul> <li> <p>To install the <code>quickstart</code> plugin by using Homebrew, run the command (Use <code>brew upgrade</code> instead if you are upgrading from a previous version):</p> <pre><code>brew install knative-extensions/kn-plugins/quickstart\n</code></pre> </li> </ul> <ol> <li> <p>Download the binary for your system from the <code>quickstart</code> release page.</p> </li> <li> <p>Rename the file to remove the OS and architecture information. For example, rename <code>kn-quickstart-amd64</code> to <code>kn-quickstart</code>.</p> </li> <li> <p>Make the plugin executable. For example, <code>chmod +x kn-quickstart</code>.</p> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn-quickstart /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the plugin is working by running the command:</p> <pre><code>kn quickstart --help\n</code></pre> </li> </ol> <ol> <li> <p>Check out the <code>kn-plugin-quickstart</code> repository:</p> <pre><code>git clone https://github.com/knative-extensions/kn-plugin-quickstart.git\ncd kn-plugin-quickstart/\n</code></pre> </li> <li> <p>Build an executable binary:</p> <pre><code>hack/build.sh\n</code></pre> </li> <li> <p>Move the executable binary file to a directory on your <code>PATH</code> by running the command:</p> <pre><code>mv kn-quickstart /usr/local/bin\n</code></pre> </li> <li> <p>Verify that the plugin is working by running the command:</p> <pre><code>kn quickstart --help\n</code></pre> </li> </ol>"},{"location":"install/quickstart-install/#run-the-knative-quickstart-plugin","title":"Run the Knative quickstart plugin","text":"<p>The <code>quickstart</code> plugin completes the following functions:</p> <ol> <li>Checks if you have the selected Kubernetes instance installed</li> <li>Creates a cluster called <code>knative</code></li> <li>Installs Knative Serving with Kourier as the default networking layer, and sslip.io as the DNS</li> <li>Installs Knative Eventing and creates an in-memory Broker and Channel implementation</li> </ol> <p>To get a local deployment of Knative, run the <code>quickstart</code> plugin:</p> Using kindUsing minikube <ol> <li> <p>Install Knative and Kubernetes using kind by running:     <pre><code>kn quickstart kind\n</code></pre></p> <p>Note</p> <p>Quickstart uses Port 80, and it will fail to install if any other services are bound on that port. If you have a service using Port 80, you'll need to stop it before using Quickstart. To check if another service is using Port 80: <pre><code>netstat -tnlp | grep 80\n</code></pre></p> </li> <li> <p>After the plugin is finished, verify you have a cluster called <code>knative</code>:</p> <pre><code>kind get clusters\n</code></pre> </li> </ol> <ol> <li> <p>Install Knative and Kubernetes in a minikube instance by running:</p> <p>Note</p> <p>The minikube cluster will be created with 3\u00a0GB of RAM. You can change to a different value not lower than 3\u00a0GB by setting a memory config in minikube. For example,  <code>minikube config set memory 4096</code> will use 4\u00a0GB of RAM.</p> <pre><code>kn quickstart minikube\n</code></pre> </li> <li> <p>The output of the previous command asked you to run minikube tunnel.    Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue:     <pre><code>minikube tunnel --profile knative\n</code></pre>     The tunnel must continue to run in a terminal window any time you are using your Knative <code>quickstart</code> environment.</p> <p>The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer.</p> <p>Note</p> <p>To terminate the tunnel process and clean up network routes, enter <code>Ctrl-C</code>. For more information about the <code>minikube tunnel</code> command, see the minikube documentation.</p> </li> <li> <p>After the plugin is finished, verify you have a cluster called <code>knative</code>:</p> <pre><code>minikube profile list\n</code></pre> </li> </ol>"},{"location":"install/quickstart-install/#next-steps","title":"Next steps","text":"<ul> <li>Learn how to deploy your first Service in the Knative tutorial.</li> <li>Try out Knative code samples.</li> <li>See the Knative Serving and Knative Eventing guides.</li> </ul>"},{"location":"install/troubleshooting/","title":"Troubleshooting a Knative Installation","text":"<p>This page describes how you can troubleshoot a Knative installation. Please try these instructions before filing a bug report.</p>"},{"location":"install/troubleshooting/#check-if-all-containers-are-running-ready-and-healthy","title":"Check if all containers are running, ready and healthy","text":"<p>Note</p> <p>The outputs below are just examples. The containers may vary depending on the installation and configuration. Please make sure all containers are running, ready and healthy (look for the <code>1/1</code> in the following examples) or are <code>Complete</code> (in case of jobs).</p> <p>Knative Serving Components</p> <pre><code>kubectl get pods -n knative-serving\n\nNAME                                      READY   STATUS    RESTARTS   AGE\nactivator-6b9dc4c9db-cl56b                1/1     Running   0          2m\nautoscaler-77f9b75856-f88qw               1/1     Running   0          2m\ncontroller-7dcb56fdb6-dbzrp               1/1     Running   0          2m\ndomain-mapping-6bb8f95654-c575d           1/1     Running   0          2m\ndomainmapping-webhook-c77dcfcfb-hg2wv     1/1     Running   0          2m\nwebhook-78dc6ddddb-6868n                  1/1     Running   0          2m\n</code></pre> <p>Knative Serving Networking Layer</p> KourierIstioContour <pre><code>kubectl get pods -n knative-serving\n\nNAME                                      READY   STATUS    RESTARTS   AGE\nnet-kourier-controller-5fcbb6d996-fprpd   1/1     Running   0          103s\n</code></pre> <pre><code>kubectl get pods -n kourier-system\nNAME                                      READY   STATUS    RESTARTS   AGE\n3scale-kourier-gateway-86b9f6dc44-xpn6h   1/1     Running   0          2m22s\n</code></pre> <p><pre><code>kubectl get pods -n knative-serving\n\nNAME                                    READY   STATUS    RESTARTS   AGE\nnet-istio-controller-ccc455b58-f98ld    1/1     Running   0          19s\nnet-istio-webhook-7558dbfc64-5jmt6      1/1     Running   0          19s\n</code></pre> <pre><code>kubectl get pods -n istio-system\n\nNAME                                   READY   STATUS    RESTARTS   AGE\nistio-ingressgateway-c7b9f6477-bgr6q   1/1     Running   0          44s\nistiod-79d65bf5f4-5zvtj                1/1     Running   0          29s\n</code></pre></p> <p><pre><code>kubectl get pods -n knative-serving\n\nNAME                                      READY   STATUS        RESTARTS   AGE\nnet-contour-controller-68547b797c-dl8pf   1/1     Running       0          14s\n</code></pre> <pre><code>kubectl get pods -n contour-external\n\nNAME                            READY   STATUS      RESTARTS   AGE\ncontour-7b995cdb68-jg5s8        1/1     Running     0          41s\ncontour-certgen-v1.24.2-zmr9r   0/1     Completed   0          41s\nenvoy-xkzck                     2/2     Running     0          41s\n</code></pre> <pre><code>kubectl get pods -n contour-internal\n\nNAME                            READY   STATUS      RESTARTS   AGE\ncontour-57fcf576fd-wb57c        1/1     Running     0          55s\ncontour-certgen-v1.24.2-gqgrx   0/1     Completed   0          55s\nenvoy-rht69                     2/2     Running     0          55s\n</code></pre></p> <p>Knative Eventing</p> <pre><code>kubectl get pods -n knative-eventing\n\nNAME                                  READY   STATUS    RESTARTS   AGE\neventing-controller-bb8b689c4-lk6pq   1/1     Running   0          41s\neventing-webhook-577bb88ccd-hcz5p     1/1     Running   0          41s\n</code></pre>"},{"location":"install/troubleshooting/#check-if-there-are-any-errors-logged-in-the-knative-components","title":"Check if there are any errors logged in the Knative components","text":"<pre><code>kubectl logs -n knative-serving &lt;pod-name&gt;\nkubectl logs -n knative-eventing &lt;pod-name&gt;\nkubectl logs -n &lt;ingress-namespaces&gt; &lt;pod-namespaces&gt; # see above for the relevant namespaces\n\n# For example\nkubectl logs -n knative-serving activator-6b9dc4c9db-cl56b\n2023/05/01 11:52:51 Registering 3 clients\n2023/05/01 11:52:51 Registering 3 informer factories\n2023/05/01 11:52:51 Registering 4 informers\n</code></pre>"},{"location":"install/troubleshooting/#check-the-status-of-the-knative-resources","title":"Check the status of the Knative Resources","text":"<p>Knative Serving <pre><code>kubectl describe -n &lt;namespace&gt; kservice\nkubectl describe -n &lt;namespace&gt; config\nkubectl describe -n &lt;namespace&gt; revision\nkubectl describe -n &lt;namespace&gt; sks # Serverless Service\nkubectl describe -n &lt;namespace&gt; kingress # Knative Ingress\nkubectl describe -n &lt;namespace&gt; rt # Knative Route\nkubectl describe -n &lt;namespace&gt; dm # Domain-Mapping\n\n# Check the status at the end. For example\nkubectl describe -n default kservice\n\n... omitted ...\nStatus:\n  Address:\n    URL:  http://hello.default.svc.cluster.local\n  Conditions:\n    Last Transition Time:        2023-05-01T12:08:18Z\n    Status:                      True\n    Type:                        ConfigurationsReady\n    Last Transition Time:        2023-05-01T12:08:18Z\n    Status:                      True\n    Type:                        Ready\n    Last Transition Time:        2023-05-01T12:08:18Z\n    Status:                      True\n    Type:                        RoutesReady\n  Latest Created Revision Name:  hello-00001\n  Latest Ready Revision Name:    hello-00001\n  Observed Generation:           1\n  Traffic:\n    Latest Revision:  true\n    Percent:          100\n    Revision Name:    hello-00001\n  URL:                http://hello.default.10.89.0.200.sslip.io\nEvents:\n  Type    Reason   Age   From                Message\n  ----    ------   ----  ----                -------\n  Normal  Created  45s   service-controller  Created Configuration \"hello\"\n  Normal  Created  45s   service-controller  Created Route \"hello\"\n</code></pre></p> <p>Knative Eventing</p> <pre><code>kubectl describe -n &lt;namespace&gt; brokers\nkubectl describe -n &lt;namespace&gt; eventtypes\nkubectl describe -n &lt;namespace&gt; triggers\nkubectl describe -n &lt;namespace&gt; channels\nkubectl describe -n &lt;namespace&gt; subscriptions\nkubectl describe -n &lt;namespace&gt; apiserversources\nkubectl describe -n &lt;namespace&gt; containersources\nkubectl describe -n &lt;namespace&gt; pingsources\nkubectl describe -n &lt;namespace&gt; sinkbindings\n\n\n# Check the status at the end. For example\nkubectl describe -n default brokers\n\n... omitted ...\nStatus:\n  Annotations:\n    bootstrap.servers:                 my-cluster-kafka-bootstrap.kafka:9092\n    default.topic.partitions:          10\n    default.topic.replication.factor:  3\n</code></pre>"},{"location":"install/uninstall/","title":"Uninstalling Knative","text":"<p>To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure.</p>"},{"location":"install/uninstall/#uninstalling-a-yaml-based-knative-installation","title":"Uninstalling a YAML-based Knative installation","text":"<p>To uninstall a YAML-based Knative installation:</p>"},{"location":"install/uninstall/#uninstalling-optional-serving-extensions","title":"Uninstalling optional Serving extensions","text":"<p>Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab:</p> HPA autoscalingTLS with cert-manager <p>Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml\n</code></pre> <ol> <li> <p>Uninstall the component that integrates Knative with cert-manager:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml\n</code></pre> </li> <li> <p>Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation.</p> </li> </ol>"},{"location":"install/uninstall/#uninstalling-a-networking-layer","title":"Uninstalling a networking layer","text":"<p>Follow the relevant procedure to uninstall the networking layer you installed:</p> ContourIstioKourier <p>The following commands uninstall Contour and enable its Knative integration.</p> <ol> <li> <p>Uninstall the Knative Contour controller by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml\n</code></pre> </li> <li> <p>Uninstall Contour:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml\n</code></pre> </li> </ol> <p>The following commands uninstall Istio and enable its Knative integration.</p> <ol> <li> <p>Uninstall the Knative Istio controller by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml\n</code></pre> </li> <li> <p>Optional: if you no longer need Istio, uninstall it by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml\n</code></pre> </li> </ol> <p>Uninstall the Knative Kourier controller by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml\n</code></pre>"},{"location":"install/uninstall/#uninstalling-the-serving-component","title":"Uninstalling the Serving component","text":"<ol> <li> <p>Uninstall the Serving core components by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml\n</code></pre> </li> <li> <p>Uninstall the required custom resources by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml\n</code></pre> </li> </ol>"},{"location":"install/uninstall/#uninstalling-optional-eventing-extensions","title":"Uninstalling optional Eventing extensions","text":"<p>Uninstall any Eventing extensions you have installed by following the relevant procedure:</p> Apache Kafka SinkGitHub SourceApache Kafka SourceGCP SourcesApache CouchDB SourceVMware Sources and Bindings <ol> <li> <p>Uninstall the Kafka Sink data plane:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml\n</code></pre> </li> <li> <p>Uninstall the Kafka controller:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> </ol> <p>Uninstall a single-tenant GitHub source by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml\n</code></pre> <p>Uninstall a multi-tenant GitHub source by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml\n</code></pre> <p>Uninstall the Apache Kafka source by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml\n</code></pre> <p>Uninstall the GCP sources by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml\n</code></pre> <p>Uninstall the Apache CouchDB source by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml\n</code></pre> <p>Uninstall the VMware sources and bindings by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml\n</code></pre>"},{"location":"install/uninstall/#uninstalling-an-optional-broker-eventing-layer","title":"Uninstalling an optional Broker (Eventing) layer","text":"<p>Uninstall a Broker (Eventing) layer, if you installed one:</p> Apache Kafka BrokerMT-Channel-based <ol> <li> <p>Uninstall the Kafka Broker data plane by running the following command:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml\n</code></pre> </li> <li> <p>Uninstall the Kafka controller by running the following command:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> </ol> <p>Uninstall the broker by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml\n</code></pre>"},{"location":"install/uninstall/#uninstalling-optional-channel-messaging-layers","title":"Uninstalling optional channel (messaging) layers","text":"<p>Uninstall each channel layer you have installed:</p> Apache Kafka ChannelGoogle Cloud Pub/Sub ChannelIn-Memory (standalone)NATS Channel <p>Uninstall the Apache Kafka Channel by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml\n</code></pre> <p>Uninstall the Google Cloud Pub/Sub Channel by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml\n</code></pre> <p>Uninstall the in-memory channel implementation by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml\n</code></pre> <ol> <li> <p>Uninstall the NATS Streaming channel by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml\n</code></pre> </li> <li> <p>Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub.</p> </li> </ol>"},{"location":"install/uninstall/#uninstalling-the-eventing-component","title":"Uninstalling the Eventing component","text":"<ol> <li> <p>Uninstall the Eventing core components by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml\n</code></pre> </li> <li> <p>Uninstall the required custom resources by running:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml\n</code></pre> </li> </ol>"},{"location":"install/uninstall/#uninstall-an-operator-based-knative-installation","title":"Uninstall an Operator-based Knative installation","text":"<p>To uninstall an Operator-based Knative installation, follow these procedures:</p>"},{"location":"install/uninstall/#removing-the-knative-serving-component","title":"Removing the Knative Serving component","text":"<p>Remove the Knative Serving CR:</p> <pre><code>kubectl delete KnativeServing knative-serving -n knative-serving\n</code></pre>"},{"location":"install/uninstall/#removing-knative-eventing-component","title":"Removing Knative Eventing component","text":"<p>Remove the Knative Eventing CR:</p> <pre><code>kubectl delete KnativeEventing knative-eventing -n knative-eventing\n</code></pre> <p>Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.</p>"},{"location":"install/uninstall/#removing-the-knative-operator","title":"Removing the Knative Operator:","text":"<p>If you have installed Knative using the Release page, remove the operator using the following command:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml\n</code></pre> <p>If you have installed Knative from source, uninstall it using the following command while in the root directory for the source:</p> <pre><code>ko delete -f config/\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/","title":"Configuring the Eventing Operator custom resource","text":"<p>You can configure the Knative Eventing operator by modifying settings in the <code>KnativeEventing</code> custom resource (CR).</p>"},{"location":"install/operator/configuring-eventing-cr/#setting-a-default-channel","title":"Setting a default channel","text":"<p>If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the <code>default-ch-webhook</code> ConfigMap.</p> <p>You can do this by modifying the KnativeEventing CR:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  config:\n    default-ch-webhook:\n      default-ch-config: |\n        clusterDefault:\n          apiVersion: messaging.knative.dev/v1beta1\n          kind: KafkaChannel\n          spec:\n            numPartitions: 10\n            replicationFactor: 1\n        namespaceDefaults:\n          my-namespace:\n            apiVersion: messaging.knative.dev/v1\n            kind: InMemoryChannel\n            spec:\n              delivery:\n                backoffDelay: PT0.5S\n                backoffPolicy: exponential\n                retry: 5\n</code></pre> <p>Note</p> <p>The <code>clusterDefault</code> setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the <code>namespaceDefaults</code> setting.</p>"},{"location":"install/operator/configuring-eventing-cr/#setting-the-default-channel-for-the-broker","title":"Setting the default channel for the broker","text":"<p>If you are using a Channel based Broker, you can change the default Channel type for the Broker from InMemoryChannel to KafkaChannel, by updating the <code>config-br-default-channel</code> ConfigMap.</p> <p>You can do this by modifying the KnativeEventing CR:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  config:\n    config-br-default-channel:\n      channel-template-spec: |\n        apiVersion: messaging.knative.dev/v1beta1\n        kind: KafkaChannel\n        spec:\n          numPartitions: 6\n          replicationFactor: 1\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#private-repository-and-private-secrets","title":"Private repository and private secrets","text":"<p>The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret.</p> <p>Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving.</p> <p>List of containers within each Deployment resource:</p> Component Deployment name Container name Core eventing <code>eventing-controller</code> <code>eventing-controller</code> Core eventing <code>eventing-webhook</code> <code>eventing-webhook</code> Eventing Broker <code>broker-controller</code> <code>eventing-controller</code> In-Memory Channel <code>imc-controller</code> <code>controller</code> In-Memory Channel <code>imc-dispatcher</code> <code>dispatcher</code> <p>The <code>default</code> field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example <code>eventing-controller</code>, you must use the <code>override</code> field to replace it, by specifying <code>deployment/container</code> as the unique key.</p> <p>Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the <code>override</code> field.</p>"},{"location":"install/operator/configuring-eventing-cr/#download-images-in-a-predefined-format-without-secrets","title":"Download images in a predefined format without secrets","text":"<p>This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format <code>docker.io/knative-images/${NAME}:{CUSTOM-TAG}</code>.</p> <p>In this example:</p> <ul> <li>The custom tag <code>latest</code> is used for all images.</li> <li>All image links are accessible without using secrets.</li> <li>Images are defined in the accepted format <code>docker.io/knative-images/${NAME}:{CUSTOM-TAG}</code>.</li> </ul> <p>To define your image links:</p> <ol> <li> <p>Push images to the following image tags:</p> Deployment Container Docker image <code>eventing-controller</code> <code>eventing-controller</code> <code>docker.io/knative-images/eventing-controller:latest</code> <code>eventing-webhook</code> <code>docker.io/knative-images/eventing-webhook:latest</code> <code>broker-controller</code> <code>eventing-controller</code> <code>docker.io/knative-images/broker-eventing-controller:latest</code> <code>controller</code> <code>docker.io/knative-images/controller:latest</code> <code>dispatcher</code> <code>docker.io/knative-images/dispatcher:latest</code> </li> <li> <p>Define your KnativeEventing CR with following content:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  registry:\n    default: docker.io/knative-images/${NAME}:latest\n    override:\n      broker-controller/eventing-controller: docker.io/knative-images-repo1/broker-eventing-controller:latest\n</code></pre> </li> </ol> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>```bash\nkn operator configure images --component eventing --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-eventing\nkn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/broker-eventing-controller:latest -n knative-eventing\n```\n\n- `${NAME}` maps to the container name in each `Deployment` resource.\n- `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override`\nfield to specify individually, by using `broker-controller/eventing-controller` as the key.\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#download-images-from-different-repositories-without-secrets","title":"Download images from different repositories without secrets","text":"<p>If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR.</p> <p>For example, given the following list of images:</p> Deployment Container Docker Image <code>eventing-controller</code> <code>eventing-controller</code> <code>docker.io/knative-images-repo1/eventing-controller:latest</code> <code>eventing-webhook</code> <code>docker.io/knative-images-repo2/eventing-webhook:latest</code> <code>controller</code> <code>docker.io/knative-images-repo3/imc-controller:latest</code> <code>dispatcher</code> <code>docker.io/knative-images-repo4/imc-dispatcher:latest</code> <code>broker-controller</code> <code>eventing-controller</code> <code>docker.io/knative-images-repo5/broker-eventing-controller:latest</code> <p>You must modify the KnativeEventing CR to include the full list. For example:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  registry:\n    override:\n      eventing-controller/eventing-controller: docker.io/knative-images-repo1/eventing-controller:latest\n      eventing-webhook/eventing-webhook: docker.io/knative-images-repo2/eventing-webhook:latest\n      imc-controller/controller: docker.io/knative-images-repo3/imc-controller:latest\n      imc-dispatcher/dispatcher: docker.io/knative-images-repo4/imc-dispatcher:latest\n      broker-controller/eventing-controller: docker.io/knative-images-repo5/broker-eventing-controller:latest\n</code></pre> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing\n</code></pre> <p>If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable <code>DISPATCHER_IMAGE</code>, in the container <code>controller</code>, of the deployment <code>imc-controller</code>, and the target image is <code>docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest</code>, the KnativeEventing CR would be as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  registry:\n    override:\n      eventing-controller/eventing-controller: docker.io/knative-images-repo1/eventing-controller:latest\n      eventing-webhook/eventing-webhook: docker.io/knative-images-repo2/eventing-webhook:latest\n      imc-controller/controller: docker.io/knative-images-repo3/imc-controller:latest\n      imc-dispatcher/dispatcher: docker.io/knative-images-repo4/imc-dispatcher:latest\n      broker-controller/eventing-controller: docker.io/knative-images-repo5/broker-eventing-controller:latest\n      DISPATCHER_IMAGE: docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest\n</code></pre> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>kn operator configure images --component eventing --deployName eventing-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo1/eventing-controller:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName eventing-webhook --imageKey eventing-webhook --imageURL docker.io/knative-images-repo2/eventing-webhook:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName imc-controller --imageKey controller --imageURL docker.io/knative-images-repo3/imc-controller:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName imc-dispatcher --imageKey dispatcher --imageURL docker.io/knative-images-repo4/imc-dispatcher:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --deployName broker-controller --imageKey eventing-controller --imageURL docker.io/knative-images-repo5/broker-eventing-controller:latest -n knative-eventing\n</code></pre> <pre><code>kn operator configure images --component eventing --imageKey DISPATCHER_IMAGE -controller --imageURL docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest -n knative-eventing\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#download-images-with-secrets","title":"Download images with secrets","text":"<p>If your image repository requires private secrets for access, you must append the <code>imagePullSecrets</code> attribute to the KnativeEventing CR.</p> <p>This example uses a secret named <code>regcred</code>. Refer to the Kubernetes documentation to create your own private secrets.</p> <p>After you create the secret, edit the KnativeEventing CR:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  registry:\n    ...\n    imagePullSecrets:\n      - name: regcred\n</code></pre> <p>The field <code>imagePullSecrets</code> requires a list of secrets. You can add multiple secrets to access the images:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  registry:\n    ...\n    imagePullSecrets:\n      - name: regcred\n      - name: regcred-2\n      ...\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-default-broker-class","title":"Configuring the default broker class","text":"<p>Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker.</p> <p>The field <code>defaultBrokerClass</code> indicates which class to use; if empty, the ChannelBasedBroker is used.</p> <p>The following example CR specifies MTChannelBasedBroker as the default:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  defaultBrokerClass: MTChannelBasedBroker\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-system-deployments","title":"Override system deployments","text":"<p>If you would like to override some configurations for a specific deployment, you can override the configuration by using <code>spec.deployments</code> in the CR. Currently <code>resources</code>, <code>replicas</code>, <code>labels</code>, <code>annotations</code> and <code>nodeSelector</code> are supported.</p>"},{"location":"install/operator/configuring-eventing-cr/#override-the-resources","title":"Override the resources","text":"<p>The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like <code>eventing-controller</code>, <code>eventing-webhook</code>, <code>imc-controller</code>, etc.</p> <p>For example, the following KnativeEventing resource configures the container <code>eventing-controller</code> in the deployment <code>eventing-controller</code> to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  workloads:\n  - name: eventing-controller\n    resources:\n    - container: eventing-controller\n      requests:\n        cpu: 300m\n        memory: 100M\n      limits:\n        cpu: 1000m\n        memory: 250M\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure resources --component eventing --deployName eventing-controller --container eventing-controller --requestCPU 300m --requestMemory 100M --limitCPU 1000m --limitMemory 250M -n knative-eventing\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-the-nodeselector","title":"Override the nodeSelector","text":"<p>The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations</p> <pre><code>nodeSelector:\n  disktype: hdd\n</code></pre> <p>to the deployment <code>eventing-controller</code>, you need to change your KnativeEventing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  workloads:\n  - name: eventing-controller\n    nodeSelector:\n      disktype: hdd\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure nodeSelector --component eventing --deployName eventing-controller --key disktype --value hdd -n knative-eventing\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-the-tolerations","title":"Override the tolerations","text":"<p>The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations</p> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n</code></pre> <p>to the deployment <code>eventing-controller</code>, you need to change your KnativeEventing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  workloads:\n  - name: eventing-controller\n    tolerations:\n    - key: \"key1\"\n      operator: \"Equal\"\n      value: \"value1\"\n      effect: \"NoSchedule\"\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure tolerations --component eventing --deployName eventing-controller --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-eventing\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-the-affinity","title":"Override the affinity","text":"<p>The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity</p> <pre><code>affinity:\n  nodeAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 1\n      preference:\n        matchExpressions:\n        - key: disktype\n          operator: In\n          values:\n          - ssd\n</code></pre> <p>to the deployment <code>eventing-controller</code>, you need to change your KnativeEventing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  workloads:\n  - name: eventing-controller\n    affinity:\n      nodeAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 1\n          preference:\n            matchExpressions:\n            - key: disktype\n              operator: In\n              values:\n              - ssd\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-the-environment-variables","title":"Override the environment variables","text":"<p>The KnativeEventing resource is able to override or add the environment variables for the containers in the Knative Eventing deployment resources. For example, if you would like to change the value of environment variable <code>METRICS_DOMAIN</code> in the container <code>eventing-controller</code> into \"knative.dev/my-repo\" for the deployment <code>eventing-controller</code>, you need to change your KnativeEventing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  workloads:\n  - name: eventing-controller\n    env:\n    - container: eventing-controller\n      envVars:\n      - name: METRICS_DOMAIN\n        value: \"knative.dev/my-repo\"\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure envvars --component eventing --deployName eventing-controller --container eventing-controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-eventing\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-system-services","title":"Override system services","text":"<p>If you would like to override some configurations for a specific service, you can override the configuration by using <code>spec.services</code> in CR. Currently <code>labels</code>, <code>annotations</code> and <code>selector</code> are supported.</p>"},{"location":"install/operator/configuring-eventing-cr/#override-labels-and-annotations-and-selector","title":"Override labels and annotations and selector","text":"<p>The following KnativeEventing resource overrides the <code>eventing-webhook</code> service to have the label <code>mylabel: foo</code>, the annotation <code>myannotations: bar</code>, the selector <code>myselector: bar</code>.</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  services:\n  - name: eventing-webhook\n    labels:\n      mylabel: foo\n    annotations:\n      myannotations: bar\n    selector:\n      myselector: bar\n</code></pre> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>kn operator configure labels --component eventing --serviceName eventing-webhook --key mylabel --value foo -n knative-eventing\nkn operator configure annotations --component eventing --serviceName eventing-webhook --key myannotations --value bar -n knative-eventing\nkn operator configure selectors --component eventing --serviceName eventing-webhook --key myselector --value bar -n knative-eventing\n</code></pre>"},{"location":"install/operator/configuring-eventing-cr/#override-system-poddisruptionbudgets","title":"Override system podDisruptionBudgets","text":"<p>A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the <code>minAvailable</code> for a specific podDisruptionBudget resource in Eventing based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here. For example, if you would like to change <code>minAvailable</code> into 70% for the podDisruptionBudget named <code>eventing-webhook</code>, you need to change your KnativeEventing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  podDisruptionBudgets:\n  - name: eventing-webhook\n    minAvailable: 70%\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/","title":"Configuring the Knative Serving Operator custom resource","text":"<p>You can modify the KnativeServing CR to configure different options for Knative Serving.</p>"},{"location":"install/operator/configuring-serving-cr/#configure-the-knative-serving-version","title":"Configure the Knative Serving version","text":"<p>Cluster administrators can install a specific version of Knative Serving by using the <code>spec.version</code> field.</p> <p>For example, if you want to install Knative Serving v1.5, you can apply the following <code>KnativeServing</code> custom resource:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  version: \"1.5\"\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator install --component serving -v 1.5 -n knative-serving\n</code></pre> <p>If <code>spec.version</code> is not specified, the Knative Operator installs the latest available version of Knative Serving.</p> <p>If users specify an invalid or unavailable version, the Knative Operator does nothing.</p> <p>The Knative Operator always includes the latest 3 release versions. For example, if the current version of the Knative Operator is v1.5, the earliest version of Knative Serving available through the Operator is v1.2.</p> <p>If Knative Serving is already managed by the Operator, updating the <code>spec.version</code> field in the <code>KnativeServing</code> resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator.</p> <p>Important</p> <p>The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v1.3, you must upgrade to v1.4 before upgrading to v1.5.</p>"},{"location":"install/operator/configuring-serving-cr/#install-customized-knative-serving","title":"Install customized Knative Serving","text":"<p>There are two modes available that you can use to install customized Knative Serving manifests: overwrite mode and append mode.</p> <p>If you are using overwrite mode, under <code>.spec.manifests</code>, you must define all required manifests to install Knative Serving, because the Operator does not install any default manifests.</p> <p>If you are using append mode, under <code>.spec.additionalManifests</code>, you only need to define your customized manifests. The customized manifests are installed after default manifests are applied.</p>"},{"location":"install/operator/configuring-serving-cr/#overwrite-mode","title":"Overwrite mode","text":"<p>You can use overwrite mode when you want to customize all Knative Serving manifests.</p> <p>Important</p> <p>You must specify both the version and valid URLs for your custom Knative Serving manifests.</p> <p>For example, if you want to install customized versions of both Knative Serving and the Istio ingress, you can create a <code>KnativeServing</code> CR similar to the following example:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  version: $spec_version\n  manifests:\n  - URL: https://my-serving/serving.yaml\n  - URL: https://my-net-istio/net-istio.yaml\n</code></pre> <p>You can make the customized Knative Serving available in one or multiple links, as the <code>spec.manifests</code> supports a list of links.</p> <p>Important</p> <p>The ordering of manifest URLs is critical. Put the manifest you want to apply first at the top of the list.</p> <p>This example installs the customized Knative Serving at version <code>$spec_version</code> which is available at <code>https://my-serving/serving.yaml</code>, and the customized ingress plugin <code>net-istio</code> which is available at <code>https://my-net-istio/net-istio.yaml</code>.</p>"},{"location":"install/operator/configuring-serving-cr/#append-mode","title":"Append mode","text":"<p>You can use append mode to add your customized Knative Serving manifests in addition to the default manifests.</p> <p>For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create a <code>KnativeServing</code> CR similar to the following example:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  version: $spec_version\n  additionalManifests:\n  - URL: https://my-serving/serving-custom.yaml\n</code></pre> <p>This example installs the default Knative Serving manifests, and then installs the customized resources available at <code>https://my-serving/serving-custom.yaml</code> for the version <code>$spec_version</code>.</p>"},{"location":"install/operator/configuring-serving-cr/#private-repository-and-private-secrets","title":"Private repository and private secrets","text":"<p>You can use the <code>spec.registry</code> section of the Operator CR to change the image references to point to a private registry or specify imagePullSecrets:</p> <ul> <li> <p><code>default</code>: this field defines a image reference template for all Knative images. The format is <code>example-registry.io/custom/path/${NAME}:{CUSTOM-TAG}</code>. If you use the same tag for all your images, the only difference is the image name. <code>${NAME}</code> is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( <code>activator</code>, <code>autoscaler</code>, <code>controller</code>, <code>webhook</code>, <code>autoscaler-hpa</code>, <code>net-istio-controller</code>, and <code>queue-proxy</code>), the <code>default</code> argument should be sufficient.</p> </li> <li> <p><code>override</code>: a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by <code>default</code>. If a container's name does not match a key in <code>override</code>, the template in <code>default</code> is used.</p> </li> <li> <p><code>imagePullSecrets</code>: a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details.</p> </li> </ul>"},{"location":"install/operator/configuring-serving-cr/#download-images-in-a-predefined-format-without-secrets","title":"Download images in a predefined format without secrets","text":"<p>This example shows how you can define custom image links that can be defined in the CR using the simplified format <code>docker.io/knative-images/${NAME}:{CUSTOM-TAG}</code>.</p> <p>In the following example:</p> <ul> <li>The custom tag <code>latest</code> is used for all images.</li> <li>All image links are accessible without using secrets.</li> <li>Images are pushed as <code>docker.io/knative-images/${NAME}:{CUSTOM-TAG}</code>.</li> </ul> <p>To define your image links:</p> <ol> <li> <p>Push images to the following image tags:</p> Container Docker Image <code>activator</code> <code>docker.io/knative-images/activator:latest</code> <code>autoscaler</code> <code>docker.io/knative-images/autoscaler:latest</code> <code>controller</code> <code>docker.io/knative-images/controller:latest</code> <code>webhook</code> <code>docker.io/knative-images/webhook:latest</code> <code>autoscaler-hpa</code> <code>docker.io/knative-images/autoscaler-hpa:latest</code> <code>net-istio-controller</code> <code>docker.io/knative-images/net-istio-controller:latest</code> <code>queue-proxy</code> <code>docker.io/knative-images/queue-proxy:latest</code> </li> <li> <p>Define your operator CR with following content:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  registry:\n    default: docker.io/knative-images/${NAME}:latest\n</code></pre> </li> </ol> <p>You can also run the following command to make the equivalent change:</p> <pre><code>```bash\nkn operator configure images --component serving --imageKey default --imageURL docker.io/knative-images/${NAME}:latest -n knative-serving\n```\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#download-images-individually-without-secrets","title":"Download images individually without secrets","text":"<p>If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR.</p> <p>For example, given the following images:</p> Container Docker Image <code>activator</code> <code>docker.io/knative-images-repo1/activator:latest</code> <code>autoscaler</code> <code>docker.io/knative-images-repo2/autoscaler:latest</code> <code>controller</code> <code>docker.io/knative-images-repo3/controller:latest</code> <code>webhook</code> <code>docker.io/knative-images-repo4/webhook:latest</code> <code>autoscaler-hpa</code> <code>docker.io/knative-images-repo5/autoscaler-hpa:latest</code> <code>net-istio-controller</code> <code>docker.io/knative-images-repo6/prefix-net-istio-controller:latest</code> <code>net-istio-webhook</code> <code>docker.io/knative-images-repo6/net-istio-webhooko:latest</code> <code>queue-proxy</code> <code>docker.io/knative-images-repo7/queue-proxy-suffix:latest</code> <p>You must modify the Operator CR to include the full list. For example:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  registry:\n    override:\n      activator: docker.io/knative-images-repo1/activator:latest\n      autoscaler: docker.io/knative-images-repo2/autoscaler:latest\n      controller: docker.io/knative-images-repo3/controller:latest\n      webhook: docker.io/knative-images-repo4/webhook:latest\n      autoscaler-hpa: docker.io/knative-images-repo5/autoscaler-hpa:latest\n      net-istio-controller/controller: docker.io/knative-images-repo6/prefix-net-istio-controller:latest\n      net-istio-webhook/webhook: docker.io/knative-images-repo6/net-istio-webhook:latest\n      queue-proxy: docker.io/knative-images-repo7/queue-proxy-suffix:latest\n</code></pre> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>kn operator configure images --component serving --imageKey activator --imageURL docker.io/knative-images-repo1/activator:latest -n knative-serving\nkn operator configure images --component serving --imageKey autoscaler --imageURL docker.io/knative-images-repo2/autoscaler:latest -n knative-serving\nkn operator configure images --component serving --imageKey controller --imageURL docker.io/knative-images-repo3/controller:latest -n knative-serving\nkn operator configure images --component serving --imageKey webhook --imageURL docker.io/knative-images-repo4/webhook:latest -n knative-serving\nkn operator configure images --component serving --imageKey autoscaler-hpa --imageURL docker.io/knative-images-repo5/autoscaler-hpa:latest -n knative-serving\nkn operator configure images --component serving --deployName net-istio-controller --imageKey controller --imageURL docker.io/knative-images-repo6/prefix-net-istio-controller:latest -n knative-serving\nkn operator configure images --component serving --deployName net-istio-webhook --imageKey webhook --imageURL docker.io/knative-images-repo6/net-istio-webhook:latest -n knative-serving\nkn operator configure images --component serving --imageKey queue-proxy --imageURL docker.io/knative-images-repo7/queue-proxy-suffix:latest -n knative-serving\n</code></pre> <p>Note</p> <p>If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, <code>istio-webhook/webhook</code>.</p>"},{"location":"install/operator/configuring-serving-cr/#download-images-with-secrets","title":"Download images with secrets","text":"<p>If your image repository requires private secrets for access, include the <code>imagePullSecrets</code> attribute.</p> <p>This example uses a secret named <code>regcred</code>. You must create your own private secrets if these are required:</p> <ul> <li>From existing docker credentials</li> <li>From command line for docker credentials</li> <li>Create your own secret</li> </ul> <p>After you create this secret, edit the Operator CR by appending the following content:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  registry:\n    ...\n    imagePullSecrets:\n      - name: regcred\n</code></pre> <p>The field <code>imagePullSecrets</code> expects a list of secrets. You can add multiple secrets to access the images as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  registry:\n    ...\n    imagePullSecrets:\n      - name: regcred\n      - name: regcred-2\n      ...\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#ssl-certificate-for-controller","title":"SSL certificate for controller","text":"<p>To enable tag to digest resolution, the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret.</p> <p>Specify the following fields in <code>spec.controller-custom-certs</code> to select a custom registry certificate:</p> <ul> <li><code>name</code>: the name of the ConfigMap or Secret.</li> <li><code>type</code>: either the string \"ConfigMap\" or \"Secret\".</li> </ul> <p>If you create a ConfigMap named <code>testCert</code> containing the certificate, change your CR:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  controller-custom-certs:\n    name: testCert\n    type: ConfigMap\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#replace-the-default-istio-ingress-gateway-service","title":"Replace the default Istio ingress gateway service","text":"<ol> <li> <p>Create a gateway Service and Deployment instance.</p> </li> <li> <p>Update the Knative gateway by updating the <code>ingress.istio.knative-ingress-gateway</code> spec to select the labels of the new ingress gateway:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  ingress:\n    istio:\n      enabled: true\n      knative-ingress-gateway:\n        selector:\n          istio: ingressgateway\n</code></pre> </li> <li> <p>Update the Istio ingress gateway ConfigMap:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  ingress:\n    istio:\n      enabled: true\n      knative-ingress-gateway:\n        selector:\n          istio: ingressgateway\n  config:\n    istio:\n      external-gateways: |\n        - name: knative-ingress-gateway\n          namespace: knative-serving\n          service: custom-ingressgateway.custom-ns.svc.cluster.local\n</code></pre> <p>The key in <code>spec.config.istio</code> is in the format of <pre><code>external-gateways: |\n  - name: &lt;gateway_name&gt;\n    namespace: &lt;gateway_namespace&gt;\n    service: istio-ingressgateway.istio-system.svc.cluster.local\n</code></pre></p> </li> </ol>"},{"location":"install/operator/configuring-serving-cr/#replace-the-ingress-gateway","title":"Replace the ingress gateway","text":"<ol> <li> <p>Create a gateway.</p> </li> <li> <p>Update the Istio ingress gateway ConfigMap:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  config:\n    istio:\n      external-gateways: |\n        - name: knative-custom-gateway\n          namespace: custom-ns\n          service: istio-ingressgateway.istio-system.svc.cluster.local\n</code></pre> <p>The key in <code>spec.config.istio</code> is in the format of <pre><code>external-gateways: |\n  - name: &lt;gateway_name&gt;\n    namespace: &lt;gateway_namespace&gt;\n    service: istio-ingressgateway.istio-system.svc.cluster.local\n</code></pre></p> </li> </ol>"},{"location":"install/operator/configuring-serving-cr/#configuration-of-cluster-local-gateway","title":"Configuration of cluster local gateway","text":"<p>Update <code>spec.ingress.istio.knative-local-gateway</code> to select the labels of the new cluster-local ingress gateway:</p>"},{"location":"install/operator/configuring-serving-cr/#default-local-gateway-name","title":"Default local gateway name","text":"<p>Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called <code>knative-local-gateway</code>.</p>"},{"location":"install/operator/configuring-serving-cr/#non-default-local-gateway-name","title":"Non-default local gateway name","text":"<p>If you create custom local gateway with a name other than <code>knative-local-gateway</code>, update <code>config.istio</code> and the <code>knative-local-gateway</code> selector:</p> <p>This example shows a service and deployment <code>knative-local-gateway</code> in the namespace <code>istio-system</code>, with the label <code>custom: custom-local-gw</code>:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  ingress:\n    istio:\n      enabled: true\n      knative-local-gateway:\n        selector:\n          custom: custom-local-gateway\n  config:\n    istio:\n      local-gateways: |\n        - name: knative-local-gateway\n          namespace: knative-serving\n          service: custom-local-gateway.istio-system.svc.cluster.local\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#servers-configuration-for-istio-gateways","title":"Servers configuration for Istio gateways:","text":"<p>You can leverage the KnativeServing CR to configure the hosts and port of the servers stanzas for <code>knative-local-gateway</code> or <code>knative-ingress-gateway</code> gateways. For example, you would like to specify the host into <code>&lt;test-ip&gt;</code> and configure the port with <code>number: 443</code>, <code>name: https</code>, <code>protocol: HTTPS</code>, and <code>target_port: 8443</code> for <code>knative-local-gateway</code>, apply the following yaml content:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  ingress:\n    istio:\n      enabled: true\n      knative-local-gateway:\n        servers:\n        - port:\n            number: 443\n            name: https\n            protocol: HTTPS\n            target_port: 8443\n          hosts:\n          - &lt;test-ip&gt;\n  config:\n    istio:\n      local-gateways: |\n        - name: knative-local-gateway\n          namespace: knative-serving\n          service: custom-local-gateway.istio-system.svc.cluster.local\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#customize-kourier-bootstrap-for-kourier-gateways","title":"Customize kourier-bootstrap for Kourier gateways:","text":"<p>By default, Kourier contains envoy bootstrap configuration in the ConfigMap <code>kourier-bootstrap</code>. The <code>spec.ingress.kourier.bootstrap-configmap</code> field allows you to specify your customized bootstrap ConfigMap.</p> <p>This example shows that Kourier Gateawy uses <code>my-configmap</code> for the envoy bootstrap config.</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  config:\n    network:\n      ingress-class: kourier.ingress.networking.knative.dev\n  ingress:\n    kourier:\n      bootstrap-configmap: my-configmap\n      enabled: true\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#high-availability","title":"High availability","text":"<p>By default, Knative Serving runs a single instance of each deployment. The <code>spec.high-availability</code> field allows you to configure the number of replicas for all deployments managed by the operator.</p> <p>The following configuration specifies a replica count of 3 for the workloads:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  high-availability:\n    replicas: 3\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure replicas --component serving --replicas 3 -n knative-serving\n</code></pre> <p>The <code>replicas</code> field also configures the <code>HorizontalPodAutoscaler</code> resources based on the <code>spec.high-availability</code>. Let's say the operator includes the following HorizontalPodAutoscaler:</p> <pre><code>apiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  ...\nspec:\n  minReplicas: 3\n  maxReplicas: 5\n</code></pre> <p>If you configure <code>replicas: 2</code>, which is less than <code>minReplicas</code>, the operator transforms <code>minReplicas</code> to <code>1</code>.</p> <p>If you configure <code>replicas: 6</code>, which is more than <code>maxReplicas</code>, the operator transforms <code>maxReplicas</code> to <code>maxReplicas + (replicas - minReplicas)</code> which is <code>8</code>.</p>"},{"location":"install/operator/configuring-serving-cr/#override-system-deployments","title":"Override system deployments","text":"<p>If you would like to override some configurations for a specific deployment, you can override the configuration by modifying the <code>deployments</code> spec in the <code>KnativeServing</code> CR. Currently <code>resources</code>, <code>replicas</code>, <code>labels</code>, <code>annotations</code> and <code>nodeSelector</code> are supported.</p>"},{"location":"install/operator/configuring-serving-cr/#override-the-resources","title":"Override the resources","text":"<p>The <code>KnativeServing</code> CR is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within a deployment.</p> <p>For example, the following <code>KnativeServing</code> CR configures the container <code>controller</code> in the deployment <code>controller</code> to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  workloads:\n  - name: controller\n    resources:\n    - container: controller\n      requests:\n        cpu: 300m\n        memory: 100Mi\n      limits:\n        cpu: 1000m\n        memory: 250Mi\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure resources --component serving --deployName controller --container controller --requestCPU 300m --requestMemory 100Mi --limitCPU 1000m --limitMemory 250Mi -n knative-serving\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#override-replicas-labels-and-annotations","title":"Override replicas, labels and annotations","text":"<p>The following KnativeServing resource overrides the <code>webhook</code> deployment to have <code>3</code> Replicas, the label <code>mylabel: foo</code>, and the annotation <code>myannotations: bar</code>, while other system deployments have <code>2</code> Replicas by using <code>spec.high-availability</code>.</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  high-availability:\n    replicas: 2\n  workloads:\n  - name: webhook\n    replicas: 3\n    labels:\n      mylabel: foo\n    annotations:\n      myannotations: bar\n</code></pre> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>kn operator configure replicas --component serving --replicas 2 -n knative-serving\nkn operator configure replicas --component serving --deployName webhook --replicas 3 -n knative-serving\nkn operator configure labels --component serving --deployName webhook --key mylabel --value foo -n knative-serving\nkn operator configure annotations --component serving --deployName webhook --key myannotations --value bar -n knative-serving\n</code></pre> <p>Note</p> <p>The <code>KnativeServing</code> CR <code>label</code> and <code>annotation</code> settings override the webhook's labels and annotations for Deployments and Pods.</p>"},{"location":"install/operator/configuring-serving-cr/#override-the-nodeselector","title":"Override the nodeSelector","text":"<p>The following <code>KnativeServing</code> CR overrides the <code>webhook</code> deployment to use the <code>disktype: hdd</code> nodeSelector:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  workloads:\n  - name: webhook\n    nodeSelector:\n      disktype: hdd\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure nodeSelectors --component serving --deployName webhook --key disktype --value hdd -n knative-serving\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#override-the-tolerations","title":"Override the tolerations","text":"<p>The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations</p> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n</code></pre> <p>to the deployment <code>activator</code>, you need to change your KnativeServing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  workloads:\n  - name: activator\n    tolerations:\n    - key: \"key1\"\n      operator: \"Equal\"\n      value: \"value1\"\n      effect: \"NoSchedule\"\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure tolerations --component serving --deployName activator --key key1 --operator Equal --value value1 --effect NoSchedule -n knative-serving\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#override-the-affinity","title":"Override the affinity","text":"<p>The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity</p> <pre><code>affinity:\n  nodeAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 1\n      preference:\n        matchExpressions:\n        - key: disktype\n          operator: In\n          values:\n          - ssd\n</code></pre> <p>to the deployment <code>activator</code>, you need to change your KnativeServing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  workloads:\n  - name: activator\n    affinity:\n      nodeAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 1\n          preference:\n            matchExpressions:\n            - key: disktype\n              operator: In\n              values:\n              - ssd\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#override-the-environment-variables","title":"Override the environment variables","text":"<p>The KnativeServing resource is able to override or add the environment variables for the containers in the Knative Serving deployment resources. For example, if you would like to change the value of environment variable <code>METRICS_DOMAIN</code> in the container <code>controller</code> into \"knative.dev/my-repo\" for the deployment <code>controller</code>, you need to change your KnativeServing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  workloads:\n  - name: controller\n    env:\n    - container: controller\n      envVars:\n      - name: METRICS_DOMAIN\n        value: \"knative.dev/my-repo\"\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator configure envvars --component serving --deployName controller --container controller --name METRICS_DOMAIN --value \"knative.dev/my-repo\" -n knative-serving\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#override-system-services","title":"Override system services","text":"<p>If you would like to override some configurations for a specific service, you can override the configuration by using <code>spec.services</code> in CR. Currently <code>labels</code>, <code>annotations</code> and <code>selector</code> are supported.</p>"},{"location":"install/operator/configuring-serving-cr/#override-labels-and-annotations-and-selector","title":"Override labels and annotations and selector","text":"<p>The following KnativeServing resource overrides the <code>webhook</code> service to have the label <code>mylabel: foo</code>, the annotation <code>myannotations: bar</code>, the selector <code>myselector: bar</code>.</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  services:\n  - name: webhook\n    labels:\n      mylabel: foo\n    annotations:\n      myannotations: bar\n    selector:\n      myselector: bar\n</code></pre> <p>You can also run the following commands to make the equivalent change:</p> <pre><code>kn operator configure labels --component serving --serviceName webhook --key mylabel --value foo -n knative-serving\nkn operator configure annotations --component serving --serviceName webhook --key myannotations --value bar -n knative-serving\nkn operator configure selectors --component serving --serviceName webhook --key myselector --value bar -n knative-serving\n</code></pre>"},{"location":"install/operator/configuring-serving-cr/#override-system-poddisruptionbudgets","title":"Override system podDisruptionBudgets","text":"<p>A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for maintenance reasons. Knative Operator allows you to configure the <code>minAvailable</code> for a specific podDisruptionBudget resource in Serving based on the name. To understand more about the configuration of the resource podDisruptionBudget, click here. For example, if you would like to change <code>minAvailable</code> into 70% for the podDisruptionBudget named <code>activator-pdb</code>, you need to change your KnativeServing CR as below:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  podDisruptionBudgets:\n  - name: activator-pdb\n    minAvailable: 70%\n</code></pre>"},{"location":"install/operator/configuring-with-operator/","title":"Configuring Knative by using the Operator","text":"<p>The Operator manages the configuration of a Knative installation, including propagating values from the <code>KnativeServing</code> and <code>KnativeEventing</code> custom resources to system ConfigMaps.</p> <p>Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps.</p> <p>Knative has multiple ConfigMaps that are named with the prefix <code>config-</code>.</p> <p>All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the <code>KnativeServing</code> custom resource is created in the <code>knative-serving</code> namespace, all Knative Serving ConfigMaps are also created in this namespace.</p> <p>The <code>spec.config</code> in the Knative custom resources have one <code>&lt;name&gt;</code> entry for each ConfigMap, named <code>config-&lt;name&gt;</code>, with a value which is be used for the ConfigMap <code>data</code>.</p>"},{"location":"install/operator/configuring-with-operator/#examples","title":"Examples","text":"<p>You can specify that the <code>KnativeServing</code> custom resource uses the <code>config-domain</code> ConfigMap  as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  config:\n    domain:\n      example.org: |\n        selector:\n          app: prod\n      example.com: \"\"\n</code></pre> <p>You can apply values to multiple ConfigMaps. This example sets <code>stable-window</code> to 60s in the <code>config-autoscaler</code> ConfigMap, as well as specifying the <code>config-domain</code> ConfigMap:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  config:\n    domain:\n      example.org: |\n        selector:\n          app: prod\n      example.com: \"\"\n    autoscaler:\n      stable-window: \"60s\"\n</code></pre>"},{"location":"install/operator/knative-with-operator-cli/","title":"Install by using the Knative Operator CLI Plugin","text":"<p>Knative provides a CLI Plugin to install, configure and manage Knative via the command lines. This CLI plugin facilitates you with a parameter-driven way to configure the Knative cluster, without interacting with the complexities of the custom resources.</p>"},{"location":"install/operator/knative-with-operator-cli/#prerequisites","title":"Prerequisites","text":"<p>Before installing Knative, you must meet the following prerequisites:</p> <ul> <li> <p>For prototyping purposes, Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3\u00a0CPUs and 4\u00a0GB of memory.</p> <p>Tip</p> <p>You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin</p> </li> <li> <p>For production purposes, it is recommended that:</p> <ul> <li>If you have only one node in your cluster, you need at least 6\u00a0CPUs, 6\u00a0GB of memory, and 30\u00a0GB of disk storage.</li> <li>If you have multiple nodes in your cluster, for each node you need at least 2\u00a0CPUs, 4\u00a0GB of memory, and 20\u00a0GB of disk storage.</li> <li>You have a cluster that uses Kubernetes v1.28 or newer.</li> <li>You have installed the <code>kubectl</code> CLI.</li> <li>Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry.</li> </ul> </li> </ul> <p>Caution</p> <p>The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.</p>"},{"location":"install/operator/knative-with-operator-cli/#verifying-cli-binaries","title":"Verifying CLI binaries","text":"<p>Knative <code>kn</code> CLI releases from 1.9 onwards are signed with cosign. You can use the following steps to verify the CLI binaries:</p> <ol> <li> <p>Download the files you want, and the <code>checksums.txt</code>, <code>checksum.txt.pem</code>, and <code>checksums.txt.sig</code> files from the releases page, by running the commands:</p> <pre><code>wget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/checksums.txt\nwget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/kn-darwin-amd64\nwget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/checksums.txt.sig\nwget https://github.com/knative/client/releases/download/&lt;kn-version&gt;/checksums.txt.pem\n</code></pre> <p>Where <code>&lt;kn-version&gt;</code> is the version of the CLI that you want to verify. For example, <code>knative-v1.8.0</code>.</p> </li> <li> <p>Verify the signature by running the command:</p> <pre><code>cosign verify-blob \\\n--cert checksums.txt.pem \\\n--signature checksums.txt.sig \\\n--certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n--certificate-oidc-issuer=https://accounts.google.com \\\nchecksums.txt\n</code></pre> </li> <li> <p>If the signature is valid, you can then verify the <code>SHA256</code> sums match the downloaded binary, by running the command:</p> <pre><code>sha256sum --ignore-missing -c checksums.txt\n</code></pre> </li> </ol> <p>Note</p> <p>Knative images are signed in <code>KEYLESS</code> mode. To learn more about keyless signing, please refer to Keyless Signatures. The signing identity for Knative releases is <code>signer@knative-releases.iam.gserviceaccount.com</code>, and the issuer is <code>https://accounts.google.com</code>.</p>"},{"location":"install/operator/knative-with-operator-cli/#install-the-knative-operator-cli-plugin","title":"Install the Knative Operator CLI Plugin","text":"<p>Before you install the Knative Operator CLI Plugin, first install the Knative CLI.</p> MacOSLinux <ol> <li> <p>Download the binary <code>kn-operator-darwin-amd64</code> for your system from the release page.</p> </li> <li> <p>Rename the binary to <code>kn-operator</code>:</p> <pre><code>mv kn-operator-darwin-amd64 kn-operator\n</code></pre> </li> </ol> <ol> <li> <p>Download the binary <code>kn-operator-linux-amd64</code> for your system from the release page.</p> </li> <li> <p>Rename the binary to <code>kn-operator</code>:</p> <pre><code>mv kn-operator-linux-amd64 kn-operator\n</code></pre> </li> </ol> <p>Make the plugin executable by running the command:</p> <pre><code>chmod +x kn-operator\n</code></pre> <p>Create the directory for the <code>kn</code> plugin:</p> <pre><code>mkdir -p ~/.config/kn/plugins\n</code></pre> <p>Move the file to a plugin directory for <code>kn</code>:</p> <pre><code>cp kn-operator ~/.config/kn/plugins\n</code></pre>"},{"location":"install/operator/knative-with-operator-cli/#verify-the-installation-of-the-knative-operator-cli-plugin","title":"Verify the installation of the Knative Operator CLI Plugin","text":"<p>You can run the following command to verify the installation:</p> <pre><code>kn operator -h\n</code></pre> <p>You should see more information about how to use this CLI plugin.</p>"},{"location":"install/operator/knative-with-operator-cli/#install-the-knative-operator","title":"Install the Knative Operator","text":"<p>You can install Knative Operator of any specific version under any specific namespace. By default, the namespace is <code>default</code>, and the version is the latest.</p> <p>To install the latest version of Knative Operator, run:</p> <pre><code>kn operator install\n</code></pre> <p>To install Knative Operator under a certain namespace, e.g. knative-operator, run:</p> <pre><code>kn operator install -n knative-operator\n</code></pre> <p>To install Knative Operator of a specific version, e.g. 1.7.1, run:</p> <pre><code>kn operator install -v 1.7.1\n</code></pre>"},{"location":"install/operator/knative-with-operator-cli/#installing-the-knative-serving-component","title":"Installing the Knative Serving component","text":"<p>You can install Knative Serving of any specific version under any specific namespace. By default, the namespace is <code>knative-serving</code>, and the version is the latest.</p> <p>To install the latest version of Knative Serving, run:</p> <pre><code>kn operator install --component serving\n</code></pre> <p>To install Knative Serving under a certain namespace, e.g. knative-serving, run:</p> <pre><code>kn operator install --component serving -n knative-serving\n</code></pre> <p>To install Knative Operator of a specific version, e.g. 1.7, run:</p> <pre><code>kn operator install --component serving -n knative-serving -v \"1.7\"\n</code></pre> <p>To install the ingress plugin, e.g Kourier, together with the install command, run:</p> <pre><code>kn operator install --component serving -n knative-serving -v \"1.7\" --kourier\n</code></pre> <p>If you do not specify the ingress plugin, istio is used as the default. However, you need to make sure you install Istio first.</p>"},{"location":"install/operator/knative-with-operator-cli/#install-the-networking-layer","title":"Install the networking layer","text":"<p>You can configure the network layer option via the Operator CLI Plugin. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses:</p> Kourier (Choose this if you are not sure)Istio (default)Contour <p>The following steps install Kourier and enable its Knative integration:</p> <ol> <li> <p>To configure Knative Serving to use Kourier, run the command as follows:</p> <pre><code>kn operator enable ingress --kourier -n knative-serving\n</code></pre> </li> </ol> <p>The following steps install Istio to enable its Knative integration:</p> <ol> <li> <p>Install Istio.</p> </li> <li> <p>To configure Knative Serving to use Istio, run the command as follows:</p> <pre><code>kn operator enable ingress --istio -n knative-serving\n</code></pre> </li> </ol> <p>The following steps install Contour and enable its Knative integration:</p> <ol> <li> <p>Install a properly configured Contour:</p> <pre><code>kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml\n</code></pre> </li> <li> <p>To configure Knative Serving to use Contour, run the command as follows:</p> <pre><code>kn operator enable ingress --contour -n knative-serving\n</code></pre> </li> </ol>"},{"location":"install/operator/knative-with-operator-cli/#installing-the-knative-eventing-component","title":"Installing the Knative Eventing component","text":"<p>You can install Knative Eventing of any specific version under any specific namespace. By default, the namespace is <code>knative-eventing</code>, and the version is the latest.</p> <p>To install the latest version of Knative Eventing, run:</p> <pre><code>kn operator install --component eventing\n</code></pre> <p>To install Knative Eventing under a certain namespace, e.g. knative-eventing, run:</p> <pre><code>kn operator install --component eventing -n knative-eventing\n</code></pre> <p>To install Knative Operator of a specific version, e.g. 1.7, run:</p> <pre><code>kn operator install --component eventing -n knative-eventing -v \"1.7\"\n</code></pre>"},{"location":"install/operator/knative-with-operator-cli/#installing-knative-eventing-with-event-sources","title":"Installing Knative Eventing with event sources","text":"<p>Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources:</p> CephGitHubGitLabApache KafkaRabbitMQRedis <ol> <li> <p>To install the eventing source Ceph, run the following command:</p> <pre><code>kn operator enable eventing-source --ceph --namespace knative-eventing\n</code></pre> </li> </ol> <ol> <li> <p>To install the eventing source Github, run the following command:</p> <pre><code>kn operator enable eventing-source --github --namespace knative-eventing\n</code></pre> </li> </ol> <ol> <li> <p>To install the eventing source Gitlab, run the following command:</p> <pre><code>kn operator enable eventing-source --gitlab --namespace knative-eventing\n</code></pre> </li> </ol> <ol> <li> <p>To install the eventing source Kafka, run the following command:</p> <pre><code>kn operator enable eventing-source --kafka --namespace knative-eventing\n</code></pre> </li> </ol> <ol> <li> <p>To install the eventing source RabbitMQ, run the following command:</p> <pre><code>kn operator enable eventing-source --rabbitmq --namespace knative-eventing\n</code></pre> </li> </ol> <ol> <li> <p>To install the eventing source Redis, run the following command:</p> <pre><code>kn operator enable eventing-source --redis --namespace knative-eventing\n</code></pre> </li> </ol>"},{"location":"install/operator/knative-with-operator-cli/#whats-next","title":"What's next","text":"<ul> <li>Configure Knative Serving using Operator</li> <li>Configure Knative Eventing using Operator</li> </ul>"},{"location":"install/operator/knative-with-operators/","title":"Install by using the Knative Operator","text":"<p>Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster.</p> <p>The following table describes the supported versions of Serving and Eventing for the Knative Operator:</p> Operator Serving Eventing v1.13 v1.13.0v1.12.0, v1.12.1, v1.12.2 and v1.12.3v1.11.0, v1.1.1, v1.11.2, v1.11.3, v1.11.6, v1.11.5 and v1.11.6v1.10.0, v1.10.1 and v1.10.2 v1.13.0v1.12.0, v1.12.1, v1.12.2 and v1.12.3v1.11.0, v1.11.1, v1.11.2, v1.11.3, v1.11.4, v1.11.5 and v1.11.6v1.10.0, v1.10.1, v1.10.2, v1.10.3, v1.10.4, v1.10.5 and v1.10.6"},{"location":"install/operator/knative-with-operators/#prerequisites","title":"Prerequisites","text":"<p>Before installing Knative, you must meet the following prerequisites:</p> <ul> <li> <p>For prototyping purposes, Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3\u00a0CPUs and 4\u00a0GB of memory.</p> <p>Tip</p> <p>You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin</p> </li> <li> <p>For production purposes, it is recommended that:</p> <ul> <li>If you have only one node in your cluster, you need at least 6\u00a0CPUs, 6\u00a0GB of memory, and 30\u00a0GB of disk storage.</li> <li>If you have multiple nodes in your cluster, for each node you need at least 2\u00a0CPUs, 4\u00a0GB of memory, and 20\u00a0GB of disk storage.</li> <li>You have a cluster that uses Kubernetes v1.28 or newer.</li> <li>You have installed the <code>kubectl</code> CLI.</li> <li>Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry.</li> </ul> </li> </ul> <p>Caution</p> <p>The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.</p>"},{"location":"install/operator/knative-with-operators/#verifying-image-signatures","title":"Verifying image signatures","text":"<p>Knative releases from 1.9 onwards are signed with cosign.</p> <ol> <li> <p>Install cosign and jq.</p> </li> <li> <p>Extract the images from a manifeset and verify the signatures.</p> </li> </ol> <pre><code>curl -sSL https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml \\\n  | grep 'gcr.io/' | awk '{print $2}' | sort | uniq \\\n  | xargs -n 1 \\\n    cosign verify -o text \\\n      --certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n      --certificate-oidc-issuer=https://accounts.google.com\n</code></pre> <p>Note</p> <p>Knative images are signed in <code>KEYLESS</code> mode. To learn more about keyless signing, please refer to Keyless Signatures Our signing identity(Subject) for our releases is <code>signer@knative-releases.iam.gserviceaccount.com</code> and the Issuer is <code>https://accounts.google.com</code></p>"},{"location":"install/operator/knative-with-operators/#install-the-knative-operator","title":"Install the Knative Operator","text":"<p>Before you install the Knative Serving and Eventing components, first install the Knative Operator.</p> <p>Warning</p> <p>Knative Operator 1.5 is the last version that supports CRDs with both <code>v1alpha1</code> and <code>v1beta1</code>. If you are upgrading an existing Operator install from v1.2 or earlier to v1.3 or later, run the following command to upgrade the existing custom resources to <code>v1beta1</code> before installing the current version:</p> <pre><code>kubectl create -f https://github.com/knative/operator/releases/download/knative-v1.5.1/operator-post-install.yaml\n</code></pre> <p>To install the latest stable Operator release, run the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml\n</code></pre> <p>You can find information about the released versions of the Knative Operator on the releases page.</p>"},{"location":"install/operator/knative-with-operators/#verify-your-knative-operator-installation","title":"Verify your Knative Operator installation","text":"<ol> <li> <p>Because the Operator is installed to the <code>default</code> namespace, ensure you set the current namespace to <code>default</code> by running the command:</p> <pre><code>kubectl config set-context --current --namespace=default\n</code></pre> </li> <li> <p>Check the Operator deployment status by running the command:</p> <pre><code>kubectl get deployment knative-operator\n</code></pre> <p>If the Operator is installed correctly, the deployment shows a <code>Ready</code> status:</p> <pre><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE\nknative-operator   1/1     1            1           19h\n</code></pre> </li> </ol>"},{"location":"install/operator/knative-with-operators/#track-the-log","title":"Track the log","text":"<p>To track the log of the Operator, run the command:</p> <pre><code>kubectl logs -f deploy/knative-operator\n</code></pre>"},{"location":"install/operator/knative-with-operators/#install-knative-serving","title":"Install Knative Serving","text":"<p>To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS.</p>"},{"location":"install/operator/knative-with-operators/#create-the-knative-serving-custom-resource","title":"Create the Knative Serving custom resource","text":"<p>To create the custom resource for the latest available Knative Serving in the Operator:</p> <ol> <li> <p>Copy the following YAML into a file:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-serving\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\n</code></pre> <p>Note</p> <p>When you don't specify a version by using <code>spec.version</code> field, the Operator defaults to the latest available version.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"install/operator/knative-with-operators/#install-the-networking-layer","title":"Install the networking layer","text":"<p>Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer.</p> <p>Click on each of the following tabs to see how you can configure Knative Serving with different ingresses:</p> Kourier (Choose this if you are not sure)Istio (default)Contour <p>The following steps install Kourier and enable its Knative integration:</p> <ol> <li> <p>To configure Knative Serving to use Kourier, add <code>spec.ingress.kourier</code> and <code>spec.config.network</code> to your Serving CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  # ...\n  ingress:\n    kourier:\n      enabled: true\n  config:\n    network:\n      ingress-class: \"kourier.ingress.networking.knative.dev\"\n</code></pre> </li> <li> <p>Apply the YAML file for your Serving CR by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of your Serving CR file.</p> </li> <li> <p>Fetch the External IP or CNAME by running the command:</p> <pre><code>kubectl --namespace knative-serving get service kourier\n</code></pre> <p>Save this for configuring DNS later.</p> </li> </ol> <p>The following steps install Istio to enable its Knative integration:</p> <ol> <li> <p>Install Istio.</p> </li> <li> <p>If you installed Istio under a namespace other than the default <code>istio-system</code>:</p> <ol> <li> <p>Add <code>spec.config.istio</code> to your Serving CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  # ...\n  config:\n    istio:\n      local-gateways: |\n        - name: knative-local-gateway\n          namespace: &lt;local-gateway-namespace&gt;\n          service: knative-local-gateway.&lt;istio-namespace&gt;.svc.cluster.local\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;local-gateway-namespace&gt;</code> is the local gateway namespace, which is the same as Knative Serving namespace <code>knative-serving</code>.</li> <li><code>&lt;istio-namespace&gt;</code> is the namespace where Istio is installed.</li> </ul> </li> <li> <p>Apply the YAML file for your Serving CR by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of your Serving CR file.</p> </li> </ol> </li> <li> <p>Fetch the External IP or CNAME by running the command:</p> <pre><code>kubectl get svc istio-ingressgateway -n &lt;istio-namespace&gt;\n</code></pre> <p>Save this for configuring DNS later.</p> </li> </ol> <p>The following steps install Contour and enable its Knative integration:</p> <ol> <li> <p>Install a properly configured Contour:</p> <pre><code>kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml\n</code></pre> </li> <li> <p>To configure Knative Serving to use Contour, add <code>spec.ingress.contour</code> <code>spec.config.network</code> to your Serving CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  # ...\n  ingress:\n    contour:\n      enabled: true\n  config:\n    network:\n      ingress-class: \"contour.ingress.networking.knative.dev\"\n</code></pre> </li> <li> <p>Apply the YAML file for your Serving CR by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of your Serving CR file.</p> </li> <li> <p>Fetch the External IP or CNAME by running the command:</p> <pre><code>kubectl --namespace contour-external get service envoy\n</code></pre> <p>Save this for configuring DNS later.</p> </li> </ol>"},{"location":"install/operator/knative-with-operators/#verify-the-knative-serving-deployment","title":"Verify the Knative Serving deployment","text":"<ol> <li> <p>Monitor the Knative deployments:</p> <pre><code>kubectl get deployment -n knative-serving\n</code></pre> <p>If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show <code>READY</code> status. Here is a sample output:</p> <pre><code>NAME                   READY   UP-TO-DATE   AVAILABLE   AGE\nactivator              1/1     1            1           18s\nautoscaler             1/1     1            1           18s\nautoscaler-hpa         1/1     1            1           14s\ncontroller             1/1     1            1           18s\ndomain-mapping         1/1     1            1           12s\ndomainmapping-webhook  1/1     1            1           12s\nwebhook                1/1     1            1           17s\n</code></pre> </li> <li> <p>Check the status of Knative Serving Custom Resource:</p> <pre><code>kubectl get KnativeServing knative-serving -n knative-serving\n</code></pre> <p>If Knative Serving is successfully installed, you should see:</p> <pre><code>NAME              VERSION             READY   REASON\nknative-serving   &lt;version number&gt;    True\n</code></pre> </li> </ol>"},{"location":"install/operator/knative-with-operators/#configure-dns","title":"Configure DNS","text":"<p>You can configure DNS to prevent the need to run curl commands with a host header.</p> <p>The following tabs expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice:</p> Magic DNS (sslip.io)Real DNSNo DNS <p>Knative provides a Kubernetes Job called <code>default-domain</code> that configures Knative Serving to use sslip.io as the default DNS suffix.</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml\n</code></pre> <p>Warning</p> <p>This will only work if the cluster <code>LoadBalancer</code> Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless <code>minikube tunnel</code> is running.</p> <p>In these cases, see the \"Real DNS\" or \"No DNS\" tabs.</p> <p>To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows:</p> <ul> <li> <p>If the networking layer produced an External IP address, then configure a   wildcard <code>A</code> record for the domain:</p> <pre><code># Here knative.example.com is the domain suffix for your cluster\n*.knative.example.com == A 35.233.41.212\n</code></pre> </li> <li> <p>If the networking layer produced a CNAME, then configure a CNAME record for the domain:</p> <pre><code># Here knative.example.com is the domain suffix for your cluster\n*.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com\n</code></pre> </li> <li> <p>Once your DNS provider has been configured, add <code>spec.config.domain</code> into your existing Serving CR, and apply it:</p> <pre><code># Replace knative.example.com with your domain suffix\napiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  ...\n  config:\n    domain:\n      \"knative.example.com\": \"\"\n  ...\n</code></pre> </li> </ul> <p>If you are using <code>curl</code> to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (sslip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters.</p> <p>To access your application using <code>curl</code> using this method:</p> <ol> <li> <p>Configure Knative to use a domain reachable from outside the cluster:   <pre><code>kubectl patch configmap/config-domain \\\n      --namespace knative-serving \\\n      --type merge \\\n      --patch '{\"data\":{\"example.com\":\"\"}}'\n</code></pre></p> </li> <li> <p>After starting your application, get the URL of your application:   <pre><code>kubectl get ksvc\n</code></pre>   The output should be similar to:   <pre><code>NAME            URL                                        LATESTCREATED         LATESTREADY           READY   REASON\nhelloworld-go   http://helloworld-go.default.example.com   helloworld-go-vqjlf   helloworld-go-vqjlf   True\n</code></pre></p> </li> <li> <p>Instruct <code>curl</code> to connect to the External IP or CNAME defined by the    networking layer mentioned in section 3, and use the <code>-H \"Host:\"</code> command-line    option to specify the Knative application's host name.    For example, if the networking layer defines your External IP and port to be <code>http://192.168.39.228:32198</code> and you wish to access the <code>helloworld-go</code> application mentioned earlier, use:    <pre><code>curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198\n</code></pre>    In the case of the provided <code>helloworld-go</code> sample application, using the default configuration, the output is:    <pre><code>Hello Go Sample v1!\n</code></pre>    Refer to the \"Real DNS\" method for a permanent solution.</p> </li> </ol>"},{"location":"install/operator/knative-with-operators/#install-knative-eventing","title":"Install Knative Eventing","text":"<p>To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources.</p>"},{"location":"install/operator/knative-with-operators/#create-the-knative-eventing-custom-resource","title":"Create the Knative Eventing custom resource","text":"<p>To create the custom resource for the latest available Knative Eventing in the Operator:</p> <ol> <li> <p>Copy the following YAML into a file:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-eventing\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\n</code></pre> <p>Note</p> <p>When you do not specify a version by using <code>spec.version</code> field, the Operator defaults to the latest available version.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> </li> </ol> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p>"},{"location":"install/operator/knative-with-operators/#installing-a-specific-version-of-eventing","title":"Installing a specific version of Eventing","text":"<p>Cluster administrators can install a specific version of Knative Eventing by using the <code>spec.version</code> field. For example, if you want to install Knative Eventing v1.7, you can apply the following KnativeEventing CR:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  version: \"1.7\"\n</code></pre> <p>You can also run the following command to make the equivalent change:</p> <pre><code>kn operator install --component eventing -v 1.7 -n knative-eventing\n</code></pre> <p>If <code>spec.version</code> is not specified, the Knative Operator installs the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions.</p> <p>If Knative Eventing is already managed by the Operator, updating the <code>spec.version</code> field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator.</p> <p>Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 1.4, you must upgrade to 1.5 before upgrading to 1.6.</p>"},{"location":"install/operator/knative-with-operators/#installing-customized-knative-eventing","title":"Installing customized Knative Eventing","text":"<p>The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them.</p> <p>There are two modes available for you to install customized manifests: overwrite mode and append mode. With overwrite mode, under <code>.spec.manifests</code>, you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, under <code>.spec.additionalManifests</code>, you only need to define your customized manifests. The customized manifests are installed after default manifests are applied.</p>"},{"location":"install/operator/knative-with-operators/#overwrite-mode","title":"Overwrite mode","text":"<p>Use overwrite mode when you want to customize all Knative Eventing manifests to be installed.</p> <p>For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-eventing\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  version: $spec_version\n  manifests:\n  - URL: https://my-eventing/eventing.yaml\n</code></pre> <p>This example installs the customized Knative Eventing at version <code>$spec_version</code> which is available at <code>https://my-eventing/eventing.yaml</code>.</p> <p>Attention</p> <p>You can make the customized Knative Eventing available in one or multiple links, as the <code>spec.manifests</code> supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top.</p> <p>We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both <code>spec.version</code> and <code>spec.manifests</code>. Do not skip either field.</p>"},{"location":"install/operator/knative-with-operators/#append-mode","title":"Append mode","text":"<p>You can use append mode to add your customized manifests into the default manifests.</p> <p>For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-eventing\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  version: $spec_version\n  additionalManifests:\n  - URL: https://my-eventing/eventing-custom.yaml\n</code></pre> <p>This example installs the default Knative Eventing, and installs your customized resources available at <code>https://my-eventing/eventing-custom.yaml</code>.</p> <p>Knative Operator installs the default manifests of Knative Eventing at the version <code>$spec_version</code>, and then installs your customized manifests based on them.</p>"},{"location":"install/operator/knative-with-operators/#installing-knative-eventing-with-event-sources","title":"Installing Knative Eventing with event sources","text":"<p>Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources:</p> CephGitHubGitLabApache KafkaRabbitMQRedis <p>To configure Knative Eventing to install Ceph as the event source:</p> <ol> <li> <p>Add <code>spec.source.ceph</code> to your Eventing CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  # ...\n  source:\n    ceph:\n      enabled: true\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To configure Knative Eventing to install GitHub as the event source:</p> <ol> <li> <p>Add <code>spec.source.github</code> to your Eventing CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  # ...\n  source:\n    github:\n      enabled: true\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To configure Knative Eventing to install GitLab as the event source:</p> <ol> <li> <p>Add <code>spec.source.gitlab</code> to your Eventing CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  # ...\n  source:\n    gitlab:\n      enabled: true\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To configure Knative Eventing to install Kafka as the event source:</p> <ol> <li> <p>Add <code>spec.source.kafka</code> to your Eventing CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  # ...\n  source:\n    kafka:\n      enabled: true\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To configure Knative Eventing to install RabbitMQ as the event source,</p> <ol> <li> <p>Add <code>spec.source.rabbitmq</code> to your Eventing CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  # ...\n  source:\n    rabbitmq:\n      enabled: true\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To configure Knative Eventing to install Redis as the event source:</p> <ol> <li> <p>Add <code>spec.source.redis</code> to your Eventing CR YAML file as follows:</p> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  # ...\n  source:\n    redis:\n      enabled: true\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"install/operator/knative-with-operators/#verify-the-knative-eventing-deployment","title":"Verify the Knative Eventing deployment","text":"<ol> <li> <p>Monitor the Knative deployments:</p> <pre><code>kubectl get deployment -n knative-eventing\n</code></pre> <p>If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show <code>READY</code> status. Here is a sample output:</p> <pre><code>NAME                    READY   UP-TO-DATE   AVAILABLE   AGE\neventing-controller     1/1     1            1           43s\neventing-webhook        1/1     1            1           42s\nimc-controller          1/1     1            1           39s\nimc-dispatcher          1/1     1            1           38s\nmt-broker-controller    1/1     1            1           36s\nmt-broker-filter        1/1     1            1           37s\nmt-broker-ingress       1/1     1            1           37s\npingsource-mt-adapter   0/0     0            0           43s\nsugar-controller        1/1     1            1           36s\n</code></pre> </li> <li> <p>Check the status of Knative Eventing Custom Resource:</p> <pre><code>kubectl get KnativeEventing knative-eventing -n knative-eventing\n</code></pre> <p>If Knative Eventing is successfully installed, you should see:</p> <pre><code>NAME               VERSION             READY   REASON\nknative-eventing   &lt;version number&gt;    True\n</code></pre> </li> </ol>"},{"location":"install/operator/knative-with-operators/#uninstalling-knative","title":"Uninstalling Knative","text":"<p>Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.</p>"},{"location":"install/operator/knative-with-operators/#removing-the-knative-serving-component","title":"Removing the Knative Serving component","text":"<p>To remove the Knative Serving CR run the command:</p> <pre><code>kubectl delete KnativeServing knative-serving -n knative-serving\n</code></pre>"},{"location":"install/operator/knative-with-operators/#removing-knative-eventing-component","title":"Removing Knative Eventing component","text":"<p>To remove the Knative Eventing CR run the command:</p> <pre><code>kubectl delete KnativeEventing knative-eventing -n knative-eventing\n</code></pre>"},{"location":"install/operator/knative-with-operators/#removing-the-knative-operator","title":"Removing the Knative Operator:","text":"<p>If you have installed Knative using the release page, remove the operator by running the command:</p> <pre><code>kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml\n</code></pre> <p>If you have installed Knative from source, uninstall it using the following command while in the root directory for the source:</p> <pre><code>ko delete -f config/\n</code></pre>"},{"location":"install/operator/knative-with-operators/#whats-next","title":"What's next","text":"<ul> <li>Configure Knative Serving using Operator</li> <li>Configure Knative Eventing using Operator</li> </ul>"},{"location":"install/upgrade/","title":"Upgrading Knative","text":"<p>Knative supports upgrading by a single minor version number. For example, if you have v0.21.0 installed, you must upgrade to v0.22.0 before attempting to upgrade to v0.23.0. To verify your current version, see Checking your Knative version.</p> <p>To upgrade Knative:</p> <ul> <li>If you installed Knative using YAML, see Upgrading with kubectl.</li> <li>If you installed Knative using the Knative Operator, see Upgrading using the Knative Operator.</li> </ul>"},{"location":"install/upgrade/check-install-version/","title":"Checking your Knative version","text":"<p>To check the version of your Knative installation, use one of the following commands, depending on whether you installed Knative with YAML or with the Operator.</p>"},{"location":"install/upgrade/check-install-version/#if-you-installed-with-yaml","title":"If you installed with YAML","text":"<p>To verify the version of the Knative component that you have running on your cluster, query for the <code>app.kubernetes.io/version</code> label in corresponding component namespace.</p> Knative ServingKnative Eventing <p>Check the installed Knative Serving version by running the command:</p> <pre><code>kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"app.kubernetes.io/version\"}}'\n</code></pre> <p>Example output:</p> <pre><code>v0.23.0\n</code></pre> <p>Check the installed Knative Eventing version by running the command:</p> <pre><code>kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"app.kubernetes.io/version\"}}'\n</code></pre> <p>Example output:</p> <pre><code>v0.23.0\n</code></pre>"},{"location":"install/upgrade/check-install-version/#if-you-installed-with-the-operator","title":"If you installed with the Operator","text":"<p>To verify the version of your current Knative installation:</p> Knative ServingKnative Eventing <p>Check the installed Knative Serving version by running the command:</p> <pre><code>kubectl get KnativeServing knative-serving --namespace knative-serving\n</code></pre> <p>Example output:</p> <pre><code>NAME              VERSION         READY   REASON\nknative-serving   0.23.0          True\n</code></pre> <p>Check the installed Knative Eventing version by running the command:</p> <pre><code>kubectl get KnativeEventing knative-eventing --namespace knative-eventing\n</code></pre> <p>Example output:</p> <pre><code>NAME               VERSION         READY   REASON\nknative-eventing   0.23.0          True\n</code></pre>"},{"location":"install/upgrade/upgrade-installation-with-operator/","title":"Upgrading using the Knative Operator","text":"<p>This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl.</p> <p>The attribute <code>spec.version</code> is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the <code>patch</code> number, because the Knative Operator matches the latest available <code>patch</code> number, as long as you specify <code>major.minor</code> for the version. For example, you only need to specify <code>\"1.1\"</code> to upgrade to the 1.1 release, you do not need to specify the exact <code>patch</code> number.</p> <p>The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 1.5, it bundles and supports the installation of Knative versions 1.5, 1.4, 1.3 and 1.2.</p> <p>Note</p> <p>In the following examples, Knative Serving custom resources are installed in the <code>knative-serving</code> namespace, and  Knative Eventing custom resources are installed in the <code>knative-eventing</code> namespace.</p>"},{"location":"install/upgrade/upgrade-installation-with-operator/#performing-the-upgrade","title":"Performing the upgrade","text":"<p>To upgrade, apply the Operator custom resources, adding the <code>spec.version</code> for the Knative version that you want to upgrade to:</p> <ol> <li> <p>Create a YAML file containing the following:</p> <p><pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  version: \"&lt;new-version&gt;\"\n</code></pre> Where <code>&lt;new-version&gt;</code> is the Knative version that you want to upgrade to.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"install/upgrade/upgrade-installation-with-operator/#verifying-an-upgrade-by-viewing-pods","title":"Verifying an upgrade by viewing pods","text":"<p>You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace.</p> <p>Note</p> <p>All pods will restart during the upgrade and their age will reset.</p> Knative ServingKnative Eventing <p>Enter the following command to view information about pods in the <code>knative-serving</code> namespace:</p> <pre><code>kubectl get pods -n knative-serving\n</code></pre> <p>The command returns an output similar to the following:</p> <pre><code>NAME                                                     READY   STATUS      RESTARTS   AGE\nactivator-6875896748-gdjgs                               1/1     Running     0          58s\nautoscaler-6bbc885cfd-vkrgg                              1/1     Running     0          57s\nautoscaler-hpa-5cdd7c6b69-hxzv4                          1/1     Running     0          55s\ncontroller-64dd4bd56-wzb2k                               1/1     Running     0          57s\nnet-istio-webhook-75cc84fbd4-dkcgt                       1/1     Running     0          50s\nnet-istio-controller-6dcbd4b5f4-mxm8q                    1/1     Running     0          51s\nstorage-version-migration-serving-serving-0.20.0-82hjt   0/1     Completed   0          50s\nwebhook-75f5d4845d-zkrdt                                 1/1     Running     0          56s\n</code></pre> <p>Enter the following command to view information about pods in the <code>knative-eventing</code> namespace:</p> <pre><code>kubectl get pods -n knative-eventing\n</code></pre> <p>The command returns an output similar to the following:</p> <pre><code>NAME                                              READY   STATUS      RESTARTS   AGE\neventing-controller-6bc59c9fd7-6svbm              1/1     Running     0          38s\neventing-webhook-85cd479f87-4dwxh                 1/1     Running     0          38s\nimc-controller-97c4fd87c-t9mnm                    1/1     Running     0          33s\nimc-dispatcher-c6db95ffd-ln4mc                    1/1     Running     0          33s\nmt-broker-controller-5f87fbd5d9-m69cd             1/1     Running     0          32s\nmt-broker-filter-5b9c64cbd5-d27p4                 1/1     Running     0          32s\nmt-broker-ingress-55c66fdfdf-gn56g                1/1     Running     0          32s\nstorage-version-migration-eventing-0.20.0-fvgqf   0/1     Completed   0          31s\nsugar-controller-684d5cfdbb-67vsv                 1/1     Running     0          31s\n</code></pre>"},{"location":"install/upgrade/upgrade-installation-with-operator/#verifying-an-upgrade-by-viewing-custom-resources","title":"Verifying an upgrade by viewing custom resources","text":"<p>You can verify the status of a Knative component by checking that the custom resource <code>READY</code> status is <code>True</code>.</p> Knative ServingKnative Eventing <pre><code>kubectl get KnativeServing knative-serving -n knative-serving\n</code></pre> <p>This command returns an output similar to the following:</p> <pre><code>NAME              VERSION         READY   REASON\nknative-serving   1.1.0          True\n</code></pre> <pre><code>kubectl get KnativeEventing knative-eventing -n knative-eventing\n</code></pre> <p>This command returns an output similar to the following:</p> <p><pre><code>NAME               VERSION        READY   REASON\nknative-eventing   1.1.0         True\n</code></pre> </p>"},{"location":"install/upgrade/upgrade-installation-with-operator/#rollback-to-an-earlier-version","title":"Rollback to an earlier version","text":"<p>If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 1.2, and your previous version is 1.1, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 1.1.</p> Knative ServingKnative Eventing <p>To rollback to a previous version of Knative Serving:</p> <ol> <li> <p>Create a YAML file containing the following:</p> <p><pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  version: \"&lt;previous-version&gt;\"\n</code></pre> Where <code>&lt;previous-version&gt;</code> is the Knative version that you want to downgrade to.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>To rollback to a previous version of Knative Eventing:</p> <ol> <li> <p>Create a YAML file containing the following:</p> <p><pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec:\n  version: \"&lt;previous-version&gt;\"\n</code></pre> Where <code>&lt;previous-version&gt;</code> is the Knative version that you want to downgrade to.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"install/upgrade/upgrade-installation/","title":"Upgrading with kubectl","text":"<p>If you installed Knative using YAML, you can use the <code>kubectl apply</code> command in this topic to upgrade your Knative components and plugins. If you installed using the Operator, see Upgrading using the Knative Operator.</p>"},{"location":"install/upgrade/upgrade-installation/#before-you-begin","title":"Before you begin","text":"<p>Before upgrading, there are a few steps you must take to ensure a successful upgrade process.</p>"},{"location":"install/upgrade/upgrade-installation/#identify-breaking-changes","title":"Identify breaking changes","text":"<p>You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications:</p> <ul> <li>Serving</li> <li>Eventing</li> </ul> <p>Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub.</p>"},{"location":"install/upgrade/upgrade-installation/#view-current-pod-status","title":"View current pod status","text":"<p>Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace:</p> <pre><code>kubectl get pods -n knative-serving\n</code></pre> <pre><code>kubectl get pods -n knative-eventing\n</code></pre>"},{"location":"install/upgrade/upgrade-installation/#upgrade-plugins","title":"Upgrade plugins","text":"<p>If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components.</p>"},{"location":"install/upgrade/upgrade-installation/#run-pre-install-tools-before-upgrade","title":"Run pre-install tools before upgrade","text":"<p>For some upgrades, there are steps that must be completed before the actual upgrade. These steps, where applicable, are identified in the release notes.</p>"},{"location":"install/upgrade/upgrade-installation/#upgrade-existing-resources-to-the-latest-stored-version","title":"Upgrade existing resources to the latest stored version","text":"<p>Knative custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading.</p> <p>For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required.</p>"},{"location":"install/upgrade/upgrade-installation/#performing-the-upgrade","title":"Performing the upgrade","text":"<p>To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time.</p> <p>Before upgrading, check your Knative version.</p> <p>For a cluster running version 1.1 of the Knative Serving and Knative Eventing components, the following command upgrades the installation to version 1.2:</p> <pre><code>kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.2.0/serving-core.yaml \\\n-f https://github.com/knative/eventing/releases/download/knative-v1.2.0/eventing.yaml \\\n</code></pre>"},{"location":"install/upgrade/upgrade-installation/#run-post-install-jobs-when-necessary","title":"Run post-install jobs when necessary","text":"<p>When the release notes point out that a Knative custom resource is transitioned to a new version, for example like:</p> <p>DomainMapping/v1alpha1 is deprecated - use v1beta1 APIs</p> <p>you need to run post-install jobs (see here for details):</p> <pre><code># Serving\nkubectl create -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-post-install-jobs.yaml\n\n# Eventing\nkubectl create -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-post-install.yaml\n</code></pre> <p>Make sure that the jobs complete successfully before continuing:</p> <pre><code># Serving\nkubectl get job -n knative-serving\n\n# Eventing\nkubectl get job -n knative-eventing\n</code></pre> <p>Caution</p> <p>You also have to transition the version in your custom resource YAML files if you store them externally, e.g. in git via GitOps.</p>"},{"location":"install/upgrade/upgrade-installation/#verifying-the-upgrade","title":"Verifying the upgrade","text":"<p>To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace:</p> <pre><code>kubectl get pods -n knative-serving\n</code></pre> <pre><code>kubectl get pods -n knative-eventing\n</code></pre> <p>These commands return something similar to:</p> <pre><code>NAME                                READY   STATUS        RESTARTS   AGE\nactivator-79f674fb7b-dgvss          2/2     Running       0          43s\nautoscaler-96dc49858-b24bm          2/2     Running       1          43s\nautoscaler-hpa-d887d4895-njtrb      1/1     Running       0          43s\ncontroller-6bcdd87fd6-zz9fx         1/1     Running       0          41s\nnet-istio-controller-7fcdf7-z2xmr   1/1     Running       0          40s\nwebhook-747b799559-4sj6q            1/1     Running       0          41s\n</code></pre> <pre><code>NAME                                READY   STATUS        RESTARTS   AGE\neventing-controller-69ffcc6f7d-5l7th   1/1     Running   0          83s\neventing-webhook-6c56fcd86c-42dr8      1/1     Running   0          81s\nimc-controller-6bcf5957b5-6ccp2        1/1     Running   0          80s\nimc-dispatcher-f59b7c57-q9xcl          1/1     Running   0          80s\nsources-controller-8596684d7b-jxkmd    1/1     Running   0          83s\n</code></pre> <p>If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of <code>Terminating</code> for the old pods as they are cleaned up.</p> <p>If necessary, repeat the upgrade process until you reach your desired minor version number.</p>"},{"location":"install/yaml-install/","title":"About YAML-based installation","text":"<p>You can install the Serving component, Eventing component, or both on your cluster by applying YAML files.</p> <ul> <li>Install Knative Serving with YAML</li> <li>Install Knative Eventing with YAML</li> </ul>"},{"location":"install/yaml-install/eventing/eventing-installation-files/","title":"Knative Eventing installation files","text":"<p>This guide provides reference information about the core Knative Eventing YAML files, including:</p> <ul> <li>The custom resource definitions (CRDs) and core components required to install Knative Eventing.</li> <li>Optional components that you can apply to customize your installation.</li> </ul> <p>For information about installing these files, see Installing Knative Eventing using YAML files.</p> <p>The following table describes the installation files included in Knative Eventing:</p> File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing.yaml Combines <code>eventing-core.yaml</code>, <code>mt-channel-broker.yaml</code>, and <code>in-memory-channel.yaml</code>. none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/","title":"Installing Knative Eventing using YAML files","text":"<p>This topic describes how to install Knative Eventing by applying YAML files using the\u00a0<code>kubectl</code>\u00a0CLI.</p>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#prerequisites","title":"Prerequisites","text":"<p>Before installing Knative, you must meet the following prerequisites:</p> <ul> <li> <p>For prototyping purposes, Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3\u00a0CPUs and 4\u00a0GB of memory.</p> <p>Tip</p> <p>You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin</p> </li> <li> <p>For production purposes, it is recommended that:</p> <ul> <li>If you have only one node in your cluster, you need at least 6\u00a0CPUs, 6\u00a0GB of memory, and 30\u00a0GB of disk storage.</li> <li>If you have multiple nodes in your cluster, for each node you need at least 2\u00a0CPUs, 4\u00a0GB of memory, and 20\u00a0GB of disk storage.</li> <li>You have a cluster that uses Kubernetes v1.28 or newer.</li> <li>You have installed the <code>kubectl</code> CLI.</li> <li>Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry.</li> </ul> </li> </ul> <p>Caution</p> <p>The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.</p>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#verifying-image-signatures","title":"Verifying image signatures","text":"<p>Knative releases from 1.9 onwards are signed with cosign.</p> <ol> <li> <p>Install cosign and jq.</p> </li> <li> <p>Extract the images from a manifeset and verify the signatures.</p> </li> </ol> <pre><code>curl -sSL https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml \\\n  | grep 'gcr.io/' | awk '{print $2}' | sort | uniq \\\n  | xargs -n 1 \\\n    cosign verify -o text \\\n      --certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n      --certificate-oidc-issuer=https://accounts.google.com\n</code></pre> <p>Note</p> <p>Knative images are signed in <code>KEYLESS</code> mode. To learn more about keyless signing, please refer to Keyless Signatures Our signing identity(Subject) for our releases is <code>signer@knative-releases.iam.gserviceaccount.com</code> and the Issuer is <code>https://accounts.google.com</code></p>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-knative-eventing","title":"Install Knative Eventing","text":"<p>To install Knative Eventing:</p> <ol> <li> <p>Install the required custom resource definitions (CRDs) by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml\n</code></pre> </li> <li> <p>Install the core components of Eventing by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml\n</code></pre> <p>Info</p> <p>For information about the YAML files in Knative Eventing, see Description Tables for YAML Files.</p> </li> </ol>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#verify-the-installation","title":"Verify the installation","text":"<p>Success</p> <p>Monitor the Knative components until all of the components show a <code>STATUS</code> of <code>Running</code> or <code>Completed</code>. You can do this by running the following command and inspecting the output:</p> <pre><code>kubectl get pods -n knative-eventing\n</code></pre> <p>Example output:</p> <pre><code>NAME                                   READY   STATUS    RESTARTS   AGE\neventing-controller-7995d654c7-qg895   1/1     Running   0          2m18s\neventing-webhook-fff97b47c-8hmt8       1/1     Running   0          2m17s\n</code></pre>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-default-channel-messaging-layer","title":"Optional: Install a default Channel (messaging) layer","text":"<p>The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice:</p> Apache Kafka ChannelIn-Memory (standalone)NATS Channel <p>The following commands install the KafkaChannel and run event routing in a system namespace. The <code>knative-eventing</code> namespace is used by default.</p> <ol> <li> <p>Install the Kafka controller by running the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> <li> <p>Install the KafkaChannel data plane by running the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-channel.yaml\n</code></pre> </li> <li> <p>If you're upgrading from the previous version, run the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml\n</code></pre> </li> </ol> <p>Warning</p> <p>This simple standalone implementation runs in-memory and is not suitable for production use cases.</p> <ul> <li> <p>Install an in-memory implementation of Channel by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml\n</code></pre> </li> </ul> <ol> <li> <p>Install NATS Streaming for Kubernetes.</p> </li> <li> <p>Install the NATS Streaming Channel by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml\n</code></pre> </li> </ol> <p>You can change the default channel implementation by following the instructions described in the Configure Channel defaults section.</p>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-broker-layer","title":"Optional: Install a Broker layer","text":"<p>The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice:</p> Apache Kafka BrokerMT-Channel-basedRabbitMQ Broker <p>The following commands install the Apache Kafka Broker and run event routing in a system namespace. The <code>knative-eventing</code> namespace is used by default.</p> <ol> <li> <p>Install the Kafka controller by running the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> <li> <p>Install the Kafka Broker data plane by running the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml\n</code></pre> </li> <li> <p>If you're upgrading from the previous version, run the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml\n</code></pre> </li> </ol> <p>For more information, see the Kafka Broker documentation.</p> <p>This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation.</p> <ul> <li> <p>Install this implementation of Broker by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml\n</code></pre> <p>To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-br-defaults\n  namespace: knative-eventing\ndata:\n  default-br-config: |\n    # This is the cluster-wide default broker channel.\n    clusterDefault:\n      brokerClass: MTChannelBasedBroker\n      apiVersion: v1\n      kind: ConfigMap\n      name: imc-channel\n      namespace: knative-eventing\n    # This allows you to specify different defaults per-namespace,\n    # in this case the \"some-namespace\" namespace will use the Kafka\n    # channel ConfigMap by default (only for example, you will need\n    # to install kafka also to make use of this).\n    namespaceDefaults:\n      some-namespace:\n        brokerClass: MTChannelBasedBroker\n        apiVersion: v1\n        kind: ConfigMap\n        name: kafka-channel\n        namespace: knative-eventing\n</code></pre> <p>The referenced <code>imc-channel</code> and <code>kafka-channel</code> example ConfigMaps would look like:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: imc-channel\n  namespace: knative-eventing\ndata:\n  channel-template-spec: |\n    apiVersion: messaging.knative.dev/v1\n    kind: InMemoryChannel\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-channel\n  namespace: knative-eventing\ndata:\n  channel-template-spec: |\n    apiVersion: messaging.knative.dev/v1alpha1\n    kind: KafkaChannel\n    spec:\n      numPartitions: 3\n      replicationFactor: 1\n</code></pre> </li> </ul> <p>Warning</p> <p>In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic.</p> <ul> <li>Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README.</li> </ul> <p>For more information, see the RabbitMQ Broker in GitHub.</p>"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-optional-eventing-extensions","title":"Install optional Eventing extensions","text":"<p>The following tabs expand to show instructions for installing each Eventing extension.</p> Apache Kafka SinkGitHub SourceApache Kafka SourceApache CouchDB SourceVMware Sources and Bindings <ol> <li> <p>Install the Kafka controller by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml\n</code></pre> </li> <li> <p>Install the Kafka Sink data plane by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml\n</code></pre> </li> </ol> <p>For more information, see the Kafka Sink documentation.</p> <p>A single-tenant GitHub source creates one Knative service per GitHub source.</p> <p>A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration.</p> <ul> <li> <p>To install a single-tenant GitHub source run the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml\n</code></pre> </li> <li> <p>To install a multi-tenant GitHub source run the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml\n</code></pre> </li> </ul> <p>To learn more, try the GitHub source sample</p> <ol> <li> <p>Install the Apache Kafka Source by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml\n</code></pre> </li> <li> <p>If you're upgrading from the previous version, run the following command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-post-install.yaml\n</code></pre> </li> </ol> <p>To learn more, try the Apache Kafka source sample.</p> <ul> <li> <p>Install the Apache CouchDB Source by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml\n</code></pre> </li> </ul> <p>To learn more, read the Apache CouchDB source documentation.</p> <ul> <li> <p>Install VMware Sources and Bindings by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml\n</code></pre> </li> </ul> <p>To learn more, try the VMware sources and bindings samples.</p>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/","title":"Installing Knative Serving using YAML files","text":"<p>This topic describes how to install Knative Serving by applying YAML files using the\u00a0<code>kubectl</code>\u00a0CLI.</p>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#prerequisites","title":"Prerequisites","text":"<p>Before installing Knative, you must meet the following prerequisites:</p> <ul> <li> <p>For prototyping purposes, Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 3\u00a0CPUs and 4\u00a0GB of memory.</p> <p>Tip</p> <p>You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin</p> </li> <li> <p>For production purposes, it is recommended that:</p> <ul> <li>If you have only one node in your cluster, you need at least 6\u00a0CPUs, 6\u00a0GB of memory, and 30\u00a0GB of disk storage.</li> <li>If you have multiple nodes in your cluster, for each node you need at least 2\u00a0CPUs, 4\u00a0GB of memory, and 20\u00a0GB of disk storage.</li> <li>You have a cluster that uses Kubernetes v1.28 or newer.</li> <li>You have installed the <code>kubectl</code> CLI.</li> <li>Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry.</li> </ul> </li> </ul> <p>Caution</p> <p>The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.</p>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#verifying-image-signatures","title":"Verifying image signatures","text":"<p>Knative releases from 1.9 onwards are signed with cosign.</p> <ol> <li> <p>Install cosign and jq.</p> </li> <li> <p>Extract the images from a manifeset and verify the signatures.</p> </li> </ol> <pre><code>curl -sSL https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml \\\n  | grep 'gcr.io/' | awk '{print $2}' | sort | uniq \\\n  | xargs -n 1 \\\n    cosign verify -o text \\\n      --certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n      --certificate-oidc-issuer=https://accounts.google.com\n</code></pre> <p>Note</p> <p>Knative images are signed in <code>KEYLESS</code> mode. To learn more about keyless signing, please refer to Keyless Signatures Our signing identity(Subject) for our releases is <code>signer@knative-releases.iam.gserviceaccount.com</code> and the Issuer is <code>https://accounts.google.com</code></p>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-the-knative-serving-component","title":"Install the Knative Serving component","text":"<p>To install the Knative Serving component:</p> <ol> <li> <p>Install the required custom resources by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml\n</code></pre> </li> <li> <p>Install the core components of Knative Serving by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml\n</code></pre> <p>Info</p> <p>For information about the YAML files in Knative Serving, see Knative Serving installation files.</p> </li> </ol>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-a-networking-layer","title":"Install a networking layer","text":"<p>The following tabs expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice:</p> Kourier (Choose this if you are not sure)IstioContour <p>The following commands install Kourier and enable its Knative integration.</p> <ol> <li> <p>Install the Knative Kourier controller by running the command: <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml\n</code></pre></p> </li> <li> <p>Configure Knative Serving to use Kourier by default by running the command:   <pre><code>kubectl patch configmap/config-network \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}'\n</code></pre></p> </li> <li> <p>Fetch the External IP address or CNAME by running the command:</p> <pre><code>kubectl --namespace kourier-system get service kourier\n</code></pre> <p>Tip</p> <p>Save this to use in the following Configure DNS section.</p> </li> </ol> <p>The following commands install Istio and enable its Knative integration.</p> <ol> <li> <p>Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command:</p> <pre><code>kubectl apply -l knative.dev/crd-install=true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml\nkubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml\n</code></pre> </li> <li> <p>Install the Knative Istio controller by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml\n</code></pre> </li> <li> <p>Fetch the External IP address or CNAME by running the command:</p> <pre><code>kubectl --namespace istio-system get service istio-ingressgateway\n</code></pre> <p>Tip</p> <p>Save this to use in the following Configure DNS section.</p> </li> </ol> <p>The following commands install Contour and enable its Knative integration.</p> <ol> <li> <p>Install a properly configured Contour by running the command:</p> <p><pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml\n</code></pre> </p> </li> <li> <p>Install the Knative Contour controller by running the command:   <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml\n</code></pre></p> </li> <li> <p>Configure Knative Serving to use Contour by default by running the command:   <pre><code>kubectl patch configmap/config-network \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}'\n</code></pre></p> </li> <li> <p>Fetch the External IP address or CNAME by running the command:</p> <pre><code>kubectl --namespace contour-external get service envoy\n</code></pre> <p>Tip</p> <p>Save this to use in the following Configure DNS section.</p> </li> </ol>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#verify-the-installation","title":"Verify the installation","text":"<p>Success</p> <p>Monitor the Knative components until all of the components show a <code>STATUS</code> of <code>Running</code> or <code>Completed</code>. You can do this by running the following command and inspecting the output:</p> <pre><code>kubectl get pods -n knative-serving\n</code></pre> <p>Example output:</p> <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\n3scale-kourier-control-54cc54cc58-mmdgq   1/1     Running   0          81s\nactivator-67656dcbbb-8mftq                1/1     Running   0          97s\nautoscaler-df6856b64-5h4lc                1/1     Running   0          97s\ncontroller-788796f49d-4x6pm               1/1     Running   0          97s\ndomain-mapping-65f58c79dc-9cw6d           1/1     Running   0          97s\ndomainmapping-webhook-cc646465c-jnwbz     1/1     Running   0          97s\nwebhook-859796bc7-8n5g2                   1/1     Running   0          96s\n</code></pre>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#configure-dns","title":"Configure DNS","text":"<p>You can configure DNS to prevent the need to run curl commands with a host header.</p> <p>The following tabs expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice:</p> Magic DNS (sslip.io)Real DNSNo DNS <p>Knative provides a Kubernetes Job called <code>default-domain</code> that configures Knative Serving to use sslip.io as the default DNS suffix.</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml\n</code></pre> <p>Warning</p> <p>This will only work if the cluster <code>LoadBalancer</code> Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless <code>minikube tunnel</code> is running.</p> <p>In these cases, see the \"Real DNS\" or \"No DNS\" tabs.</p> <p>To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows:</p> <ul> <li> <p>If the networking layer produced an External IP address, then configure a   wildcard <code>A</code> record for the domain:</p> <pre><code># Here knative.example.com is the domain suffix for your cluster\n*.knative.example.com == A 35.233.41.212\n</code></pre> </li> <li> <p>If the networking layer produced a CNAME, then configure a CNAME record for the domain:</p> <pre><code># Here knative.example.com is the domain suffix for your cluster\n*.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com\n</code></pre> </li> <li> <p>Once your DNS provider has been configured, direct Knative to use that domain:</p> <pre><code># Replace knative.example.com with your domain suffix\nkubectl patch configmap/config-domain \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"knative.example.com\":\"\"}}'\n</code></pre> </li> </ul> <p>If you are using <code>curl</code> to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (sslip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters.</p> <p>To access your application using <code>curl</code> using this method:</p> <ol> <li> <p>Configure Knative to use a domain reachable from outside the cluster:   <pre><code>kubectl patch configmap/config-domain \\\n      --namespace knative-serving \\\n      --type merge \\\n      --patch '{\"data\":{\"example.com\":\"\"}}'\n</code></pre></p> </li> <li> <p>After starting your application, get the URL of your application:   <pre><code>kubectl get ksvc\n</code></pre>   The output should be similar to:   <pre><code>NAME            URL                                        LATESTCREATED         LATESTREADY           READY   REASON\nhelloworld-go   http://helloworld-go.default.example.com   helloworld-go-vqjlf   helloworld-go-vqjlf   True\n</code></pre></p> </li> <li> <p>Instruct <code>curl</code> to connect to the External IP or CNAME defined by the    networking layer mentioned in section 3, and use the <code>-H \"Host:\"</code> command-line    option to specify the Knative application's host name.    For example, if the networking layer defines your External IP and port to be <code>http://192.168.39.228:32198</code> and you wish to access the <code>helloworld-go</code> application mentioned earlier, use:    <pre><code>curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198\n</code></pre>    In the case of the provided <code>helloworld-go</code> sample application, using the default configuration, the output is:    <pre><code>Hello Go Sample v1!\n</code></pre>    Refer to the \"Real DNS\" method for a permanent solution.</p> </li> </ol>"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-optional-serving-extensions","title":"Install optional Serving extensions","text":"<p>The following tabs expand to show instructions for installing each Serving extension.</p> HPA autoscalingKnative encryption with cert-manager <p>Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions.</p> <ul> <li> <p>Install the components needed to support HPA-class autoscaling by running the command:</p> <pre><code>kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml\n</code></pre> </li> </ul> <p>Knative supports encryption features through  cert-manager. Follow the documentation in Serving encryption for more information.</p>"},{"location":"install/yaml-install/serving/serving-installation-files/","title":"Knative Serving installation files","text":"<p>This guide provides reference information about the core Knative Serving YAML files, including:</p> <ul> <li>The custom resource definitions (CRDs) and core components required to install Knative Serving.</li> <li>Optional components that you can apply to customize your installation.</li> </ul> <p>For information about installing these files, see Installing Knative Serving using YAML files.</p> <p>The following table describes the installation files included in Knative Serving:</p> File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://sslip.io as the default DNS suffix. serving-core.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing <code>serving-core.yaml</code>. Currently it is the same as <code>serving-storage-version-migration.yaml</code>. serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from <code>v1alpha1</code> and <code>v1beta1</code> to <code>v1</code>. Required by upgrade from version 0.18 to 0.19. serving-core.yaml"},{"location":"reference/relnotes/","title":"Knative release notes","text":"<p>For details about the Knative releases, see the following pages:</p> <ul> <li>Knative CLI releases</li> <li>Knative Functions releases</li> <li>Knative Eventing releases</li> <li>Knative Serving releases</li> <li>Knative Operator releases</li> </ul>"},{"location":"reference/security/","title":"Knative Security and Disclosure Information","text":"<p>This page describes Knative security and disclosure information.</p>"},{"location":"reference/security/#knative-threat-model","title":"Knative threat model","text":"<ul> <li>Threat model</li> </ul>"},{"location":"reference/security/#code-signature-verification","title":"Code Signature Verification","text":""},{"location":"reference/security/#all-platforms","title":"All platforms","text":"<p>Our releases from 1.9 are signed with cosign. You can use the following steps to verify our binaries.</p> <ol> <li>Download the files you want, and the <code>checksums.txt</code>, <code>checksum.txt.pem</code> and <code>checksums.txt.sig</code> files from the releases page:     <pre><code># this example verifies the 1.10.0 kn cli from the knative/client repository\nwget https://github.com/knative/client/releases/download/knative-v1.10.0/checksums.txt\nwget https://github.com/knative/client/releases/download/knative-v1.10.0/kn-darwin-amd64\nwget https://github.com/knative/client/releases/download/knative-v1.10.0/checksums.txt.sig\nwget https://github.com/knative/client/releases/download/knative-v1.10.0/checksums.txt.pem\n</code></pre></li> <li>Verify the signature:     <pre><code>cosign verify-blob \\\n--certificate-identity=signer@knative-releases.iam.gserviceaccount.com \\\n--certificate-oidc-issuer=https://accounts.google.com \\\n--cert checksums.txt.pem \\\n--signature checksums.txt.sig \\\nchecksums.txt\n</code></pre></li> <li>If the signature is valid, you can then verify the SHA256 sums match with the downloaded binary:     <pre><code>sha256sum --ignore-missing -c checksums.txt\n</code></pre></li> </ol> <p>Note</p> <p>Knative images are signed in <code>KEYLESS</code> mode. To learn more about keyless signing, please refer to Keyless Signatures Our signing identity(Subject) for our releases is <code>signer@knative-releases.iam.gserviceaccount.com</code> and the Issuer is <code>https://accounts.google.com</code></p>"},{"location":"reference/security/#apple-macos","title":"Apple macOS","text":"<p>In addition to signing our binaries with <code>cosign</code>, we notarize our macOS binaries. You can use the <code>codesign</code> utility to verify our binaries from 1.9 release. You should expect an output that looks like this. The expected TeamIdentifier is <code>7R64489VHL</code></p> <pre><code>codesign --verify -d --verbose=2 ~/Downloads/kn-quickstart-darwin-amd64\n\nExecutable=/Users/REDACTED/Downloads/kn-quickstart-darwin-amd64\nIdentifier=kn-quickstart-darwin-amd64\n...\nAuthority=Developer ID Application: Mahamed Ali (7R64489VHL)\nAuthority=Developer ID Certification Authority\nAuthority=Apple Root CA\nTimestamp=3 Oct 2022 at 22:50:07\n...\nTeamIdentifier=7R64489VHL\n</code></pre>"},{"location":"reference/security/#report-a-vulnerability","title":"Report a vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the Knative Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>To make a report, please email the private security@knative.team list with the security details and the details expected for all Knative bug reports.</p>"},{"location":"reference/security/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in Knative</li> <li>You are unsure how a vulnerability affects Knative</li> <li>You think you discovered a vulnerability in another project that Knative depends on<ul> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul> </li> </ul>"},{"location":"reference/security/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning Knative components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"reference/security/#vulnerability-response","title":"Vulnerability response","text":"<ul> <li>Early disclosure of security vulnerabilities</li> <li>Vulnerability disclosure response policy</li> </ul>"},{"location":"reference/security/#security-working-group","title":"Security working group","text":"<ul> <li>General information</li> <li>Security Working Group Charter</li> </ul>"},{"location":"samples/","title":"Knative code samples","text":"<p>You can use Knative code samples to help you get up and running with common use cases.</p>"},{"location":"samples/#knative-owned-samples","title":"Knative owned samples","text":"<p>Knative code samples that are actively tested and maintained by Knative working groups:</p> <ul> <li>Eventing and Eventing Sources code samples</li> <li>Serving code samples</li> </ul>"},{"location":"samples/#community-owned-samples","title":"Community owned samples","text":"<p>Get up and running with one of the community code samples. These samples are contributed and maintained by members of the Knative community. View code samples that are contributed and maintained by the community.</p> <p>Note: These samples might become outdated or the original author might be unable to maintain their contribution. If you find that something isn't working, lend a helping hand and fix it in a PR.</p> <p>Learn more about the lifespan of samples</p> Sample Name Description Language(s) Hello World A quick introduction to Knative Serving that highlights how to deploy an app. Clojure, Dart, Elixir, Haskell, Java - Micronaut, Java - Quarkus, R - Go Server, Rust, Swift, Vertx Machine Learning A quick introduction to using Knative Serving to serve machine learning models Python - BentoML"},{"location":"samples/#external-code-samples","title":"External code samples","text":"<p>A list of links to Knative code samples located outside of Knative repos:</p> <ul> <li>Image processing using Knative Eventing, Cloud Run on GKE and Google Cloud Vision API</li> <li>Knative Eventing Examples</li> <li>knfun</li> <li>Getting Started with Knative 2020</li> <li>Image Processing Pipeline</li> <li>BigQuery Processing Pipeline</li> <li>Simple Performance Testing with SLOs</li> </ul> <p>Tip</p> <p>Add a link here to your externally hosted Knative code sample.</p>"},{"location":"samples/eventing/","title":"Knative Eventing code samples","text":"<p>Use the following code samples to help you understand the various use cases for Knative Eventing and Event Sources. Learn more about Knative Eventing and Eventing Sources.</p> <p>See all Knative code samples in GitHub.</p> Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative. Go and Python GitHub source Shows how to wire GitHub events for consumption by a Knative Service. YAML GitLab source Shows how to wire GitLab events for consumption by a Knative Service. YAML Apache Kafka Channel Install and configure the Apache Kafka Channel as the default Channel configuration for Knative Eventing. YAML Writing an event source using JavaScript This tutorial provides instructions to build an event source in JavaScript and implement it with a ContainerSource or SinkBinding. JavaScript Parallel with multiple cases Create a Parallel with two branches. YAML Parallel with mutually exclusive cases Create a Parallel with mutually exclusive branches. YAML"},{"location":"samples/serving/","title":"Knative Serving code samples","text":"<p>Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving.</p> <p>See all Knative code samples in GitHub.</p> Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative Serving. C#, Go, Java (Spark), Java (Spring), Kotlin, Node.js, PHP, Python, Ruby, Scala, Shell Cloud Events A quick introduction that highlights how to send and receive Cloud Events. C#, Go, Node.js, Rust, Java (Vert.x) Traffic Splitting An example of manual traffic splitting. YAML Advanced Deployment Simple blue/green-like application deployment pattern illustrating the process of updating a live application without dropping any traffic. YAML Autoscale A demonstration of the autoscaling capabilities of Knative. Go Github Webhook A simple webhook handler that demonstrates interacting with Github. Go gRPC A simple gRPC server. Go Knative Routing An example of mapping multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Go Kong Routing An example of mapping multiple Knative services to different paths under a single domain name using the Kong API gateway. Go Knative Secrets A simple app that demonstrates how to use a Kubernetes secret as a Volume in Knative. Go Multi Container A quick introduction that highlights how to build and deploy an app using Knative Serving for multiple containers. Go"},{"location":"serving/","title":"Knative Serving","text":"<p>Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These resources are used to define and control how your serverless workload behaves on the cluster.</p> <p></p> <p>The primary Knative Serving resources are Services, Routes, Configurations, and Revisions:</p> <ul> <li> <p>Services:   The <code>service.serving.knative.dev</code> resource automatically manages the whole   lifecycle of your workload. It controls the creation of other objects to   ensure that your app has a route, a configuration, and a new revision for each   update of the service. Service can be defined to always route traffic to the   latest revision or to a pinned revision.</p> </li> <li> <p>Routes:   The <code>route.serving.knative.dev</code> resource maps a network endpoint to one or   more revisions. You can manage the traffic in several ways, including   fractional traffic and named routes.</p> </li> <li> <p>Configurations:   The <code>configuration.serving.knative.dev</code> resource maintains the desired state   for your deployment. It provides a clean separation between code and   configuration and follows the Twelve-Factor App methodology. Modifying a   configuration creates a new revision.</p> </li> <li> <p>Revisions:   The <code>revision.serving.knative.dev</code> resource is a point-in-time snapshot of the   code and configuration for each modification made to the workload. Revisions   are immutable objects and can be retained for as long as useful. Knative   Serving Revisions can be automatically scaled up and down according to   incoming traffic.</p> </li> </ul> <p>For more information on the resources and their interactions, see the Resource Types Overview in the <code>serving</code> Github repository.</p>"},{"location":"serving/#common-use-cases","title":"Common use cases","text":"<p>Examples of supported Knative Serving use cases:</p> <ul> <li>Rapid deployment of serverless containers.</li> <li>Autoscaling, including scaling pods down to zero.</li> <li>Support for multiple networking layers, such as Contour, Kourier, and Istio, for integration into existing environments.</li> </ul> <p>Knative Serving supports both HTTP and HTTPS networking protocols.</p>"},{"location":"serving/#installation","title":"Installation","text":"<p>You can install Knative Serving via the methods listed on the installation page.</p>"},{"location":"serving/#getting-started","title":"Getting Started","text":"<p>To get started with Serving, check out one of the hello world sample projects. These projects use the <code>Service</code> resource, which manages all the details for you.</p> <p>With the <code>Service</code> resource, a deployed service will automatically have a matching route and configuration created. Each time the <code>Service</code> is updated, a new revision is created.</p>"},{"location":"serving/#more-samples-and-demos","title":"More samples and demos","text":"<ul> <li>Knative Serving code samples</li> </ul>"},{"location":"serving/#debugging-knative-serving-issues","title":"Debugging Knative Serving issues","text":"<ul> <li>Debugging application issues</li> </ul>"},{"location":"serving/#configuration-and-networking","title":"Configuration and Networking","text":"<ul> <li>Configuring cluster local routes</li> <li>Using a custom domain</li> <li>Traffic management</li> </ul>"},{"location":"serving/#observability","title":"Observability","text":"<ul> <li>Serving Metrics API</li> </ul>"},{"location":"serving/#known-issues","title":"Known Issues","text":"<p>See the Knative Serving Issues page for a full list of known issues.</p>"},{"location":"serving/accessing-traces/","title":"Accessing request traces","text":"<p>Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests.</p>"},{"location":"serving/accessing-traces/#configuring-traces","title":"Configuring Traces","text":"<p>You can update the configuration file for tracing in tracing.yaml.</p> <p>Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or none).</p> <p>You can quickly explore and update the ConfigMap object with the following command: <pre><code>kubectl -n knative-serving edit configmap config-tracing\n</code></pre></p>"},{"location":"serving/accessing-traces/#zipkin","title":"Zipkin","text":"<p>In order to access request traces, you use the Zipkin visualization tool.</p> <ol> <li> <p>To open the Zipkin UI, enter the following command:</p> <pre><code>kubectl proxy\n</code></pre> <p>This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster.</p> </li> <li> <p>Access the Zipkin UI at the following URL:</p> <p><pre><code>http://localhost:8001/api/v1/namespaces/&lt;namespace&gt;/services/zipkin:9411/proxy/zipkin/\n</code></pre> Where <code>&lt;namespace&gt;</code> is the namespace where Zipkin is deployed, for example, <code>knative-serving</code>. 1.  Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call.</p> </li> </ol>"},{"location":"serving/accessing-traces/#jaeger","title":"Jaeger","text":"<p>In order to access request traces, you use the Jaeger visualization tool.</p> <ol> <li> <p>To open the Jaeger UI, enter the following command:</p> <pre><code>kubectl proxy\n</code></pre> <p>This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster.</p> </li> <li> <p>Access the Jaeger UI at the following URL:</p> <p><pre><code>http://localhost:8001/api/v1/namespaces/&lt;namespace&gt;/services/jaeger-query:16686/proxy/search/\n</code></pre> Where <code>&lt;namespace&gt;</code> is the namespace where Jaeger is deployed, for example, <code>knative-serving</code>.</p> </li> <li> <p>Select the service of interest and click \"Find Traces\" to see the latest     traces. Click on a trace to see a detailed view of a specific call.</p> </li> </ol>"},{"location":"serving/architecture/","title":"Knative Serving Architecture","text":"<p>Knative Serving consists of several components forming the backbone of the Serverless Platform. This page explains the high-level architecture of Knative Serving. Please also refer to the Knative Serving Overview  and the Request Flow for additional information.</p>"},{"location":"serving/architecture/#diagram","title":"Diagram","text":""},{"location":"serving/architecture/#components","title":"Components","text":"Component Responsibilities Activator The activator is part of the data-plane. It is responsible to queue incoming requests (if a <code>Knative Service</code> is scaled-to-zero). It communicates with the autoscaler to bring scaled-to-zero Services back up and forward the queued requests. Activator can also act as a request buffer to handle traffic bursts. Additional details can be found here. Autoscaler The autoscaler is responsible to scale the Knative Services based on configuration, metrics and incoming requests. Controller The controller manages the state of Knative resources within the cluster. It watches several objects, manages the lifecycle of dependent resources, and updates the resource state. Queue-Proxy The Queue-Proxy is a sidecar container in the Knative Service's Pod. It is responsible to collect metrics and enforcing the desired concurrency when forwarding requests to the user's container. It can also act as a queue if necessary, similar to the Activator. Webhooks Knative Serving has several webhooks responsible to validate and mutate Knative Resources."},{"location":"serving/architecture/#networking-layer-and-ingress","title":"Networking Layer and Ingress","text":"<p>Note</p> <p><code>Ingress</code> in this case, does not refer to the Kubernetes Ingress Resource. It refers to the concept of exposing external access to a resource on the cluster. </p> <p>Knative Serving depends on a <code>Networking Layer</code> that fulfils the Knative Networking Specification.  For this, Knative Serving defines an internal <code>KIngress</code> resource, which acts as an abstraction for different multiple pluggable networking layers. Currently, three networking layers are available and supported by the community:</p> <ul> <li>net-kourier</li> <li>net-contour</li> <li>net-istio</li> </ul>"},{"location":"serving/architecture/#traffic-flow-and-dns","title":"Traffic flow and DNS","text":"<p>Note</p> <p>There are fine differences between the different networking layers, the following section describes the general concept. Also, there are multiple ways to expose your <code>Ingress Gateway</code> and configure DNS. Please refer the installation documentation for more information.</p> <p></p> <ul> <li>Each networking layer has a controller that is responsible to watch the <code>KIngress</code> resources and configure the <code>Ingress Gateway</code> accordingly. It will also report back <code>status</code> information through this resource.</li> <li>The <code>Ingress Gateway</code> is used to route requests to the <code>activator</code> or directly to a Knative Service Pod, depending on the mode (proxy/serve, see here for more details). The <code>Ingress Gateway</code> is handling requests  from inside the cluster and from outside the cluster.</li> <li>For the <code>Ingress Gateway</code> to be reachable outside the cluster, it must be exposed using a Kubernetes Service of <code>type: LoadBalancer</code> or <code>type: NodePort</code>. The community supported networking layers include this as part of the installation. Then DNS is configured to point to the <code>IP</code> or <code>Name</code> of the <code>Ingress Gateway</code></li> </ul> <p>Note</p> <p>Please note, if you do use/set DNS, you should also set the same domain for Knative.</p>"},{"location":"serving/architecture/#autoscaling","title":"Autoscaling","text":"<p>You can find more detailed information on our autoscaling mechanism here.</p>"},{"location":"serving/config-ha/","title":"Configuring high-availability components","text":"<p>Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable.</p> <p>When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader.</p> <p>Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica.</p>"},{"location":"serving/config-ha/#disabling-leader-election","title":"Disabling leader election","text":"<p>For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: <code>--disable-ha</code>.  This option will go away when HA graduates to \"stable\".</p>"},{"location":"serving/config-ha/#scaling-the-control-plane","title":"Scaling the control plane","text":"<p>With the exception of the <code>activator</code> component you can scale up any deployment running in <code>knative-serving</code> (or <code>kourier-system</code>) with a command like:</p> <pre><code>$ kubectl -n knative-serving scale deployment &lt;deployment-name&gt; --replicas=2\n</code></pre> <ul> <li>Setting <code>--replicas</code> to a value of <code>2</code> enables HA.</li> <li>You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 <code>controller</code> deployments, set <code>--replicas=3</code>.</li> <li>Setting <code>--replicas=1</code> disables HA.</li> </ul> <p>Note</p> <p>If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the <code>stable-window</code> value. This is because when an <code>autoscaler</code> pod is terminating, ownership of the revisions belonging to that pod is passed to other <code>autoscaler</code> pods that are on stand by. The <code>autoscaler</code> pods that take over ownership of those revisions use the <code>stable-window</code> time to build the scaling metrics state for those Revisions.</p>"},{"location":"serving/config-ha/#scaling-the-data-plane","title":"Scaling the data plane","text":"<p>The scale of the <code>activator</code> component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running:</p> <pre><code>$ kubectl get hpa activator -n knative-serving\n</code></pre> <p>The output looks similar to the following:</p> <pre><code>NAME        REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nactivator   Deployment/activator   2%/100%   5         15        11         346d\n</code></pre> <p>By default <code>minReplicas</code> and <code>maxReplicas</code> are set to <code>1</code> and <code>20</code>, correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to <code>minScale=9</code> and <code>maxScale=19</code> using the following command:</p> <pre><code>$ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}'\n</code></pre> <p>To set the activator scale to a particular value, just set <code>minScale</code> and <code>maxScale</code> to the same desired value.</p> <p>It is recommended for production deployments to run at least 3 <code>activator</code> instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.</p>"},{"location":"serving/convert-deployment-to-knative-service/","title":"Converting a Kubernetes Deployment to a Knative Service","text":"<p>This topic shows how to convert a Kubernetes Deployment to a Knative Service.</p>"},{"location":"serving/convert-deployment-to-knative-service/#benefits","title":"Benefits","text":"<p>Converting to a Knative Service has the following benefits:</p> <ul> <li>Reduces the footprint of the service instance because the instance scales to 0 when it becomes idle.</li> <li>Improves performance due to built-in autoscaling for the Knative Service.</li> </ul>"},{"location":"serving/convert-deployment-to-knative-service/#determine-if-your-workload-is-a-good-fit-for-knative","title":"Determine if your workload is a good fit for Knative","text":"<p>In general, if your Kubernetes workload is a good fit for Knative, you can remove a lot of your manifest to create a Knative Service.</p> <p>There are three aspects you need to consider:</p> <ul> <li>All work done is triggered by HTTP.</li> <li>The container is stateless. All state is stored elsewhere or can be re-created.</li> <li>Your workload uses only Secret and ConfigMap volumes.</li> </ul>"},{"location":"serving/convert-deployment-to-knative-service/#example-conversion","title":"Example conversion","text":"<p>The following example shows a Kubernetes Nginx Deployment and Service, and shows how it converts to a Knative Service.</p>"},{"location":"serving/convert-deployment-to-knative-service/#kubernetes-nginx-deployment-and-service","title":"Kubernetes Nginx Deployment and Service","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx\n  labels:\n    run: my-nginx\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n  selector:\n    run: my-nginx\n</code></pre>"},{"location":"serving/convert-deployment-to-knative-service/#knative-service","title":"Knative Service","text":"<pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: my-nginx\nspec:\n  template:\n    spec:\n      containers:\n      - image: nginx\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"serving/deploying-from-private-registry/","title":"Deploying images from a private container registry","text":"<p>You can configure your Knative cluster to deploy images from a private registry across multiple Services and Revisions. To do this, you must create a list of Kubernetes secrets (<code>imagePullSecrets</code>) by using your registry credentials. You must then add those secrets to the default service account for all Services, or the Revision template for a single Service.</p>"},{"location":"serving/deploying-from-private-registry/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have a Kubernetes cluster with Knative Serving installed.</li> <li>You must have access to credentials for the private container registry where your container images are stored.</li> </ul>"},{"location":"serving/deploying-from-private-registry/#procedure","title":"Procedure","text":"<ol> <li> <p>Create a <code>imagePullSecrets</code> object that contains your credentials as a list of secrets:</p> <pre><code>kubectl create secret docker-registry &lt;registry-credential-secrets&gt; \\\n  --docker-server=&lt;private-registry-url&gt; \\\n  --docker-email=&lt;private-registry-email&gt; \\\n  --docker-username=&lt;private-registry-user&gt; \\\n  --docker-password=&lt;private-registry-password&gt;\n</code></pre> <p>Where:</p> <ul> <li> <p><code>&lt;registry-credential-secrets&gt;</code> is the name that you want to use for your secrets (the <code>imagePullSecrets</code> object). For example, <code>container-registry</code>.</p> </li> <li> <p><code>&lt;private-registry-url&gt;</code> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub.</p> </li> <li> <p><code>&lt;private-registry-email&gt;</code> is the email address that is associated with   the private registry.</p> </li> <li> <p><code>&lt;private-registry-user&gt;</code> is the username that you use to access the   private container registry.</p> </li> <li> <p><code>&lt;private-registry-password&gt;</code> is the password that you use to access   the private container registry.</p> </li> </ul> <p>Example:</p> <pre><code>kubectl create secret docker-registry container-registry \\\n  --docker-server=https://gcr.io/ \\\n  --docker-email=my-account-email@address.com \\\n  --docker-username=my-grc-username \\\n  --docker-password=my-gcr-password\n</code></pre> </li> <li> <p>Optional. After you have created the <code>imagePullSecrets</code> object, you can view the secrets by running:</p> <pre><code>kubectl get secret &lt;registry-credential-secrets&gt; -o=yaml\n</code></pre> </li> <li> <p>Optional. Add the <code>imagePullSecrets</code> object to the <code>default</code> service account in the <code>default</code> namespace.</p> <p>Note</p> <p>By default, the <code>default</code> service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the <code>serviceAccountName</code> is specified.</p> <p>For example, if have you named your secrets <code>container-registry</code>, you can run the following command to modify the <code>default</code> service account:</p> <pre><code>kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\"\n</code></pre> <p>New pods that are created in the <code>default</code> namespace now include your credentials and have access to your container images in the private registry.</p> </li> <li> <p>Optional. Add the <code>imagePullSecrets</code> object to a Service:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: hello\nspec:\n  template:\n    spec:\n      imagePullSecrets:\n      - name: &lt;secret-name&gt;\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          ports:\n            - containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"World\"\n</code></pre> </li> </ol>"},{"location":"serving/istio-authorization/","title":"Enabling requests to Knative services when additional authorization policies are enabled","text":"<p>Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods.</p>"},{"location":"serving/istio-authorization/#before-you-begin","title":"Before you begin","text":"<p>You must meet the following prerequisites to use Istio AuthorizationPolicy:</p> <ul> <li>Istio must be used for your Knative Ingress. See Install a networking layer.</li> <li>Istio sidecar injection must be enabled. See the Istio Documentation.</li> </ul>"},{"location":"serving/istio-authorization/#mutual-tls-in-knative","title":"Mutual TLS in Knative","text":"<p>Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS.</p> <p></p> <p>Generally, mutual TLS can be configured normally as in Istio's documentation. However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the <code>knative-serving</code> namespace:</p> <pre><code>kubectl label namespace knative-serving istio-injection=enabled\n</code></pre> <p>If the activator isn't injected:</p> <ul> <li> <p>In PERMISSIVE mode, you'll see requests appear without the expected <code>X-Forwarded-Client-Cert</code> header when forwarded by the activator.</p> <pre><code>$ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers\n{\n  \"headers\": {\n    \"Accept\": \"*/*\",\n    \"Accept-Encoding\": \"gzip\",\n    \"Forwarded\": \"for=10.72.0.30;proto=http\",\n    \"Host\": \"httpbin.knative.svc.cluster.local\",\n    \"K-Proxy-Request\": \"activator\",\n    \"User-Agent\": \"curl/7.58.0\",\n    \"X-B3-Parentspanid\": \"b240bdb1c29ae638\",\n    \"X-B3-Sampled\": \"0\",\n    \"X-B3-Spanid\": \"416960c27be6d484\",\n    \"X-B3-Traceid\": \"750362ce9d878281b240bdb1c29ae638\",\n    \"X-Envoy-Attempt-Count\": \"1\",\n    \"X-Envoy-Internal\": \"true\"\n  }\n}\n</code></pre> </li> <li> <p>In STRICT mode, requests will simply be rejected.</p> </li> </ul> <p>To understand when requests are forwarded through the activator, see the target burst capacity documentation.</p> <p>This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator.</p> <p>For example, the following policy allows requests from within pods in the <code>serving-tests</code> namespace to other pods in the <code>serving-tests</code> namespace.</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n name: allow-serving-tests\n namespace: serving-tests\nspec:\n action: ALLOW\n rules:\n - from:\n   - source:\n      namespaces: [\"serving-tests\"]\n</code></pre> <p>Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as <code>knative-serving</code>, which is the namespace of the activator.</p> <p>Currently, the easiest way around this is to explicitly allow requests from the <code>knative-serving</code> namespace, for example by adding it to the list in the policy mentioned earlier:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n name: allow-serving-tests\n namespace: serving-tests\nspec:\n action: ALLOW\n rules:\n - from:\n   - source:\n      namespaces: [\"serving-tests\", \"knative-serving\"]\n</code></pre>"},{"location":"serving/istio-authorization/#health-checking-and-metrics-collection","title":"Health checking and metrics collection","text":"<p>In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths.</p>"},{"location":"serving/istio-authorization/#allowing-access-from-system-pods-by-paths","title":"Allowing access from system pods by paths","text":"<p>Knative system pods access your application using the following paths:</p> <ul> <li><code>/metrics</code></li> <li><code>/healthz</code></li> </ul> <p>The <code>/metrics</code> path allows the autoscaler pod to collect metrics. The <code>/healthz</code> path allows system pods to probe the service.</p> <p>To add the <code>/metrics</code> and <code>/healthz</code> paths to the AuthorizationPolicy:</p> <ol> <li> <p>Create a YAML file for your AuthorizationPolicy using the following example:</p> <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allowlist-by-paths\n  namespace: serving-tests\nspec:\n  action: ALLOW\n  rules:\n  - to:\n    - operation:\n        paths:\n        - /metrics   # The path to collect metrics by system pod.\n        - /healthz   # The path to probe by system pod.\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"serving/knative-kubernetes-services/","title":"Kubernetes services","text":"<p>This guide describes the Kubernetes Services that are active when running Knative Serving.</p>"},{"location":"serving/knative-kubernetes-services/#before-you-begin","title":"Before You Begin","text":"<ol> <li>This guide assumes that you have installed Knative Serving.</li> <li> <p>Verify that you have the proper components in your cluster. To view the    services installed in your cluster, use the command:</p> <pre><code>$ kubectl get services -n knative-serving\n</code></pre> <p>This returns an output similar to the following:</p> <pre><code>NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\nactivator-service   ClusterIP   10.96.61.11      &lt;none&gt;        80/TCP,81/TCP,9090/TCP   1h\nautoscaler          ClusterIP   10.104.217.223   &lt;none&gt;        8080/TCP,9090/TCP        1h\ncontroller          ClusterIP   10.101.39.220    &lt;none&gt;        9090/TCP                 1h\nwebhook             ClusterIP   10.107.144.50    &lt;none&gt;        443/TCP                  1h\n</code></pre> </li> <li> <p>To view the deployments in your cluster, use the following command:</p> <pre><code>$ kubectl get deployments -n knative-serving\n</code></pre> <p>This returns an output similar to the following:</p> <pre><code>NAME                         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nactivator                    1         1         1            1           1h\nautoscaler                   1         1         1            1           1h\ncontroller                   1         1         1            1           1h\nnet-certmanager-controller   1         1         1            1           1h\nnet-istio-controller         1         1         1            1           1h\nwebhook                      1         1         1            1           1h\n</code></pre> </li> </ol> <p>These services and deployments are installed by the <code>serving.yaml</code> file during install. The next section describes their function.</p>"},{"location":"serving/knative-kubernetes-services/#components","title":"Components","text":""},{"location":"serving/knative-kubernetes-services/#service-activator","title":"Service: activator","text":"<p>The activator is responsible for receiving &amp; buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics.</p>"},{"location":"serving/knative-kubernetes-services/#service-autoscaler","title":"Service: autoscaler","text":"<p>The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic.</p>"},{"location":"serving/knative-kubernetes-services/#service-controller","title":"Service: controller","text":"<p>The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs).</p>"},{"location":"serving/knative-kubernetes-services/#service-webhook","title":"Service: webhook","text":"<p>The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsistent and invalid objects, and validates and mutates Kubernetes API calls.</p>"},{"location":"serving/knative-kubernetes-services/#deployment-net-certmanager-controller","title":"Deployment: net-certmanager-controller","text":"<p>The certmanager reconciles cluster ingresses into cert manager objects.</p>"},{"location":"serving/knative-kubernetes-services/#deployment-net-istio-controller","title":"Deployment: net-istio-controller","text":"<p>The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service.</p>"},{"location":"serving/knative-kubernetes-services/#whats-next","title":"What's Next","text":"<ul> <li>For a deeper look at the services and deployments involved in Knative Serving,   see the specs repository.</li> <li>For a high-level analysis of Knative Serving, see the Knative Serving overview.</li> <li>For hands-on tutorials, see the Knative Serving code samples.</li> </ul>"},{"location":"serving/queue-extensions/","title":"Extending Queue Proxy image with QPOptions","text":"<p>Knative service pods include two containers:</p> <ul> <li>The user main service container, which is named <code>user-container</code></li> <li>The Queue Proxy - a sidecar named <code>queue-proxy</code> that serves as a reverse proxy in front of the <code>user-container</code></li> </ul> <p>You can extend Queue Proxy to offer additional features. The QPOptions feature of Queue Proxy allows additional runtime packages to extend Queue Proxy capabilities.</p> <p>For example, the security-guard repository provides an extension that uses the QPOptions feature. The QPOption package enables users to add additional security features to Queue Proxy.</p> <p>The runtime features available are determined when the Queue Proxy image is built. Queue Proxy defines an orderly manner to activate and to configure extensions.</p>"},{"location":"serving/queue-extensions/#additional-information","title":"Additional information","text":"<ul> <li>Enabling Queue Proxy Pod Info - discussing a necessary step to enable the use of extensions.</li> <li>Using extensions enabled by QPOptions - discussing how to configure a service to use features implemented in extensions.</li> </ul>"},{"location":"serving/queue-extensions/#adding-extensions","title":"Adding extensions","text":"<p>You can add extensions by replacing the <code>cmd/queue/main.go</code> file before the Queue Proxy image is built. The following example shows a <code>cmd/queue/main.go</code> file that adds the <code>test-gate</code> extension:</p> <pre><code>  package main\n\n  import \"os\"\n\n  import \"knative.dev/serving/pkg/queue/sharedmain\"\n  import \"knative.dev/security-guard/pkg/qpoption\"\n  import _ \"knative.dev/security-guard/pkg/test-gate\"\n\n  func main() {\n      qOpt := qpoption.NewQPSecurityPlugs()\n      defer qOpt.Shutdown()\n\n        if sharedmain.Main(qOpt.Setup) != nil {\n          os.Exit(1)\n      }\n  }\n</code></pre>"},{"location":"serving/request-flow/","title":"HTTP Request Flows","text":"<p>While the overview describes the logical components and the architecture describes the over all architecture of Knative Serving, this page explains the behavior and flow of HTTP requests to an application which is running on Knative Serving.</p> <p>The following diagram shows the different request flows and control plane loops for Knative Serving.  Note that some components, such as the autoscaler and the apiserver are not updated on every request, but instead measure the system periodically (this is referred to as the control plane).</p> <p></p> <p>The HTTP router, activator, and autoscaler are all shared cluster-level resources; this reduces the overhead for a new Knative service to only metadata when the Service is not in use and allows more efficient management and upgrade of these shared components.</p> <p>Routing decisions are made once on a per-request level at the HTTP Router (the pluggable ingress layer), and are recorded on the request in an internal header. Once a request has been assigned to a Revision, the subsequent routing depends on the measured traffic flow; at low or zero traffic, incoming requests are routed to the activator, while at high traffic levels (spare capacity greater than <code>target-burst-capacity</code>) traffic is routed directly to the application pods.</p>"},{"location":"serving/request-flow/#scale-from-zero","title":"Scale From Zero","text":"<p>When there is low or zero traffic to a particular Revision, the HTTP router sends traffic to the activator, including a header indicating the selected Revision.  The activator serves as a buffer or queue for incoming requests -- if a request is routed to a Revision which does not currently have available capacity, the activator delays the request and signals to the autoscaler that additional capacity is needed.</p> <p>When the autoscaler detects that the available capacity for a Revision is below the requested capacity, it increases the number of pods requested from Kubernetes.</p> <p>When these new pods become ready or an existing pod has capacity, the activator will forward the delayed request to a ready pod.  If a new pod needs to be started to handle a request, this is called a cold-start.</p>"},{"location":"serving/request-flow/#high-scale","title":"High scale","text":"<p>When a Revision has a high amount of traffic (the spare capacity is greater than <code>target-burst-capacity</code>), the ingress router is programmed directly with the pod adresses of the Revision, and the activator is removed from the traffic flow.  This reduces latency and increases efficiency when the additional buffering of the activator is not needed.  In all cases, the queue-proxy remains in the request path.</p> <p>If traffic drops below the burst capacity threshold (calculated as: <code>current_demand + target-burst-capacity &gt; (pods * concurrency-target)</code>), then the ingress router will be re-programmed to send traffic to the activator pods.</p> <p>Routing traffic between ingress and activators is done by writing a subset of the activator endpoints into the Endpoints for the Revision's Service.  The Kubernetes Service corresponding to the Revision is selectorless, and may either contain the Revision's pod endpoints or the activator endpoints.  The use of a selectorless service is for the following reasons:</p> <ul> <li> <p>Some ingress implementations do not allow cross-namespace service references.   The activator runs in the <code>knative-serving</code> namespace.</p> </li> <li> <p>Some ingress implementations do not handle changing the backing Kubernetes   Service of a routing endpoint seamlessly.</p> </li> <li> <p>By using subsetting on a per-Revision basis, incoming requests are funneled to   a small number of activators which can more efficiently make ready/not-ready   capacity decisions.  The small number of effective activators per Revision is   not a scaling problem because the activators will be removed from the request   path when the Revision scales up to receive more traffic.</p> </li> </ul>"},{"location":"serving/request-flow/#queue-proxy","title":"Queue-Proxy","text":"<p>The queue-proxy component implements a number of features to improve the reliability and scaling of Knative:</p> <ul> <li> <p>Measures concurrent requests for the autoscaler, particularly when the   activator is removed from the request path.</p> </li> <li> <p>Implements the <code>containerConcurrency</code> hard limit on request   concurrency   if requested.</p> </li> <li> <p>Handles graceful shutdown on Pod termination (refuse new requests, fail   readiness checks, continue serving existing requests).</p> </li> <li> <p>Reports HTTP metrics and traces from just outside the user container, so that   the infrastructure latency contribution can be measured.</p> </li> <li> <p>During startup (before ready), probes the user container more aggressively to   enable earlier serving that Kubelet probes (which can probe at most once per   second).</p> </li> <li> <p>To support this functionality, Knative Serving rewrites the user-container's     <code>readinessProbe</code> to an argument to queue-proxy; the queue-proxy's readiness     check incorporates both queue-proxy's own readiness and the     user-container's.</p> </li> </ul>"},{"location":"serving/rolling-out-latest-revision/","title":"Configuring gradual rollout of traffic to Revisions","text":"<p>If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision.</p> <p>This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP.</p> <p>Knative provides a <code>rollout-duration</code> parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic.</p> <p>Note</p> <p><code>rollout-duration</code> is time-based, and does not interact with the autoscaling subsystem.</p> <p>This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.</p>"},{"location":"serving/rolling-out-latest-revision/#procedure","title":"Procedure","text":"<p>You can configure the <code>rollout-duration</code> parameter per Knative Service or Route by using an annotation.</p> <p>Tip</p> <p>For information about global, ConfigMap configurations for rollout durations, see the Administration guide.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\n  annotations:\n    serving.knative.dev/rollout-duration: \"380s\"\n</code></pre>"},{"location":"serving/rolling-out-latest-revision/#route-status-updates","title":"Route status updates","text":"<p>During a rollout, the system updates the Route and Knative Service status conditions. Both the <code>traffic</code> and <code>conditions</code> status parameters are affected.</p> <p>For example, for the following traffic configuration:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 55\n    configurationName: config # Pinned to latest ready Revision\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>Initially 1% of the traffic is rolled out to the Revisions:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 54\n    revisionName: config-00008\n  - percent: 1\n    revisionName: config-00009\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>Then the rest of the traffic is rolled out in increments of 18%:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 36\n    revisionName: config-00008\n  - percent: 19\n    revisionName: config-00009\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>The rollout continues until the target traffic configuration is reached:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 55\n    revisionName: config-00009\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>During the rollout, the Route and Knative Service status conditions are as follows:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\nstatus:\n  conditions:\n  ...\n  - lastTransitionTime: \"...\"\n    message: A gradual rollout of the latest revision(s) is in progress.\n    reason: RolloutInProgress\n    status: Unknown\n    type: Ready\n</code></pre>"},{"location":"serving/rolling-out-latest-revision/#multiple-rollouts","title":"Multiple rollouts","text":"<p>If a new revision is created while a rollout is in progress, the system begins to shift traffic immediately to the newest Revision, and drains the incomplete rollouts from newest to oldest.</p>"},{"location":"serving/setting-up-custom-ingress-gateway/","title":"Configuring the ingress gateway","text":"<p>Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the <code>knative-ingress-gateway</code> Gateway under the <code>knative-serving</code> namespace. By default, we use Istio gateway service <code>istio-ingressgateway</code> under <code>istio-system</code> namespace as its underlying service. You can replace the service and the gateway with that of your own as follows.</p>"},{"location":"serving/setting-up-custom-ingress-gateway/#replace-the-default-istio-ingressgateway-service","title":"Replace the default <code>istio-ingressgateway</code> service","text":""},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-the-gateway-service-and-deployment-instance","title":"Step 1: Create the gateway service and deployment instance","text":"<p>You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default <code>istio-ingressgateway</code> to <code>custom-ingressgateway</code> as follows.</p> <pre><code>apiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  components:\n    ingressGateways:\n      - name: custom-ingressgateway\n        enabled: true\n        namespace: custom-ns\n        label:\n          istio: custom-gateway\n</code></pre>"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-the-knative-gateway","title":"Step 2: Update the Knative gateway","text":"<p>Update gateway instance <code>knative-ingress-gateway</code> under <code>knative-serving</code> namespace:</p> <pre><code>kubectl edit gateway knative-ingress-gateway -n knative-serving\n</code></pre> <p>Replace the label selector with the label of your service:</p> <pre><code>istio: ingressgateway\n</code></pre> <p>For the example <code>custom-ingressgateway</code> service mentioned earlier, it should be updated to:</p> <pre><code>istio: custom-gateway\n</code></pre> <p>If there is a change in service ports (compared with that of <code>istio-ingressgateway</code>), update the port info in the gateway accordingly.</p>"},{"location":"serving/setting-up-custom-ingress-gateway/#step-3-update-the-gateway-configmap","title":"Step 3: Update the gateway ConfigMap","text":"<ol> <li> <p>Update gateway configmap <code>config-istio</code> under <code>knative-serving</code> namespace:</p> <pre><code>kubectl edit configmap config-istio -n knative-serving\n</code></pre> <p>This command opens your default text editor and allows you to edit the config-istio ConfigMap.</p> <pre><code>apiVersion: v1\ndata:\n  _example: |\n    ################################\n    #                              #\n    #    EXAMPLE CONFIGURATION     #\n    #                              #\n    ################################\n    # ...\n    external-gateways: |\n      - name: knative-ingress-gateway\n        namespace: knative-serving\n        service: istio-ingressgateway.istio-system.svc.cluster.local\n</code></pre> </li> <li> <p>Edit the file to add the <code>external-gateways</code> field with the fully qualified url of your service. For the example <code>custom-ingressgateway</code> service mentioned earlier, it should be updated to:</p> <pre><code>apiVersion: v1\ndata:\n  external-gateways: |\n    - name: knative-ingress-gateway\n      namespace: knative-serving\n      service: custom-ingressgateway.custom-ns.svc.cluster.local\nkind: ConfigMap\n[...]\n</code></pre> </li> </ol>"},{"location":"serving/setting-up-custom-ingress-gateway/#replace-the-knative-ingress-gateway-gateway","title":"Replace the <code>knative-ingress-gateway</code> gateway","text":"<p>We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps.</p>"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-the-gateway","title":"Step 1: Create the gateway","text":"<p>Let's say you replace the default <code>knative-ingress-gateway</code> gateway with <code>knative-custom-gateway</code> in <code>custom-ns</code>. First, create the <code>knative-custom-gateway</code> gateway:</p> <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: knative-custom-gateway\n  namespace: custom-ns\nspec:\n  selector:\n    istio: &lt;service-label&gt;\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n</code></pre> Where <code>&lt;service-label&gt;</code> is a label to select your service, for example, <code>ingressgateway</code>.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol>"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-the-gateway-configmap","title":"Step 2: Update the gateway ConfigMap","text":"<ol> <li> <p>Update gateway configmap <code>config-istio</code> under <code>knative-serving</code> namespace:</p> <pre><code>kubectl edit configmap config-istio -n knative-serving\n</code></pre> <p>This command opens your default text editor and allows you to edit the config-istio ConfigMap.</p> <pre><code>apiVersion: v1\ndata:\n  _example: |\n    ################################\n    #                              #\n    #    EXAMPLE CONFIGURATION     #\n    #                              #\n    ################################\n    # ...\n    external-gateways: |\n      - name: knative-ingress-gateway\n        namespace: knative-serving\n        service: istio-ingressgateway.istio-system.svc.cluster.local\n</code></pre> </li> <li> <p>Edit the file to add the <code>external-gateways</code> field with the customized gateway. For the example <code>knative-custom-gateway</code> mentioned earlier, it should be updated to:</p> <pre><code>apiVersion: v1\ndata:\n  external-gateways: |\n    - name: knative-custom-gateway\n      namespace: custom-ns\n      service: istio-ingressgateway.istio-system.svc.cluster.local\nkind: ConfigMap\n[...]\n</code></pre> </li> </ol> <p>The configuration format should be <pre><code>  external-gateways: |\n    - name: &lt;gateway-name&gt;\n      namespace: &lt;gateway-namespace&gt;\n      service: &lt;fully-qualified-url-of-istio-ingress-service&gt;\n</code></pre></p>"},{"location":"serving/tag-resolution/","title":"Tag resolution","text":"<p>Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative.</p> <p>Important</p> <p>The Knative Serving controller must be configured to access the container registry to use this feature.</p>"},{"location":"serving/tag-resolution/#custom-certificates","title":"Custom certificates","text":"<p>If you are using a registry that has a self-signed certificate, you must configure the default Knative Serving <code>controller</code> Deployment to trust that certificate. You can configure trusting certificates by mounting your certificates into the <code>controller</code> Deployment, and then setting the environment variable appropriately.</p>"},{"location":"serving/tag-resolution/#procedure","title":"Procedure","text":"<ol> <li> <p>If you are using a <code>custom-certs</code> secret that contains your CA certificates, add the following spec to the default Knative Serving <code>controller</code> Deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: knative-serving\nspec:\n  template:\n    spec:\n      containers:\n        - name: controller\n          volumeMounts:\n            - name: custom-certs\n              mountPath: /path/to/custom/certs\n          env:\n            - name: SSL_CERT_DIR\n              value: /path/to/custom/certs\n      volumes:\n        - name: custom-certs\n          secret:\n            secretName: custom-certs\n</code></pre> <p>Knative Serving accepts the <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code> environment variables.</p> </li> <li> <p>Create a secret in the <code>knative-serving</code> namespace that points to your root CA certificate, and then save the current Knative Serving <code>controller</code> Deployment:</p> <pre><code>kubectl -n knative-serving create secret generic customca --from-file=ca.crt=/root/ca.crt\n</code></pre> <pre><code>kubectl -n knative-serving get deploy/controller -o yaml &gt; knative-serving-controller.yaml\n</code></pre> </li> </ol>"},{"location":"serving/tag-resolution/#corporate-proxy","title":"Corporate proxy","text":"<p>If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry.</p> <p>Knative accepts the <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> environment variables, so you can configure the controller Deployment as follows:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: knative-serving\nspec:\n  template:\n    spec:\n      containers:\n        - name: controller\n          env:\n            - name: HTTP_PROXY\n              value: http://proxy.example.com\n            - name: HTTPS_PROXY\n              value: https://proxy.example.com\n</code></pre>"},{"location":"serving/traffic-management/","title":"Traffic management","text":"<p>You can manage traffic routing to different Revisions of a Knative Service by modifying the <code>traffic</code> spec of the Service resource.</p> <p>When you create a Knative Service, it does not have any default <code>traffic</code> spec settings. By setting the <code>traffic</code> spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting <code>latestRevision: true</code> in the spec for a Service.</p>"},{"location":"serving/traffic-management/#using-tags-to-create-target-urls","title":"Using tags to create target URLs","text":"<p>In the following example, the spec defines an attribute called <code>tag</code>:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n...\n  traffic:\n  - percent: 0\n    revisionName: example-service-1\n    tag: staging\n  - percent: 40\n    revisionName: example-service-2\n  - percent: 60\n    revisionName: example-service-3\n</code></pre> <p>When a <code>tag</code> attribute is applied to a Route, an address for the specific traffic target is created.</p> <p>In this example, you can access the staging target by accessing <code>staging-&lt;route name&gt;.&lt;namespace&gt;.&lt;domain&gt;</code>. The targets for <code>example-service-2</code> and <code>example-service-3</code> can only be accessed using the main route, <code>&lt;route name&gt;.&lt;namespace&gt;.&lt;domain&gt;</code>.</p> <p>When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called <code>staging-&lt;route name&gt;</code> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label <code>networking.knative.dev/visibility</code> with value <code>cluster-local</code>. See the documentation on private services for more information about how to restrict visibility on specific Routes.</p>"},{"location":"serving/traffic-management/#traffic-routing-examples","title":"Traffic routing examples","text":"<p>The following example shows a <code>traffic</code> spec where 100% of traffic is routed to the <code>latestRevision</code> of the Service. Under <code>status</code> you can see the name of the latest Revision that <code>latestRevision</code> was resolved to:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n...\n  traffic:\n  - latestRevision: true\n    percent: 100\nstatus:\n  ...\n  traffic:\n  - percent: 100\n    revisionName: example-service-1\n</code></pre> <p>The following example shows a <code>traffic</code> spec where 100% of traffic is routed to the <code>current</code> Revision, and the name of that Revision is specified as <code>example-service-1</code>. The latest ready Revision is kept available, even though no traffic is being routed to it:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n...\n  traffic:\n  - tag: current\n    revisionName: example-service-1\n    percent: 100\n  - tag: latest\n    latestRevision: true\n    percent: 0\n</code></pre> <p>The following example shows how the list of Revisions in the <code>traffic</code> spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the <code>current</code> Revision, <code>example-service-1</code>, and 50% of traffic to the <code>candidate</code> Revision, <code>example-service-2</code>:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n...\n  traffic:\n  - tag: current\n    revisionName: example-service-1\n    percent: 50\n  - tag: candidate\n    revisionName: example-service-2\n    percent: 50\n  - tag: latest\n    latestRevision: true\n    percent: 0\n</code></pre>"},{"location":"serving/traffic-management/#routing-and-managing-traffic-by-using-the-knative-cli","title":"Routing and managing traffic by using the Knative CLI","text":"<p>You can use the following <code>kn</code> CLI command to split traffic between revisions:</p> <pre><code>kn service update &lt;service-name&gt; --traffic &lt;revision-name&gt;=&lt;percent&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;service-name&gt;</code> is the name of the Knative Service that you are configuring traffic routing for.</li> <li><code>&lt;revision-name&gt;</code> is the name of the revision that you want to configure to receive a percentage of traffic.</li> <li><code>&lt;percent&gt;</code> is the percentage of traffic that you want to send to the revision specified by <code>&lt;revision-name&gt;</code>.</li> </ul> <p>For example, to split traffic for a Service named <code>example</code>, by sending 80% of traffic to the Revision <code>green</code> and 20% of traffic to the Revision <code>blue</code>, you could run the following command:</p> <pre><code>kn service update example-service --traffic green=80 --traffic blue=20\n</code></pre> <p>It is also possible to add tags to Revisions and then split traffic according to the tags you have set:</p> <pre><code>kn service update example --tag revision-0001=green --tag @latest=blue\n</code></pre> <p>The <code>@latest</code> tag means that <code>blue</code> resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named <code>v1</code>.</p> <pre><code>kn service update example-service --traffic @latest=80 --traffic v1=20\n</code></pre>"},{"location":"serving/traffic-management/#routing-and-managing-traffic-with-bluegreen-deployment","title":"Routing and managing traffic with blue/green deployment","text":"<p>You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy.</p>"},{"location":"serving/traffic-management/#procedure","title":"Procedure","text":"<ol> <li>Create and deploy an app as a Knative Service.</li> <li> <p>Find the name of the first Revision that was created when you deployed the Service, by running the command:</p> <pre><code>kubectl get configurations &lt;service-name&gt; -o=jsonpath='{.status.latestCreatedRevisionName}'\n</code></pre> <p>Where <code>&lt;service-name&gt;</code> is the name of the Service that you have deployed.</p> </li> <li> <p>Define a Route to send inbound traffic to the Revision.</p> <p>Example Route <pre><code>apiVersion: serving.knative.dev/v1\nkind: Route\nmetadata:\n  name: &lt;route-name&gt;\n  namespace: default\nspec:\n  traffic:\n    - revisionName: &lt;first-revision-name&gt;\n      percent: 100 # All traffic goes to this revision\n</code></pre></p> <p>Where;</p> <ul> <li><code>&lt;route-name&gt;</code> is the name you choose for your route.</li> <li><code>&lt;first-revision-name&gt;</code> is the name of the initial Revision from the previous step.</li> </ul> </li> <li> <p>Verify that you can view your app at the URL output you get from using the following command:</p> <pre><code>kubectl get route &lt;route-name&gt;\n</code></pre> <p>Where <code>&lt;route-name&gt;</code> is the name of the Route you created in the previous step.</p> </li> <li> <p>Deploy a second Revision of your app by modifying at least one field in the <code>template</code> spec of the Service resource. For example, you can modify the <code>image</code> of the Service, or an <code>env</code> environment variable.</p> </li> <li> <p>Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the <code>kn service update</code> command if you have installed the <code>kn</code> CLI.</p> </li> <li> <p>Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command:</p> <pre><code>kubectl get configurations &lt;service-name&gt; -o=jsonpath='{.status.latestCreatedRevisionName}'\n</code></pre> <p>Where <code>&lt;service-name&gt;</code> is the name of the Service that you have redeployed.</p> <p>At this point, both the first and second Revisions of the Service are deployed and running.</p> </li> <li> <p>Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision.</p> <p>Example of updated Route <pre><code>apiVersion: serving.knative.dev/v1\nkind: Route\nmetadata:\n  name: &lt;route-name&gt;\n  namespace: default\nspec:\n  traffic:\n    - revisionName: &lt;first-revision-name&gt;\n      percent: 100 # All traffic is still being routed to the first revision\n    - revisionName: &lt;second-revision-name&gt;\n      percent: 0 # 0% of traffic routed to the second revision\n      tag: v2 # A named route\n</code></pre></p> <p>Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged.</p> <p>No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named <code>v2</code> for testing the newly deployed Revision.</p> </li> <li> <p>Get the URL of the new Route for the second Revision, by running the command:</p> <pre><code>kubectl get route &lt;route-name&gt; --output jsonpath=\"{.status.traffic[*].url}\"\n</code></pre> <p>You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it.</p> </li> <li> <p>Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision:</p> <p>Example of updated Route <pre><code>apiVersion: serving.knative.dev/v1\nkind: Route\nmetadata:\n  name: &lt;route-name&gt;\n  namespace: default\nspec:\n  traffic:\n    - revisionName: &lt;first-revision-name&gt;\n      percent: 50\n    - revisionName: &lt;second-revision-name&gt;\n      percent: 50\n      tag: v2\n</code></pre></p> </li> <li> <p>Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision:</p> <p>Example of updated Route <pre><code>apiVersion: serving.knative.dev/v1\nkind: Route\nmetadata:\n  name: &lt;route-name&gt;\n  namespace: default\nspec:\n  traffic:\n    - revisionName: &lt;first-revision-name&gt;\n      percent: 0\n    - revisionName: &lt;second-revision-name&gt;\n      percent: 100\n      tag: v2\n</code></pre></p> <p>Tip</p> <p>You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected.</p> </li> <li> <p>Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.</p> </li> </ol>"},{"location":"serving/using-a-custom-domain/","title":"Configuring domain names","text":"<p>You can customize the domain of an individual Knative Service, or set a global default domain for all Services created on a cluster. The fully qualified domain name for a route by default is <code>{route}.{namespace}.svc.cluster.local</code>.</p>"},{"location":"serving/using-a-custom-domain/#configuring-a-domain-for-a-single-knative-service","title":"Configuring a domain for a single Knative Service","text":"<p>If you want to customize the domain of an individual Service, see the documentation about <code>DomainMapping</code>.</p>"},{"location":"serving/using-a-custom-domain/#configuring-the-default-domain-for-all-knative-services-on-a-cluster","title":"Configuring the default domain for all Knative Services on a cluster","text":"<p>You can change the default domain for all Knative Services on a cluster by modifying the <code>config-domain</code> ConfigMap.</p>"},{"location":"serving/using-a-custom-domain/#procedure","title":"Procedure","text":"<ol> <li> <p>Open the <code>config-domain</code> ConfigMap in your default text editor:</p> <pre><code>kubectl edit configmap config-domain -n knative-serving\n</code></pre> </li> <li> <p>Edit the file to replace <code>svc.cluster.local</code> with the domain you want to use, then remove the <code>_example</code> key and save your changes. In this example, <code>knative.dev</code> is configured as the domain for all routes:</p> <pre><code>apiVersion: v1\ndata:\n  knative.dev: \"\"\nkind: ConfigMap\n[...]\n</code></pre> </li> </ol> <p>If you have an existing deployment, Knative reconciles the change made to the ConfigMap, and automatically updates the host name for all of the deployed Services and Routes.</p>"},{"location":"serving/using-a-custom-domain/#verification-steps","title":"Verification steps","text":"<ol> <li>Deploy an app to your cluster.</li> <li> <p>Retrieve the URL for the Route:</p> <pre><code>kubectl get route &lt;route-name&gt; --output jsonpath=\"{.status.url}\"\n</code></pre> <p>Where <code>&lt;route-name&gt;</code> is the name of the Route.</p> </li> <li> <p>Observe the customized domain that you have configured.</p> </li> </ol>"},{"location":"serving/using-a-custom-domain/#publish-your-domain","title":"Publish your Domain","text":"<p>To make your domain publicly accessible, you must update your DNS provider to point to the IP address for your service ingress.</p> <ol> <li> <p>Create a wildcard record   for the namespace and custom domain to the ingress IP Address, which would   enable hostnames for multiple services in the same namespace to work without   creating additional DNS entries.</p> <pre><code>*.default.knative.dev                   59     IN     A   35.237.28.44\n</code></pre> </li> <li> <p>Create an A record to point from the fully qualified domain name to the IP   address of your Knative gateway. This step needs to be done for each Knative   Service or Route created.</p> <pre><code>helloworld-go.default.knative.dev       59     IN     A   35.237.28.44\n</code></pre> </li> <li> <p>After the domain update has propagated, you can access your app by using the fully qualified domain name of the deployed route.</p> </li> </ol>"},{"location":"serving/webhook-customizations/","title":"Exclude namespaces from the Knative webhook","text":"<p>The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades.</p> <p>You can configure the label <code>webhooks.knative.dev/exclude</code> to allow namespaces to bypass the Knative webhook.</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-dev\n  labels:\n    webhooks.knative.dev/exclude: \"true\"\n</code></pre>"},{"location":"serving/app-security/security-guard-about/","title":"About Security-Guard","text":"<p>Security-Guard provides visibility into the security status of deployed Knative Services, by monitoring the behaviors of user containers and events. Security-Guard also supports optional blocking of events and termination of user container instances, all based on behavior.</p>"},{"location":"serving/app-security/security-guard-about/#security-guard-profile-and-criteria","title":"Security-Guard profile and criteria","text":"<p>Security-Guard creates a profile of each user container behavior and of each event behavior. The behaviors are then compared to a pre-defined criteria. If the profile does not meet the criteria, Security-Guard can log alerts, block misbehaving events, or stop misbehaving Service instances, depending on user configurations.</p> <p>The criteria that a profile is compared to is composed of a set of micro-rules. These rules describe expected behaviors for events and user containers, including expected responses. You can choose to set micro-rules manually, or use Security-Guard's machine learning feature to automate the creation of micro-rules.</p>"},{"location":"serving/app-security/security-guard-about/#guardians","title":"Guardians","text":"<p>A per-Service set of micro-rules is stored in the Kubernetes system as a <code>Guardian</code> object. Under Knative, Security-Guard store Guardians using the <code>guardians.guard.security.knative.dev</code> CRDs.</p> <p>To list all CRD Guardians use:</p> <pre><code>kubectl get guardians.guard.security.knative.dev\n</code></pre> <p>Example Output:</p> <pre><code>NAME            AGE\nhelloworld-go   10h\n</code></pre>"},{"location":"serving/app-security/security-guard-about/#using-security-guard","title":"Using Security-Guard","text":"<p>Security-Guard offers situational awareness by writing its alerts to the Service queue proxy log. You may observe the queue-proxy to see alerts.</p> <p>Security alerts appear in the queue proxy log file and start with the string <code>SECURITY ALERT!</code>. The default setup of Security-Guard is to to learn any new pattern after reporting it. By default, Security-Guard will never block events and will never stop Service instances.</p> <p>When a new Service is deployed and is actively serving requests, it typically takes about 30 min for Security-Guard to learn the patterns of the Service requests and responses and build corresponding micro-rules. After the initial learning period, Security-Guard updates the micro-rules in the Service Guardian, following which, it sends alerts only when a change in behavior is detected.</p> <p>Note that in the default setup, Security-Guard continues to learn any new behavior and therefore avoids reporting alerts repeatedly when the new behavior reoccurs. Correct security procedures should include reviewing any new behavior detected by Security-Guard.</p> <p>Security-Guard can also be configured to operate in other modes of operation, such as:</p> <ul> <li>Move from auto learning to manual micro-rules management after the initial learning period</li> <li>Block requests/responses when they do not conform to the micro-rules</li> </ul> <p>For more information or for troubleshooting help, see the #knative-security channel in Knative Slack.</p>"},{"location":"serving/app-security/security-guard-about/#security-guard-use-cases","title":"Security-Guard Use Cases","text":"<p>Security-Guard support four different stages in the life of a knative service from a security standpoint.</p> <ul> <li>Zero-Day</li> <li>Vulnerable</li> <li>Exploitable</li> <li>Misused</li> </ul> <p>We next detail each stage and how Security-Guard is used to manage the security of the service in that stage.</p>"},{"location":"serving/app-security/security-guard-about/#zero-day","title":"Zero-Day","text":"<p>Under normal conditions, the Knative user who owns the service is not aware of any known vulnerabilities in the service. Yet, it is reasonable to assume that the service has weaknesses.</p> <p>Security-Guard offers Knative users the ability to detect/block patterns sent as part of incoming events that may be used to exploit unknown, zero-day, service vulnerabilities.</p>"},{"location":"serving/app-security/security-guard-about/#vulnerable","title":"Vulnerable","text":"<p>Once a CVE that describes a vulnerability in the service is published, the Knative user who owns the service is required to start a process to eliminate the vulnerability by introducing a new revision of the service. This process of removing a known vulnerability may take many weeks to accomplish.</p> <p>Security-Guard enables Knative users to set micro-rules to detect/block incoming events that include patterns that may be used as part of some future exploit targeting the discovered vulnerability. In this way, users are able to continue offering services, although the service has a known vulnerability.</p>"},{"location":"serving/app-security/security-guard-about/#exploitable","title":"Exploitable","text":"<p>When a known exploit is found effective in compromising a service, the Knative user who owns the Service needs a way to filter incoming events that contain the specific exploit. This is normally the case during a successful attack, where a working exploit is able to compromise the user-container.</p> <p>Security-Guard enables Knative users a way to set micro-rules to detect/block incoming events that include specific exploits while allowing other events to be served.</p>"},{"location":"serving/app-security/security-guard-about/#misused","title":"Misused","text":"<p>When an offender has established an attack pattern that is able to take over a service instance, by first exploiting one or more vulnerabilities and then starting to misuse the service instance, stopping the service instance requires the offender to repeat the attack pattern. At any given time, some service instances may be compromised and misused while others behave as designed.</p> <p>Security-Guard enables Knative users a way to detect/remove misused Service instances while allowing other instances to continue serve events.</p>"},{"location":"serving/app-security/security-guard-about/#additional-resources","title":"Additional resources","text":"<p>See Readme files in the Security-Guard Github Repository.</p>"},{"location":"serving/app-security/security-guard-example-alerts/","title":"Security-Guard example alerts","text":"<ol> <li> <p>Send an event with unexpected query string, for example:</p> <pre><code>curl \"http://helloworld-go.default.52.118.14.2.sslip.io?a=3\"\n</code></pre> <p>This returns an output similar to the following:</p> <pre><code>Hello Secured World!\n</code></pre> </li> <li> <p>Check alerts:</p> <pre><code>kubectl logs deployment/helloworld-go-00001-deployment queue-proxy|grep \"SECURITY ALERT!\"\n</code></pre> <p>This returns an output similar to the following:</p> <pre><code>...SECURITY ALERT! HttpRequest -&gt; [QueryString:[KeyVal:[Key a is not known,],],]\n</code></pre> </li> <li> <p>Send an event with unexpected long url, for example:</p> <pre><code>curl \"http://helloworld-go.default.52.118.14.2.sslip.io/AAAAAAAAAAAAAAAA\"\n</code></pre> <p>This returns an output similar to the following:</p> <pre><code>Hello Secured World!\n</code></pre> </li> <li> <p>Check alerts:</p> <pre><code>kubectl logs deployment/helloworld-go-00001-deployment queue-proxy|grep \"SECURITY ALERT!\"\n</code></pre> <p>This returns an output similar to the following:</p> <pre><code>...SECURITY ALERT! HttpRequest -&gt; [Url:[Segments:[Counter out of Range: 1,],Val:[Letters:[Counter out of Range: 16,],Sequences:[Counter out of Range: 1,],],],].\n</code></pre> </li> </ol>"},{"location":"serving/app-security/security-guard-install/","title":"Installing Security-Guard","text":"<p>Here we show how to install Security-Guard in Knative. Security-Guard is an enhancement to knative-Serving and needs to be installed after the Knative-Serving is successfully installed.</p> <p>Using Security-Guard requires that your cluster will use an enhanced queue-proxy image.</p> <p>In addition, Security-Guard includes automation for auto-learning a per service Guardian. Auto-learning requires you to deploy a <code>guard-service</code> on your kubernetes cluster. <code>guard-service</code> should be installed in in the <code>knative-serving</code> namespace.</p> <p>In production you would typically also wish to enable TLS and Token support to protect the queue-proxy communication with the <code>guard-service</code> as described below.</p>"},{"location":"serving/app-security/security-guard-install/#before-you-begin","title":"Before you begin","text":"<p>Before installing Security-Guard, learn about Security-Guard</p>"},{"location":"serving/app-security/security-guard-install/#install-steps","title":"Install steps","text":"<p>To start this tutorial, after installing Knative Serving, run the following procedure to replace your queue-proxy image and deploy a <code>guard-service</code>.</p> Install from sourceInstall from released images and yamlsInstall using the Knative Operator <ol> <li> <p>Clone the Security-Guard repository using <code>git clone git@github.com:knative-extensions/security-guard.git</code></p> </li> <li> <p>Do <code>cd security-guard</code></p> </li> <li> <p>Run <code>ko apply -Rf ./config</code></p> </li> </ol> <p>Use released images to update your system to enable Security-Guard:</p> <ol> <li> <p>Set the feature named <code>queueproxy.mount-podinfo</code> to <code>allowed</code> in the config-features ConfigMap.</p> <p>An easy way to do that is using:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/security-guard/release-0.4/config/deploy/config-features.yaml\n</code></pre> </li> <li> <p>Set the deployment parameter <code>queue-sidecar-image</code> to <code>gcr.io/knative-releases/knative.dev/security-guard/cmd/queue</code> in the config-deployment ConfigMap.</p> <p>An easy way to do that is using:</p> <pre><code>kubectl apply -f https://github.com/knative-extensions/security-guard/releases/download/v0.4.0/queue-proxy.yaml\n</code></pre> </li> <li> <p>Add the necessary Security-Guard resources to your cluster using:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/security-guard/release-0.4/config/resources/gateAccount.yaml\nkubectl apply -f https://raw.githubusercontent.com/knative-extensions/security-guard/release-0.4/config/resources/serviceAccount.yaml\nkubectl apply -f https://raw.githubusercontent.com/knative-extensions/security-guard/release-0.4/config/resources/guardiansCrd.yaml\n</code></pre> </li> <li> <p>Deploy <code>guard-service</code> on your system to enable automated learning of micro-rules.</p> <p>An easy way to do that is using:</p> <pre><code>kubectl apply -f https://github.com/knative-extensions/security-guard/releases/download/v0.4.0/guard-service.yaml\n</code></pre> </li> </ol> <p>Note</p> <p>The example below shows a case where kourier ingress is used, make the necessary changes when installing with istio or contour.</p> <p>Example script to install Security-Guard and Serving with Kourier using the Knative Operator.</p> <pre><code>kubectl apply --filename - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-serving\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  security:\n    securityGuard:\n      enabled: true\n  ingress:\n    kourier:\n      enabled: true\n  config:\n    network:\n      ingress.class: \"kourier.ingress.networking.knative.dev\"\nEOF\n\nkubectl apply -f https://raw.githubusercontent.com/knative-extensions/security-guard/release-0.4/config/resources/gateAccount.yaml\n</code></pre>"},{"location":"serving/app-security/security-guard-install/#per-namespace-setup","title":"Per Namespace Setup","text":"<p>In order to deploy guard protected services in a namespace, provide <code>guard-gate</code> with the necessary permissions on each namespace used:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/security-guard/release-0.4/config/resources/gateAccount.yaml\n</code></pre>"},{"location":"serving/app-security/security-guard-install/#additional-production-configuration","title":"Additional Production Configuration","text":"<p>It is recommended to secure the communication between queue-proxy with the <code>guard-service</code> using one of the following methods:</p> Manual changesUsing scriptsUsing Knative Operator <ol> <li> <p>Add <code>GUARD_SERVICE_TLS=true</code> to the environment of <code>guard-service</code> to enable TLS and server side authentication using a Knative issued certificate. The <code>guard-service</code> will be using the keys in the <code>knative-serving-certs</code> secret of the <code>knative-serving</code> namespace.</p> </li> <li> <p>Add <code>GUARD_SERVICE_AUTH=true</code> to the environment of <code>guard-service</code> to enable client side authentication using tokens</p> </li> <li> <p>Set the <code>queue-sidecar-rootca</code> parameter of the <code>config-deployment</code> configmap in the <code>knative-serving</code> namespace to the public key defined under <code>ca-cert.pem</code> key in the <code>knative-serving-certs</code> secret of the <code>knative-serving</code> namespace. This will inform queue-proxy to use TLS and approve the guard-service certificates.</p> </li> <li> <p>Set <code>queue-sidecar-token-audiences = \"guard-service\"</code> at the <code>config-deployment</code> configmap in the <code>knative-serving</code> namespace. This will produce a a token with audience <code>guard-service</code> for every queue-proxy instance.</p> </li> </ol> <p>Use the following script to set TLS and Tokens support in guard-service:</p> <pre><code>echo \"Add TLS and Tokens to guard-service\"\nkubectl patch deployment guard-service -n knative-serving -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"guard-service\",\"env\":[{\"name\": \"GUARD_SERVICE_TLS\", \"value\": \"true\"}, {\"name\": \"GUARD_SERVICE_AUTH\", \"value\": \"true\"}]}]}}}}'\n</code></pre> <p>Use the following script to set TLS and Tokens support in guard-gates:</p> <pre><code>echo \"Copy the certificate to a temporary file\"\nROOTCA=\"$(mktemp)\"\nFILENAME=`basename $ROOTCA`\nkubectl get secret -n knative-serving knative-serving-certs -o json| jq -r '.data.\"ca-cert.pem\"' | base64 -d &gt;  $ROOTCA\n\necho \"Get the certificate in a configmap friendly form\"\nCERT=`kubectl create cm config-deployment --from-file $ROOTCA -o json --dry-run=client |jq .data.\\\"$FILENAME\\\"`\n\necho \"Add TLS and Tokens to config-deployment configmap\"\nkubectl patch cm config-deployment -n knative-serving -p '{\"data\":{\"queue-sidecar-token-audiences\": \"guard-service\", \"queue-sidecar-rootca\": '\"$CERT\"'}}'\n\necho \"cleanup\"\nrm  $ROOTCA\n</code></pre> <p>Use the following script to read the TLS and Token settings of both guard-service and guard-gates:</p> <pre><code>echo \"Results:\"\nkubectl get cm config-deployment -n knative-serving -o json|jq '.data'\nkubectl get deployment guard-service -n knative-serving -o json|jq .spec.template.spec.containers[0].env\n</code></pre> <p>Use the following script to unset TLS and Tokens support in guard-service:</p> <pre><code>echo \"Remove TLS and Tokens from  guard-service deployment\"\nkubectl patch deployment guard-service -n knative-serving -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"guard-service\",\"env\":[{\"name\": \"GUARD_SERVICE_TLS\", \"value\": \"false\"}, {\"name\": \"GUARD_SERVICE_AUTH\", \"value\": \"false\"}]}]}}}}'\n</code></pre> <p>Use the following script to unset TLS and Tokens support in guard-gates:</p> <pre><code>echo \"Remove TLS and Tokens from config-deployment configmap\"\nkubectl patch cm config-deployment -n knative-serving -p '{\"data\":{\"queue-sidecar-token-audiences\": \"\", \"queue-sidecar-rootca\": \"\"}}'\n</code></pre> <p>Note</p> <p>The example below shows a case where kourier ingress is used, make the necessary changes when installing with istio or contour.</p> <p>Example script to install Security-Guard with TLS and Serving with Kourier using the Knative Operator.</p> <pre><code>kubectl apply --filename - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: knative-serving\n---\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nEOF\n\necho \"Waiting for secret to be created (CTRL-C to exit)\"\nPEM=\"\"\nwhile [[ -z $PEM ]]\ndo\n  echo -n \".\"\n  sleep 1\n  DOC=`kubectl get secret -n knative-serving knative-serving-certs -o json 2&gt; /dev/null`\n  PEM=`echo $DOC | jq -r '.data.\"ca-cert.pem\"'`\ndone\necho \" Secret found!\"\n\necho \"Copy the certificate to file\"\nROOTCA=\"$(mktemp)\"\nFILENAME=`basename $ROOTCA`\necho $PEM | base64 -d &gt;  $ROOTCA\n\necho \"Create a temporary config-deployment configmap with the certificate\"\nCERT=`kubectl create cm config-deployment --from-file $ROOTCA -o json --dry-run=client |jq .data.\\\"$FILENAME\\\"`\n\necho \"cleanup\"\nrm $ROOTCA\n\nkubectl apply --filename - &lt;&lt;EOF\napiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  deployments:\n  - name: guard-service\n    env:\n    - container: guard-service\n      envVars:\n      - name: GUARD_SERVICE_TLS\n        value: \"true\"\n      - name: GUARD_SERVICE_AUTH\n        value: \"true\"\n  security:\n    securityGuard:\n      enabled: true\n  ingress:\n    kourier:\n      enabled: true\n  config:\n    network:\n      ingress.class: \"kourier.ingress.networking.knative.dev\"\n    deployment:\n      queue-sidecar-rootca: ${CERT}\n      queue-sidecar-token-audiences: guard-service\nEOF\n</code></pre>"},{"location":"serving/app-security/security-guard-quickstart/","title":"Security-Guard monitoring quickstart","text":"<p>This tutorial shows how you can use Security-Guard to protect a deployed Knative Service.</p>"},{"location":"serving/app-security/security-guard-quickstart/#before-you-begin","title":"Before you begin","text":"<p>Before starting the tutorial, make sure to install Security-Guard</p>"},{"location":"serving/app-security/security-guard-quickstart/#creating-and-deploying-a-service","title":"Creating and deploying a service","text":"<p>Tip</p> <p>The following commands create a <code>helloworld-go</code> sample Service while activating and configuring the Security-Guard extension for this Service. You can modify these commands, including changing the Security-Guard configuration for your service using either the <code>kn</code> CLI or changing the service yaml based on this example.</p> <p>Create a sample securedService:</p> Apply YAMLkn services CLIkn func CLI <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n        annotations:\n          features.knative.dev/queueproxy-podinfo: enabled\n          qpoption.knative.dev/guard-activate: enable\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          env:\n            - name: TARGET\n              value: \"Secured World\"\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>Creating a service using CLI</p> <pre><code>kn service create helloworld-go \\\n    --image ghcr.io/knative/helloworld-go:latest \\\n    --env \"TARGET=Secured World\" \\\n    --annotation features.knative.dev/queueproxy-podinfo=enabled \\\n    --annotation qpoption.knative.dev/guard-activate=enable\n</code></pre> <p>Creating a function using CLI.</p> <p>Add the following <code>deploy.annotations</code> to your <code>func.yaml</code> file located in your project dir\"</p> <pre><code>deploy:\n  annotations:\n    features.knative.dev/queueproxy-podinfo: enabled\n    qpoption.knative.dev/guard-activate: enable\n</code></pre> <p>Deploy as you would deploy any other function</p> <pre><code>kn func deploy\n</code></pre> <p>After the Service has been created, Guard starts monitoring the Service Pods and all Events sent to the Service.</p> <p>Continue to Security-Guard alert example to test your installation</p> <p>See the Using Security-Guard section to learn about managing the security of the service</p>"},{"location":"serving/app-security/security-guard-quickstart/#cleanup","title":"Cleanup","text":"<p>To remove the deployed service use:</p> Apply YAMLkn CLI <p>Delete using the YAML file used to create the service by running the command:</p> <pre><code>kubectl delete -f &lt;filename&gt;.yaml\n</code></pre> <p>Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> <pre><code>kn service delete helloworld-go\n</code></pre> <p>To remove the Guardian of the deployed service use:</p> <pre><code>```bash\nkubectl delete guardians.guard.security.knative.dev helloworld-go\n```\n</code></pre>"},{"location":"serving/autoscaling/","title":"Autoscaling","text":"<p>Knative Serving provides automatic scaling, or autoscaling, for applications to match incoming demand. This is provided by default, by using the Knative Pod Autoscaler (KPA).</p> <p>For example, if an application is receiving no traffic and scale to zero is enabled, Knative Serving scales the application down to zero replicas. If scaling to zero is disabled, the application is scaled down to the minimum number of replicas specified for applications on the cluster. Replicas are scaled up to meet demand if traffic to the application increases.</p> <p>You can enable and disable scale to zero functionality for your cluster if you have cluster administrator permissions. See Configuring scale to zero.</p> <p>To use autoscaling for your application if it is enabled on your cluster, you must configure concurrency and scale bounds.</p>"},{"location":"serving/autoscaling/#additional-resources","title":"Additional resources","text":"<ul> <li>Try out the Go Autoscale Sample App.</li> <li>Configure your Knative deployment to use the Kubernetes Horizontal Pod Autoscaler (HPA) instead of the default KPA. For how to install HPA, see Install optional Serving extensions.</li> <li>Configure the types of metrics that the Autoscaler consumes.</li> </ul>"},{"location":"serving/autoscaling/autoscaler-types/","title":"Supported Autoscaler types","text":"<p>Knative Serving supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes' Horizontal Pod Autoscaler (HPA). This topic lists the features and limitations of each of these Autoscalers, as well as how to configure them.</p> <p>Important</p> <p>If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install it after you install Knative Serving.</p> <p>For how to install HPA, see Install optional Serving extensions.</p>"},{"location":"serving/autoscaling/autoscaler-types/#knative-pod-autoscaler-kpa","title":"Knative Pod Autoscaler (KPA)","text":"<ul> <li>Part of the Knative Serving core and enabled by default once Knative Serving is installed.</li> <li>Supports scale to zero functionality.</li> <li>Does not support CPU-based autoscaling.</li> </ul>"},{"location":"serving/autoscaling/autoscaler-types/#horizontal-pod-autoscaler-hpa","title":"Horizontal Pod Autoscaler (HPA)","text":"<ul> <li>Not part of the Knative Serving core, and you must install Knative Serving first.</li> <li>Does not support scale to zero functionality.</li> <li>Supports CPU-based autoscaling.</li> </ul>"},{"location":"serving/autoscaling/autoscaler-types/#configuring-the-autoscaler-implementation","title":"Configuring the Autoscaler implementation","text":"<p>The type of Autoscaler implementation (KPA or HPA) can be configured by using the <code>class</code> annotation.</p> <ul> <li>Global settings key: <code>pod-autoscaler-class</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/class</code></li> <li>Possible values: <code>\"kpa.autoscaling.knative.dev\"</code> or <code>\"hpa.autoscaling.knative.dev\"</code></li> <li>Default: <code>\"kpa.autoscaling.knative.dev\"</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/class: \"kpa.autoscaling.knative.dev\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n pod-autoscaler-class: \"kpa.autoscaling.knative.dev\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      pod-autoscaler-class: \"kpa.autoscaling.knative.dev\"\n</code></pre>"},{"location":"serving/autoscaling/autoscaler-types/#global-versus-per-revision-settings","title":"Global versus per-revision settings","text":"<p>Configuring for autoscaling in Knative can be set using either global or per-revision settings.</p> <ol> <li>If no per-revision autoscaling settings are specified, the global settings will be used.</li> <li>If per-revision settings are specified, these will override the global settings when both types of settings exist.</li> </ol>"},{"location":"serving/autoscaling/autoscaler-types/#global-settings","title":"Global settings","text":"<p>Global settings for autoscaling are configured using the <code>config-autoscaler</code> ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the <code>spec.config.autoscaler</code> ConfigMap, located in the <code>KnativeServing</code> custom resource (CR).</p>"},{"location":"serving/autoscaling/autoscaler-types/#example-of-the-default-autoscaling-configmap","title":"Example of the default autoscaling ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n container-concurrency-target-default: \"100\"\n container-concurrency-target-percentage: \"0.7\"\n enable-scale-to-zero: \"true\"\n max-scale-up-rate: \"1000\"\n max-scale-down-rate: \"2\"\n panic-window-percentage: \"10\"\n panic-threshold-percentage: \"200\"\n scale-to-zero-grace-period: \"30s\"\n scale-to-zero-pod-retention-period: \"0s\"\n stable-window: \"60s\"\n target-burst-capacity: \"200\"\n requests-per-second-target-default: \"200\"\n</code></pre>"},{"location":"serving/autoscaling/autoscaler-types/#per-revision-settings","title":"Per-revision settings","text":"<p>Per-revision settings for autoscaling are configured by adding annotations to a revision.</p> <p>Example:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/target: \"70\"\n</code></pre> <p>Important</p> <p>If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.</p>"},{"location":"serving/autoscaling/autoscaling-metrics/","title":"Metrics","text":"<p>The metric configuration defines which metric type is watched by the Autoscaler.</p>"},{"location":"serving/autoscaling/autoscaling-metrics/#setting-metrics-per-revision","title":"Setting metrics per revision","text":"<p>For per-revision configuration, this is determined using the <code>autoscaling.knative.dev/metric</code> annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using:</p> <ul> <li>The default KPA Autoscaler supports the <code>concurrency</code> and <code>rps</code> metrics.</li> <li>The HPA Autoscaler supports the <code>cpu</code> metric.</li> </ul> <p>For more information about KPA and HPA, see the documentation on Supported Autoscaler types.</p> <ul> <li>Per-revision annotation key: <code>autoscaling.knative.dev/metric</code></li> <li>Possible values: <code>\"concurrency\"</code>, <code>\"rps\"</code>, <code>\"cpu\"</code>, <code>\"memory\"</code> or any custom metric name, depending on your Autoscaler type. The <code>\"cpu\"</code>, <code>\"memory\"</code>, and custom metrics are supported on revisions that use the HPA class.</li> <li>Default: <code>\"concurrency\"</code></li> </ul> ConcurrencyRequests per secondCPUMemoryCustom metric <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/metric: \"concurrency\"\n        autoscaling.knative.dev/target-utilization-percentage: \"70\" \n</code></pre> <p>Note</p> <p>The <code>autoscaling.knative.dev/target-utilization-percentage</code> annotation for \"Concurrency\" specifies a percentage value. See Configuring targets for more details.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/metric: \"rps\"\n        autoscaling.knative.dev/target: \"150\"\n</code></pre> <p>Note</p> <p>The <code>autoscaling.knative.dev/target</code> annotation for \"Requests per second\" specifies an integer value. See Configuring targets for more details.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/class: \"hpa.autoscaling.knative.dev\"\n        autoscaling.knative.dev/metric: \"cpu\"\n        autoscaling.knative.dev/target: \"100\"\n</code></pre> <p>Note</p> <p>The <code>autoscaling.knative.dev/target</code> annotation for \"CPU\" specifies the integer value in millicore. See Configuring targets for more details.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/class: \"hpa.autoscaling.knative.dev\"\n        autoscaling.knative.dev/metric: \"memory\"\n        autoscaling.knative.dev/target: \"75\"\n</code></pre> <p>Note</p> <p>The <code>autoscaling.knative.dev/target</code> annotation for \"Memory\" specifies the integer value in Mi. See Configuring targets for more details.</p> <p>You can create an HPA to scale the revision by a metric that you specify. The HPA will be configured to use the average value of your metric over all the Pods of the revision.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/class: \"hpa.autoscaling.knative.dev\"\n        autoscaling.knative.dev/metric: \"&lt;metric-name&gt;\"\n        autoscaling.knative.dev/target: \"&lt;target&gt;\"\n</code></pre> <p>Where <code>&lt;metric-name&gt;</code> is your custom metric.</p>"},{"location":"serving/autoscaling/autoscaling-metrics/#next-steps","title":"Next steps","text":"<ul> <li>Configure concurrency targets for applications</li> <li>Configure requests per second targets for replicas of an application</li> </ul>"},{"location":"serving/autoscaling/autoscaling-targets/","title":"Targets","text":"<p>Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types.</p> <p>The <code>target</code> annotation, used to configure per-revision targets,  is metric agnostic. This means the target is simply an integer value, which can be applied for any metric type.</p>"},{"location":"serving/autoscaling/autoscaling-targets/#configuring-targets","title":"Configuring targets","text":"<ul> <li>Global settings key: <code>container-concurrency-target-default</code>. For more information, see the documentation on metrics.</li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/target</code></li> <li>Possible values: An integer (metric agnostic).</li> <li>Default: <code>\"100\"</code> for <code>container-concurrency-target-default</code>. There is no default value set for the <code>target</code> annotation.</li> </ul> Target annotation - Per-revisionConcurrency target - Global (ConfigMap)Concurrency target - Container Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/target: \"50\"\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n container-concurrency-target-default: \"200\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      container-concurrency-target-default: \"200\"\n</code></pre>"},{"location":"serving/autoscaling/concurrency/","title":"Configuring concurrency","text":"<p>Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time.</p> <p>For per-revision concurrency, you must configure both <code>autoscaling.knative.dev/metric</code>and <code>autoscaling.knative.dev/target</code> for a soft limit, or <code>containerConcurrency</code> for a hard limit.</p> <p>For global concurrency, you can set the <code>container-concurrency-target-default</code> value.</p>"},{"location":"serving/autoscaling/concurrency/#soft-versus-hard-concurrency-limits","title":"Soft versus hard concurrency limits","text":"<p>It is possible to set either a soft or hard concurrency limit.</p> <p>Note</p> <p>If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value.</p> <p>The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded.</p> <p>The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests.</p> <p>Warning</p> <p>Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts.</p>"},{"location":"serving/autoscaling/concurrency/#soft-limit","title":"Soft limit","text":"<ul> <li>Global key: <code>container-concurrency-target-default</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/target</code></li> <li>Possible values: An integer.</li> <li>Default: <code>\"100\"</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/target: \"200\"\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n container-concurrency-target-default: \"200\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      container-concurrency-target-default: \"200\"\n</code></pre>"},{"location":"serving/autoscaling/concurrency/#hard-limit","title":"Hard limit","text":"<p>The hard limit is specified per Revision using the <code>containerConcurrency</code> field on the Revision spec. This setting is not an annotation.</p> <p>There is no global setting for the hard limit in the autoscaling ConfigMap, because <code>containerConcurrency</code> has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's <code>containerConcurrency</code> field in <code>config-defaults.yaml</code>.</p> <p>The default value is <code>0</code>, meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than <code>0</code> specifies the exact number of requests that are allowed to flow to the replica at any one time.</p> <ul> <li>Global key: <code>container-concurrency</code> (in <code>config-defaults.yaml</code>)</li> <li>Per-revision spec key: <code>containerConcurrency</code></li> <li>Possible values: integer</li> <li>Default: <code>0</code>, meaning no limit</li> </ul> <p>Example:</p> Per RevisionGlobal (Defaults ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containerConcurrency: 50\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-defaults\n namespace: knative-serving\ndata:\n container-concurrency: \"50\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    defaults:\n      container-concurrency: \"50\"\n</code></pre>"},{"location":"serving/autoscaling/concurrency/#target-utilization","title":"Target utilization","text":"<p>In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value.</p> <p>This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached.</p> <p>For example, if <code>containerConcurrency</code> is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the <code>containerConcurrency</code> limit is reached.</p> <ul> <li>Global key: <code>container-concurrency-target-percentage</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/target-utilization-percentage</code></li> <li>Possible values: float</li> <li>Default: <code>70</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/target-utilization-percentage: \"80\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n container-concurrency-target-percentage: \"80\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      container-concurrency-target-percentage: \"80\"\n</code></pre>"},{"location":"serving/autoscaling/kpa-specific/","title":"Additional autoscaling configuration for Knative Pod Autoscaler","text":"<p>The following settings are specific to the Knative Pod Autoscaler (KPA).</p>"},{"location":"serving/autoscaling/kpa-specific/#modes","title":"Modes","text":"<p>The KPA acts on metrics (<code>concurrency</code> or <code>rps</code>) aggregated over time-based windows.</p> <p>These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react.</p> <p>The KPA's implementation has two modes: stable and panic. There are separate aggregate windows for each mode: <code>stable-window</code> and <code>panic-window</code>, respectively.</p> <p>Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives.</p> <p>Note</p> <p>When using panic mode, the Revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window timeframe.</p>"},{"location":"serving/autoscaling/kpa-specific/#stable-window","title":"Stable window","text":"<ul> <li>Global key: <code>stable-window</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/window</code></li> <li>Possible values: Duration, <code>6s</code> &lt;= value &lt;= <code>1h</code></li> <li>Default: <code>60s</code></li> </ul> <p>Note</p> <p>When scaling to zero Replicas, the last Replica will only be removed after there has not been any traffic to the Revision for the entire duration of the stable window.</p> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/window: \"40s\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n stable-window: \"40s\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      stable-window: \"40s\"\n</code></pre>"},{"location":"serving/autoscaling/kpa-specific/#panic-window","title":"Panic window","text":"<p>The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way.</p> <p>This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of <code>10.0</code> means that in panic mode the window will be 10% of the stable window size.</p> <ul> <li>Global key: <code>panic-window-percentage</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/panic-window-percentage</code></li> <li>Possible values: float, <code>1.0</code> &lt;= value &lt;= <code>100.0</code></li> <li>Default: <code>10.0</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/panic-window-percentage: \"20.0\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n panic-window-percentage: \"20.0\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      panic-window-percentage: \"20.0\"\n</code></pre>"},{"location":"serving/autoscaling/kpa-specific/#panic-mode-threshold","title":"Panic mode threshold","text":"<p>This threshold defines when the Autoscaler will move from stable mode into panic mode.</p> <p>This value is a percentage of the traffic that the current amount of replicas can handle.</p> <p>Note</p> <p>A value of <code>100.0</code> (100 percent) means that the Autoscaler is always in panic mode, therefore the  minimum value should be higher than <code>100.0</code>.</p> <p>The default setting of <code>200.0</code> means that panic mode will be started if traffic is twice as high as the current replica population can handle.</p> <ul> <li>Global key: <code>panic-threshold-percentage</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/panic-threshold-percentage</code></li> <li>Possible values: float, <code>110.0</code> &lt;= value &lt;= <code>1000.0</code></li> <li>Default: <code>200.0</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/panic-threshold-percentage: \"150.0\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n panic-threshold-percentage: \"150.0\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      panic-threshold-percentage: \"150.0\"\n</code></pre>"},{"location":"serving/autoscaling/kpa-specific/#scale-rates","title":"Scale rates","text":"<p>These settings control by how much the replica population can scale up or down in a single evaluation cycle.</p> <p>A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set.</p>"},{"location":"serving/autoscaling/kpa-specific/#scale-up-rate","title":"Scale up rate","text":"<p>This setting determines the maximum ratio of desired to existing pods. For example, with a value of <code>2.0</code>, the revision can only scale from <code>N</code> to <code>2*N</code> pods in one evaluation cycle.</p> <ul> <li>Global key: <code>max-scale-up-rate</code></li> <li>Per-revision annotation key: n/a</li> <li>Possible values: float</li> <li>Default: <code>1000.0</code></li> </ul> <p>Example:</p> Global (ConfigMap)Global (Operator) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n max-scale-up-rate: \"500.0\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      max-scale-up-rate: \"500.0\"\n</code></pre>"},{"location":"serving/autoscaling/kpa-specific/#scale-down-rate","title":"Scale down rate","text":"<p>This setting determines the maximum ratio of existing to desired pods. For example, with a value of <code>2.0</code>, the revision can only scale from <code>N</code> to <code>N/2</code> pods in one evaluation cycle.</p> <ul> <li>Global key: <code>max-scale-down-rate</code></li> <li>Per-revision annotation key: n/a</li> <li>Possible values: float</li> <li>Default: <code>2.0</code></li> </ul> <p>Example:</p> Global (ConfigMap)Global (Operator) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n max-scale-down-rate: \"4.0\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      max-scale-down-rate: \"4.0\"\n</code></pre>"},{"location":"serving/autoscaling/rps-target/","title":"Configuring the requests per second (RPS) target","text":"<p>This setting specifies a target for requests-per-second per replica of an application. Your revision must also be configured to use the <code>rps</code> metric annotation.</p> <ul> <li>Global key: <code>requests-per-second-target-default</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/target</code></li> <li>Possible values: An integer.</li> <li>Default: <code>\"200\"</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/target: \"150\"\n        autoscaling.knative.dev/metric: \"rps\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n requests-per-second-target-default: \"150\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      requests-per-second-target-default: \"150\"\n</code></pre>"},{"location":"serving/autoscaling/scale-bounds/","title":"Configuring scale bounds","text":"<p>You can configure upper and lower bounds to control autoscaling behavior.</p> <p>You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation.</p>"},{"location":"serving/autoscaling/scale-bounds/#lower-bound","title":"Lower bound","text":"<p>This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time.</p> <ul> <li>Global key: <code>min-scale</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/min-scale</code></li> <li>Possible values: integer</li> <li>Default: <code>0</code> if scale-to-zero is enabled and class KPA is used, <code>1</code> otherwise</li> </ul> <p>Note</p> <p>For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero.</p> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/min-scale: \"3\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-autoscaler\n  namespace: knative-serving\ndata:\n  min-scale: \"3\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      min-scale: \"3\"\n</code></pre>"},{"location":"serving/autoscaling/scale-bounds/#upper-bound","title":"Upper bound","text":"<p>This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time.</p> <p>If the <code>max-scale-limit</code> global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When <code>max-scale-limit</code> is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed.</p> <ul> <li>Global key: <code>max-scale</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/max-scale</code></li> <li>Possible values: integer</li> <li>Default: <code>0</code> which means unlimited</li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/max-scale: \"3\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-autoscaler\n  namespace: knative-serving\ndata:\n  max-scale: \"3\"\n  max-scale-limit: \"100\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      max-scale: \"3\"\n      max-scale-limit: \"100\"\n</code></pre>"},{"location":"serving/autoscaling/scale-bounds/#initial-scale","title":"Initial scale","text":"<p>This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as <code>Ready</code>. After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale.</p> <p>When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale.</p> <ul> <li>Global key: <code>initial-scale</code> in combination with <code>allow-zero-initial-scale</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/initial-scale</code></li> <li>Possible values: integer</li> <li>Default: <code>1</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/initial-scale: \"0\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-autoscaler\n  namespace: knative-serving\ndata:\n  initial-scale: \"0\"\n  allow-zero-initial-scale: \"true\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      initial-scale: \"0\"\n      allow-zero-initial-scale: \"true\"\n</code></pre>"},{"location":"serving/autoscaling/scale-bounds/#scale-up-minimum","title":"Scale Up Minimum","text":"<p>This value controls the minimum number of replicas that will be created when the Revision scales up from zero. After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the activation scale is reached if the actual traffic received needs a smaller scale.</p> <p>When the Revision is created, the larger of activation scale and lower bound is automatically chosen as the initial target scale.</p> <ul> <li>Global key: n/a</li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/activation-scale</code></li> <li>Possible values: integer</li> <li>Default: <code>1</code></li> </ul> <p>Example:</p> Per Revision <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/activation-scale: \"5\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre>"},{"location":"serving/autoscaling/scale-bounds/#scale-down-delay","title":"Scale Down Delay","text":"<p>Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. </p> <p>Note</p> <p>Only supported for the default KPA autoscaler class.</p> <ul> <li>Global key: <code>scale-down-delay</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/scale-down-delay</code></li> <li>Possible values: Duration, <code>0s</code> &lt;= value &lt;= <code>1h</code></li> <li>Default: <code>0s</code> (no delay)</li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/scale-down-delay: \"15m\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-autoscaler\n  namespace: knative-serving\ndata:\n  scale-down-delay: \"15m\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      scale-down-delay: \"15m\"\n</code></pre>"},{"location":"serving/autoscaling/scale-bounds/#stable-window","title":"Stable window","text":"<p>The stable window defines the sliding time window over which metrics are averaged to provide the input for scaling decisions when the autoscaler is not in Panic mode.</p> <ul> <li>Global key: <code>stable-window</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/window</code></li> <li>Possible values: Duration, <code>6s</code> &lt;= value &lt;= <code>1h</code></li> <li>Default: <code>60s</code></li> </ul> <p>Note</p> <p>During scale down, in most cases the last Replica is removed after there has been no traffic to the Revision for the entire duration of the stable window.</p> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/window: \"40s\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n stable-window: \"40s\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      stable-window: \"40s\"\n</code></pre>"},{"location":"serving/autoscaling/scale-to-zero/","title":"Configuring scale to zero","text":"<p>Warning</p> <p>Scale to zero can only be enabled if you are using the KnativePodAutoscaler (KPA), and can only be configured globally. For more information about using KPA or global configuration, see the documentation on Supported Autoscaler types.</p>"},{"location":"serving/autoscaling/scale-to-zero/#enable-scale-to-zero","title":"Enable scale to zero","text":"<p>The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to <code>true</code>), or stop at 1 replica if set to <code>false</code>.</p> <p>Note</p> <p>For more information about scale bounds configuration per Revision, see the documentation on Configuring scale bounds.</p> <ul> <li>Global key: <code>enable-scale-to-zero</code></li> <li>Per-revision annotation key: No per-revision setting.</li> <li>Possible values: boolean</li> <li>Default: <code>true</code></li> </ul> <p>Example:</p> Global (ConfigMap)Global (Operator) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n enable-scale-to-zero: \"false\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      enable-scale-to-zero: \"false\"\n</code></pre>"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-grace-period","title":"Scale to zero grace period","text":"<p>This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed.</p> <p>Warning</p> <p>This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you experience issues with requests being dropped while a Revision is scaling to zero Replicas.</p> <p>This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration.</p> <ul> <li>Global key: <code>scale-to-zero-grace-period</code></li> <li>Per-revision annotation key: n/a</li> <li>Possible values: Duration</li> <li>Default: <code>30s</code></li> </ul> <p>Example:</p> Global (ConfigMap)Global (Operator) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n scale-to-zero-grace-period: \"40s\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      scale-to-zero-grace-period: \"40s\"\n</code></pre>"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period","title":"Scale to zero last pod retention period","text":"<p>The <code>scale-to-zero-pod-retention-period</code> flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero.</p> <p>This contrasts with the <code>scale-to-zero-grace-period</code> flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero.</p> <ul> <li>Global key: <code>scale-to-zero-pod-retention-period</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/scale-to-zero-pod-retention-period</code></li> <li>Possible values: Non-negative duration string</li> <li>Default: <code>0s</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/scale-to-zero-pod-retention-period: \"1m5s\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-autoscaler\n namespace: knative-serving\ndata:\n scale-to-zero-pod-retention-period: \"42s\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      scale-to-zero-pod-retention-period: \"42s\"\n</code></pre>"},{"location":"serving/autoscaling/autoscale-go/","title":"Autoscale Sample App - Go","text":"<p>A demonstration of the autoscaling capabilities of a Knative Serving Revision.</p>"},{"location":"serving/autoscaling/autoscale-go/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Kubernetes cluster with Knative Serving)    installed.</li> <li>The <code>hey</code> load generator installed (<code>go install github.com/rakyll/hey@latest</code>).</li> <li> <p>Clone this repository, and move into the sample directory:</p> <pre><code>git clone -b \"main\" https://github.com/knative/docs knative-docs\ncd knative-docs\n</code></pre> </li> </ol>"},{"location":"serving/autoscaling/autoscale-go/#deploy-the-service","title":"Deploy the Service","text":"<ol> <li> <p>Deploy the sample Knative Service:</p> <pre><code>kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml\n</code></pre> </li> <li> <p>Obtain the URL of the service (once <code>Ready</code>):</p> <pre><code>$ kubectl get ksvc autoscale-go\nNAME            URL                                                LATESTCREATED         LATESTREADY           READY   REASON\nautoscale-go    http://autoscale-go.default.1.2.3.4.sslip.io    autoscale-go-96dtk    autoscale-go-96dtk    True\n</code></pre> </li> </ol>"},{"location":"serving/autoscaling/autoscale-go/#load-the-service","title":"Load the Service","text":"<ol> <li> <p>Make a request to the autoscale app to see it consume some resources.</p> <pre><code>curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&amp;prime=10000&amp;bloat=5\"\n</code></pre> <pre><code>Allocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.13 milliseconds.\n</code></pre> </li> <li> <p>Send 30 seconds of traffic maintaining 50 in-flight requests.</p> <pre><code>hey -z 30s -c 50 \\\n  \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&amp;prime=10000&amp;bloat=5\" \\\n  &amp;&amp; kubectl get pods\n</code></pre> <pre><code>Summary:\n  Total:        30.3379 secs\n  Slowest:      0.7433 secs\n  Fastest:      0.1672 secs\n  Average:      0.2778 secs\n  Requests/sec: 178.7861\n\n  Total data:   542038 bytes\n  Size/request: 99 bytes\n\nResponse time histogram:\n  0.167 [1]     |\n  0.225 [1462]  |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\n  0.282 [1303]  |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\n  0.340 [1894]  |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\n  0.398 [471]   |\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\n  0.455 [159]   |\u25a0\u25a0\u25a0\n  0.513 [68]    |\u25a0\n  0.570 [18]    |\n  0.628 [14]    |\n  0.686 [21]    |\n  0.743 [13]    |\n\nLatency distribution:\n  10% in 0.1805 secs\n  25% in 0.2197 secs\n  50% in 0.2801 secs\n  75% in 0.3129 secs\n  90% in 0.3596 secs\n  95% in 0.4020 secs\n  99% in 0.5457 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:   0.0007 secs, 0.1672 secs, 0.7433 secs\n  DNS-lookup:   0.0000 secs, 0.0000 secs, 0.0000 secs\n  req write:    0.0001 secs, 0.0000 secs, 0.0045 secs\n  resp wait:    0.2766 secs, 0.1669 secs, 0.6633 secs\n  resp read:    0.0002 secs, 0.0000 secs, 0.0065 secs\n\nStatus code distribution:\n  [200] 5424 responses\n</code></pre> <pre><code>NAME                                             READY   STATUS    RESTARTS   AGE\nautoscale-go-00001-deployment-78cdc67bf4-2w4sk   3/3     Running   0          26s\nautoscale-go-00001-deployment-78cdc67bf4-dd2zb   3/3     Running   0          24s\nautoscale-go-00001-deployment-78cdc67bf4-pg55p   3/3     Running   0          18s\nautoscale-go-00001-deployment-78cdc67bf4-q8bf9   3/3     Running   0          1m\nautoscale-go-00001-deployment-78cdc67bf4-thjbq   3/3     Running   0          26s\n</code></pre> </li> </ol>"},{"location":"serving/autoscaling/autoscale-go/#analysis","title":"Analysis","text":""},{"location":"serving/autoscaling/autoscale-go/#algorithm","title":"Algorithm","text":"<p>Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100(Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods (<code>50 concurrent requests / target of 10 = 5 pods</code>)</p>"},{"location":"serving/autoscaling/autoscale-go/#panic","title":"Panic","text":"<p>The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stabilize at the desired level of concurrency. However the autoscaler also calculates a 6 second <code>panic</code> window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second <code>stable</code> window.</p> <pre><code>                                                       |\n                                  Panic Target---&gt;  +--| 20\n                                                    |  |\n                                                    | &lt;------Panic Window\n                                                    |  |\n       Stable Target---&gt;  +-------------------------|--| 10   CONCURRENCY\n                          |                         |  |\n                          |                      &lt;-----------Stable Window\n                          |                         |  |\n--------------------------+-------------------------+--+ 0\n120                       60                           0\n                     TIME\n</code></pre>"},{"location":"serving/autoscaling/autoscale-go/#customization","title":"Customization","text":"<p>The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative:</p> <ol> <li><code>kpa.autoscaling.knative.dev</code> which is the concurrency-based autoscaler    described earlier (the default), and</li> <li><code>hpa.autoscaling.knative.dev</code> which delegates to the Kubernetes HPA which    autoscales on CPU usage.</li> </ol> <p>Example of a Service scaled on CPU:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: autoscale-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        # Standard Kubernetes CPU-based autoscaling.\n        autoscaling.knative.dev/class: hpa.autoscaling.knative.dev\n        autoscaling.knative.dev/metric: cpu\n    spec:\n      containers:\n        - image: ghcr.io/knative/autoscale-go:latest\n</code></pre> <p>Additionally the autoscaler targets and scaling bounds can be specified in    annotations. Example of a Service with custom targets and scale bounds:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: autoscale-go\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        # Knative concurrency-based autoscaling (default).\n        autoscaling.knative.dev/class: kpa.autoscaling.knative.dev\n        autoscaling.knative.dev/metric: concurrency\n        # Target 10 requests in-flight per pod.\n        autoscaling.knative.dev/target: \"10\"\n        # Disable scale to zero with a min scale of 1.\n        autoscaling.knative.dev/min-scale: \"1\"\n        # Limit scaling to 100 pods.\n        autoscaling.knative.dev/max-scale: \"100\"\n    spec:\n      containers:\n        - image: ghcr.io/knative/autoscale-go:latest\n</code></pre> <p>Note</p> <p>For an <code>hpa.autoscaling.knative.dev</code> class Service, the <code>autoscaling.knative.dev/target</code> specifies the CPU percentage target (default <code>\"80\"</code>).</p>"},{"location":"serving/autoscaling/autoscale-go/#demo","title":"Demo","text":"<p>View the Kubecon Demo of Knative autoscaler customization (32 minutes).</p>"},{"location":"serving/autoscaling/autoscale-go/#other-experiments","title":"Other Experiments","text":"<ol> <li> <p>Send 60 seconds of traffic maintaining 100 concurrent requests.</p> <pre><code>hey -z 60s -c 100 \\\n  \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&amp;prime=10000&amp;bloat=5\"\n</code></pre> </li> <li> <p>Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms).</p> <pre><code>hey -z 60s -q 100 \\\n  \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\"\n</code></pre> </li> <li> <p>Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec).</p> <pre><code>hey -z 60s -q 100 \\\n  \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\"\n</code></pre> </li> <li> <p>Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total    100 cpus).</p> <pre><code>hey -z 60s -q 100 \\\n  \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\"\n</code></pre> </li> <li> <p>Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5    gb).</p> <pre><code>hey -z 60s -c 5 \\\n  \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\"\n</code></pre> </li> </ol>"},{"location":"serving/autoscaling/autoscale-go/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml\n</code></pre>"},{"location":"serving/autoscaling/autoscale-go/#further-reading","title":"Further reading","text":"<p>Autoscaling Developer Documentation</p>"},{"location":"serving/configuration/config-defaults/","title":"Configuring the Defaults ConfigMap","text":"<p>The <code>config-defaults</code> ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources.</p> <p>This ConfigMap is located in the <code>knative-serving</code> namespace.</p> <p>You can view the current <code>config-defaults</code> ConfigMap by running the following command:</p> <pre><code>kubectl get configmap -n knative-serving config-defaults -oyaml\n</code></pre>"},{"location":"serving/configuration/config-defaults/#example-config-defaults-configmap","title":"Example config-defaults ConfigMap","text":"<pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-timeout-seconds: \"300\"\n  max-revision-timeout-seconds: \"600\"\n  revision-response-start-timeout-seconds: \"300\"\n  revision-idle-timeout-seconds: \"0\"  # infinite\n  revision-cpu-request: \"400m\"\n  revision-memory-request: \"100M\"\n  revision-ephemeral-storage-request: \"500M\"\n  revision-cpu-limit: \"1000m\"\n  revision-memory-limit: \"200M\"\n  revision-ephemeral-storage-limit: \"750M\"\n  container-name-template: \"user-container\"\n  container-concurrency: \"0\"\n  container-concurrency-max-limit: \"1000\"\n  allow-container-concurrency-zero: \"true\"\n  enable-service-links: \"false\"\n</code></pre> <p>See below for a description of each property.</p>"},{"location":"serving/configuration/config-defaults/#properties","title":"Properties","text":""},{"location":"serving/configuration/config-defaults/#revision-timeout-seconds","title":"Revision timeout seconds","text":"<p>The revision timeout value determines the default number of seconds to use for the revision's per-request timeout if none is specified.</p> <ul> <li>Global key: <code>revision-timeout-seconds</code></li> <li>Per-revision spec key: <code>timeoutSeconds</code></li> <li>Possible values: integer</li> <li>Default: <code>\"300\"</code> (5 minutes)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-timeout-seconds: \"300\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\n  spec:\n    template:\n      spec:\n        timeoutSeconds: 300\n        containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre>"},{"location":"serving/configuration/config-defaults/#max-revision-timeout-seconds","title":"Max revision timeout seconds","text":"<p>The <code>max-revision-timeout-seconds</code> value determines the maximum number of seconds that can be used for <code>revision-timeout-seconds</code>. This value must be greater than or equal to <code>revision-timeout-seconds</code>. If omitted, the system default is used (600 seconds).</p> <p>If this value is increased, the activator's <code>terminationGracePeriodSeconds</code> should also be increased to prevent in-flight requests from being disrupted.</p> <ul> <li>Global key: <code>max-revision-timeout-seconds</code></li> <li>Per-revision annotation key: N/A</li> <li>Possible values: integer</li> <li>Default: <code>\"600\"</code> (10 minutes)</li> </ul> <p>Example:</p> Global (ConfigMap) <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  max-revision-timeout-seconds: \"600\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-response-start-timeout-seconds","title":"Revision response start timeout seconds","text":"<p>The revision response start timeout value determines the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin sending any network traffic. If omitted, the system default is used (300 seconds).</p> <ul> <li>Global key: <code>revision-response-start-timeout-seconds</code></li> <li>Per-revision spec key: <code>responseStartTimeoutSeconds</code></li> <li>Possible values: integer</li> <li>Default: <code>\"300\"</code> (5 minutes)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-response-start-timeout-seconds: \"300\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\n  spec:\n    template:\n      spec:\n        responseStartTimeoutSeconds: 300\n        containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-idle-timeout-seconds","title":"Revision idle timeout seconds","text":"<p>The revision idle timeout value determines the maximum duration in seconds a request will be allowed to stay open while not receiving any bytes from the user's application. If omitted, the system default is used (infinite).</p> <ul> <li>Global key: <code>revision-idle-timeout-seconds</code></li> <li>Per-revision spec key: <code>idleTimeoutSeconds</code></li> <li>Possible values: integer</li> <li>Default: <code>\"0\"</code> (infinite)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-idle-timeout-seconds: \"0\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\n  spec:\n    template:\n      spec:\n        idleTimeoutSeconds: 0\n        containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-cpu-request","title":"Revision CPU request","text":"<p>The <code>revision-cpu-request</code> value determines the CPU allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative.</p> <ul> <li>Global key: <code>revision-cpu-request</code></li> <li>Per-revision annotation key: <code>cpu</code></li> <li>Possible values: integer</li> <li>Default: <code>\"400m\"</code> (0.4 of a CPU, or 400 milli-CPU)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-cpu-request: \"400m\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          resources:\n            requests:\n              cpu: \"400m\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-memory-request","title":"Revision memory request","text":"<p>The <code>revision-memory-request</code> value determines the memory allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative.</p> <ul> <li>Global key: <code>revision-memory-request</code></li> <li>Per-revision annotation key: <code>memory</code></li> <li>Possible values: integer</li> <li>Default: <code>\"100M\"</code> (100 megabytes of memory)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-memory-request: \"100M\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          resources:\n            requests:\n              memory: \"100M\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-ephemeral-storage-request","title":"Revision Ephemeral Storage Request","text":"<p>The <code>revision-ephemeral-storage-request</code> value determines the ephemeral storage allocation assigned to revisions by default. If this value is omitted, the system default is used. This key is not enabled by default for Knative.</p> <ul> <li>Global key: <code>revision-ephemeral-storage-request</code></li> <li>Per-revision annotation key: <code>ephemeral-storage</code></li> <li>Possible values: integer</li> <li>Default: <code>\"500M\"</code> (500 megabytes of storage)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-ephemeral-storage-request: \"500M\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          resources:\n            requests:\n              ephemeral-storage: \"500M\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-cpu-limit","title":"Revision CPU limit","text":"<p>The <code>revision-cpu-limit</code> value determines the default CPU allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative.</p> <ul> <li>Global key: <code>revision-cpu-limit</code></li> <li>Per-revision annotation key: <code>cpu</code></li> <li>Possible values: integer</li> <li>Default: <code>\"1000m\"</code> (1 CPU, or 1000 milli-CPU)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-cpu-limit: \"1000m\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          resources:\n            requests:\n              cpu: \"1000m\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-memory-limit","title":"Revision memory limit","text":"<p>The <code>revision-memory-limit</code> value determines the default memory allocation limit for revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative.</p> <ul> <li>Global key: <code>revision-memory-limit</code></li> <li>Per-revision annotation key: <code>memory</code></li> <li>Possible values: integer</li> <li>Default: <code>\"200M\"</code> (200 megabytes of memory)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-memory-limit: \"200M\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          resources:\n            requests:\n              memory: \"200M\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#revision-ephemeral-storage-limit","title":"Revision Ephemeral Storage Limit","text":"<p>The <code>revision-ephemeral-storage-limit</code> value determines the default ephemeral storage limit allocated to revisions. If this value is omitted, the system default is used. This key is not enabled by default for Knative.</p> <ul> <li>Global key: <code>revision-ephemeral-storage-limit</code></li> <li>Per-revision annotation key: <code>ephemeral-storage</code></li> <li>Possible values: integer</li> <li>Default: <code>\"750M\"</code> (750 megabytes of storage)</li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  revision-ephemeral-storage-limit: \"750M\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          resources:\n            requests:\n              ephemeral-storage: \"750M\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#container-name-template","title":"Container name template","text":"<p>The <code>container-name-template</code> value provides a template for the default container name if no container name is specified. This field supports Go templating and is supplied by the <code>ObjectMeta</code> of the enclosing Service or Configuration, so values such as <code>{{.Name}}</code> are also valid.</p> <ul> <li>Global key: <code>container-name-template</code></li> <li>Per-revision annotation key: <code>name</code></li> <li>Possible values: string</li> <li>Default: <code>\"user-container\"</code></li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  container-name-template: \"user-container\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - name: user-container\n          image: ghcr.io/knative/helloworld-go:latest\n</code></pre>"},{"location":"serving/configuration/config-defaults/#container-concurrency","title":"Container concurrency","text":"<p>The <code>container-concurrency</code> value specifies the maximum number of requests the container can handle at once. Requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives.</p> <ul> <li>Global key: <code>container-concurrency</code></li> <li>Per-revision spec key: <code>containerConcurrency</code></li> <li>Possible values: integer</li> <li>Default: <code>\"0\"</code></li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  container-concurrency: \"0\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containerConcurrency: 0\n</code></pre>"},{"location":"serving/configuration/config-defaults/#container-concurrency-max-limit","title":"Container concurrency max limit","text":"<p>The <code>container-concurrency-max-limit</code> setting disables arbitrary large concurrency values, or autoscaling targets, for individual revisions. The <code>container-concurrency</code> default setting must be at or below this value. The value of the <code>container-concurrency-max-limit</code> setting must be greater than 1.</p> <p>Note</p> <p>Even with this set, a user can choose a <code>containerConcurrency</code> value of zero (unbounded), unless <code>allow-container-concurrency-zero</code> is set to <code>\"false\"</code>.</p> <ul> <li>Global key: <code>container-concurrency-max-limit</code></li> <li>Per-revision annotation key: N/A</li> <li>Possible values: integer</li> <li>Default: <code>\"1000\"</code></li> </ul> <p>Example:</p> Global (ConfigMap)Global (Operator) <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  container-concurrency-max-limit: \"1000\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1beta1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\nspec:\n  config:\n    defaults:\n      container-concurrency-max-limit: \"1000\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#allow-container-concurrency-zero","title":"Allow container concurrency zero","text":"<p>The <code>allow-container-concurrency-zero</code> value determines whether users can specify <code>0</code> (unbounded) for <code>containerConcurrency</code>.</p> <ul> <li>Global key: <code>allow-container-concurrency-zero</code></li> <li>Per-revision annotation key: N/A</li> <li>Possible values: boolean</li> <li>Default: <code>\"true\"</code></li> </ul> <p>Example:</p> Global (ConfigMap) <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  allow-container-concurrency-zero: \"true\"\n</code></pre>"},{"location":"serving/configuration/config-defaults/#enable-service-links","title":"Enable Service links","text":"<p>The <code>enable-service-links</code> value specifies the default value used for the <code>enableServiceLinks</code> field of the <code>PodSpec</code> when it is omitted by the user. See the Kubernetes documentation about the <code>enableServiceLinks</code> feature.</p> <p>This is a tri-state flag with possible values of (true|false|default).</p> <p>In environments with large number of Services, it is suggested to set this value to <code>false</code>. See serving#8498.</p> <ul> <li>Global key: <code>enable-service-links</code></li> <li>Per-revision annotation key: N/A</li> <li>Possible values: <code>true|false|default</code></li> <li>Default: <code>\"false\"</code></li> </ul> <p>Example:</p> Global (ConfigMap) <pre><code>apiVersion:  v1\nkind:  ConfigMap\nmetadata:\n  name:  config-defaults\n  namespace:  knative-serving\ndata:\n  enable-service-links: \"false\"\n</code></pre>"},{"location":"serving/configuration/deployment/","title":"Configure Deployment resources","text":"<p>The <code>config-deployment</code> ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes <code>Deployment</code> resources, which back Knative services, are configured. This ConfigMap is located in the <code>knative-serving</code> namespace.</p> <p>You can view the current <code>config-deployment</code> ConfigMap by running the following command:</p> <pre><code>kubectl get configmap -n knative-serving config-deployment -oyaml\n</code></pre>"},{"location":"serving/configuration/deployment/#example-config-deployment-configmap","title":"Example <code>config-deployment</code> ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-deployment\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\n  annotations:\n    knative.dev/example-checksum: \"fa67b403\"\ndata:\n  # This is the Go import path for the binary that is containerized\n  # and substituted here.\n  queue-sidecar-image: ko://knative.dev/serving/cmd/queue\n  # List of repositories for which tag to digest resolving should be skipped\n  registries-skipping-tag-resolving: \"kind.local,ko.local,dev.local\"\n  # digest-resolution-timeout is the maximum time allowed for an image's\n  # digests to be resolved.\n  digest-resolution-timeout: \"10s\"\n  # progress-deadline is the duration we wait for the deployment to\n  # be ready before considering it failed.\n  progress-deadline: \"600s\"\n  # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container.\n  # If omitted, a default value (currently \"25m\"), is used.\n  queue-sidecar-cpu-request: \"25m\"\n  # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container.\n  # If omitted, no value is specified and the system default is used.\n  queue-sidecar-cpu-limit: \"1000m\"\n  # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container.\n  # If omitted, no value is specified and the system default is used.\n  queue-sidecar-memory-request: \"400Mi\"\n  # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container.\n  # If omitted, no value is specified and the system default is used.\n  queue-sidecar-memory-limit: \"800Mi\"\n  # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to\n  # set for the queue proxy sidecar container.\n  # If omitted, no value is specified and the system default is used.\n  queue-sidecar-ephemeral-storage-request: \"512Mi\"\n  # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set\n  # for the queue proxy sidecar container.\n  # If omitted, no value is specified and the system default is used.\n  queue-sidecar-ephemeral-storage-limit: \"1024Mi\"\n  # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or\n  # scales up from zero.\n  concurrency-state-endpoint: \"\"\n</code></pre>"},{"location":"serving/configuration/deployment/#configuring-progress-deadlines","title":"Configuring progress deadlines","text":"<p>Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision.</p> <p>The default progress deadline is 600 seconds. This value is expressed as a Golang <code>time.Duration</code> string representation, and must be rounded to a second precision.</p> <p>The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal <code>Failed</code> state, if the initial scale cannot be achieved within the time limit defined by this setting.</p> <p>You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment:</p> <ul> <li>It takes a long time to pull the Service image, due to the size of the image.</li> <li>It takes a long time for the Service to become <code>READY</code>, due to priming of the initial cache state.</li> <li>The cluster is relies on cluster autoscaling to allocate resources for new pods.</li> </ul> <p>See the Kubernetes documentation for more information.</p> <p>Progress deadline setting can be configured at global level through a ConfigMap or at the per-revision level using an annotation.</p> <ul> <li>Global key: <code>progress-deadline</code></li> <li>Per-revision annotation key: <code>serving.knative.dev/progress-deadline</code></li> <li>Possible values: <code>time.Duration</code></li> <li>Default: <code>\"600s\"</code></li> </ul> <p>Example:</p> Global (ConfigMap)Per Revision <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-deployment\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\n  annotations:\n    knative.dev/example-checksum: \"fa67b403\"\ndata:\n  progress-deadline: \"10m\"\n</code></pre> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\nspec:\n  template:\n    metadata:\n      annotations:\n        serving.knative.dev/progress-deadline: \"60s\" \n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n</code></pre>"},{"location":"serving/configuration/deployment/#skipping-tag-resolution","title":"Skipping tag resolution","text":"<p>You can configure Knative Serving to skip tag resolution for Deployments by modifying the <code>registries-skipping-tag-resolving</code> ConfigMap setting.</p> <p>The following example shows how to disable tag resolution for <code>registry.example.com</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-deployment\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\n  annotations:\n    knative.dev/example-checksum: \"fa67b403\"\ndata:\n  # List of repositories for which tag to digest resolving should be skipped\n  registries-skipping-tag-resolving: registry.example.com\n</code></pre>"},{"location":"serving/configuration/feature-flags/","title":"Feature and extension flags","text":"<p>The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice.</p> <p>This document introduces two concepts:</p> Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API."},{"location":"serving/configuration/feature-flags/#configuring-flags","title":"Configuring flags","text":"<p>Features and extensions are controlled by flags.</p> <p>You can define flags in the <code>config-features</code> ConfigMap in the <code>knative-serving</code> namespace.</p> <p>Flags can have the following values:</p> Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used."},{"location":"serving/configuration/feature-flags/#lifecycle","title":"Lifecycle","text":"<p>When features and extensions are introduced to Knative, they follow a lifecycle of three stages:</p> Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases."},{"location":"serving/configuration/feature-flags/#feature-lifecycle-stages","title":"Feature lifecycle stages","text":"<p>Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages:</p> Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative."},{"location":"serving/configuration/feature-flags/#extension-lifecycle-stages","title":"Extension lifecycle stages","text":"<p>An extension surfaces details of a specific Knative implementation, or features of the underlying environment.</p> <p>Note</p> <p>Extensions are never included in the core Knative API due to their lack of portability.</p> <p>Each extension is always controlled by a flag and is never enabled by default.</p> Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default."},{"location":"serving/configuration/feature-flags/#available-flags","title":"Available Flags","text":""},{"location":"serving/configuration/feature-flags/#multiple-containers","title":"Multiple containers","text":"<ul> <li>Type: Feature</li> <li>ConfigMap key: <code>multi-container</code></li> </ul> <p>This flag allows specifying multiple user containers in a Knative Service spec.</p> <p>Only one container can handle requests, so exactly one container must have a <code>port</code> specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      containers:\n        - name: first-container\n          image: ghcr.io/knative/helloworld-go:latest\n          ports:\n            - containerPort: 8080\n        - name: second-container\n          image: gcr.io/knative-samples/helloworld-java\n</code></pre>"},{"location":"serving/configuration/feature-flags/#multiple-container-probing","title":"Multiple Container Probing","text":"<ul> <li>Type: Feature</li> <li>ConfigMap key: <code>multi-container-probing</code></li> </ul> <p>This flag allows specifying probes (readiness/liveness) for multiple containers in a Knative Service spec. Please use this feature flag in combination with multiple containers above.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      containers:\n        - name: first-container\n          image: ghcr.io/knative/helloworld-go:latest\n          ports:\n            - containerPort: 8080\n          readinessProbe:\n            httpGet:\n              port: 8080\n        - name: second-container\n          image: gcr.io/knative-samples/helloworld-java\n          readinessProbe:\n            httpGet:\n              port: 8090\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-emptydir-volume","title":"Kubernetes EmptyDir Volume","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-volumes-emptydir</code></li> </ul> <p>This extension controls whether <code>emptyDir</code> volumes can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      containers:\n          ...\n          volumeMounts:\n            - name: cache\n              mountPath: /cache\n      volumes:\n        - name: cache\n          emptyDir: {}\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-persistentvolumeclaim-pvc","title":"Kubernetes PersistentVolumeClaim (PVC)","text":"<ul> <li>Type: Extension</li> <li>ConfigMap keys: <code>kubernetes.podspec-persistent-volume-claim</code> <code>kubernetes.podspec-persistent-volume-write</code></li> </ul> <p>This extension controls whether <code>PersistentVolumeClaim (PVC)</code> can be specified and whether write access is allowed for the corresponding volume.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n template:\n   spec:\n     containers:\n         ...\n         volumeMounts:\n           - mountPath: /data\n             name: mydata\n             readOnly: true\n     volumes:\n       - name: mydata\n         persistentVolumeClaim:\n           claimName: minio-pv-claim\n           readOnly: true\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-node-affinity","title":"Kubernetes node affinity","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-affinity</code></li> </ul> <p>This extension controls whether node affinity can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: kubernetes.io/e2e-az-name\n                operator: In\n                values:\n                - e2e-az1\n                - e2e-az2\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-host-aliases","title":"Kubernetes host aliases","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-hostaliases</code></li> </ul> <p>This flag controls whether host aliases can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      hostAliases:\n      - ip: \"127.0.0.1\"\n        hostnames:\n        - \"foo.local\"\n        - \"bar.local\"\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-node-selector","title":"Kubernetes node selector","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-nodeselector</code></li> </ul> <p>This flag controls whether node selector can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      nodeSelector:\n        labelName: labelValue\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-toleration","title":"Kubernetes toleration","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-tolerations</code></li> </ul> <p>This flag controls whether tolerations can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      tolerations:\n      - key: \"example-key\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-downward-api","title":"Kubernetes Downward API","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-fieldref</code></li> </ul> <p>This flag controls whether the Downward API (environment variable based) can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      containers:\n        - name: user-container\n          image: ghcr.io/knative/helloworld-go:latest\n          env:\n            - name: MY_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-priority-class-name","title":"Kubernetes priority class name","text":"<ul> <li>Type: extension</li> <li>ConfigMap key: <code>kubernetes.podspec-priorityclassname</code></li> </ul> <p>This flag controls whether the <code>priorityClassName</code> can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      priorityClassName: high-priority\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-dry-run","title":"Kubernetes dry run","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-dryrun</code></li> </ul> <p>This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object.</p> <p>When this extension is <code>enabled</code>, the server always runs this validation.</p> <p>When this extension is <code>allowed</code>, the server does not run this validation by default.</p> <p>When this extension is <code>allowed</code>, you can run this validation for individual Services, by adding the <code>features.knative.dev/podspec-dryrun: enabled</code> annotation:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  annotations: features.knative.dev/podspec-dryrun: enabled\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-runtime-class","title":"Kubernetes runtime class","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-runtimeclassname</code></li> </ul> <p>This flag controls whether the runtime class can be used.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      runtimeClassName: myclass\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context","title":"Kubernetes security context","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-securitycontext</code></li> </ul> <p>This flag controls whether a subset of the security context can be used.</p> <p>When set to <code>enabled</code> or <code>allowed</code>, the following <code>PodSecurityContext</code> properties are permitted:</p> <ul> <li>FSGroup</li> <li>RunAsGroup</li> <li>RunAsNonRoot</li> <li>SupplementalGroups</li> <li>RunAsUser</li> </ul> <p>When set to <code>enabled</code> or <code>allowed</code>, the following container <code>SecurityContext</code> properties are permitted:</p> <ul> <li><code>RunAsNonRoot</code> (also allowed without this flag only when set to true)</li> <li><code>RunAsGroup</code></li> <li><code>RunAsUser</code> (already allowed without this flag)</li> </ul> <p>Warning</p> <p>Use this flag with caution. <code>PodSecurityContext</code> properties can affect non-user sidecar containers that come from Knative or your service mesh.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1000\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context-capabilities","title":"Kubernetes security context capabilities","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.containerspec-addcapabilities</code></li> </ul> <p>This flag controls whether users can add capabilities on the <code>securityContext</code> of the container.</p> <p>When set to <code>enabled</code> or <code>allowed</code> it allows Linux capabilities to be added to the container.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\nspec:\n template:\n  spec:\n   containers:\n     - image: ghcr.io/knative/helloworld-go:latest\n       env:\n         - name: TARGET\n           value: \"Go Sample v1\"\n       securityContext:\n         capabilities:\n           add:\n             - NET_BIND_SERVICE\n</code></pre>"},{"location":"serving/configuration/feature-flags/#tag-header-based-routing","title":"Tag header based routing","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>tag-header-based-routing</code></li> </ul> <p>This flags controls whether tag header based routing is enabled.</p>"},{"location":"serving/configuration/feature-flags/#kubernetes-init-containers","title":"Kubernetes init containers","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-init-containers</code></li> </ul> <p>This flag controls whether init containers can be used.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      ...\n      initContainers:\n        - name: init-myservice\n          image: busybox\n          command: ['sh', '-c', \"service_setup.sh\"]\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#queue-proxy-pod-info","title":"Queue Proxy Pod Info","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>queueproxy.mount-podinfo</code></li> </ul> <p>You must set this feature to either \"enabled or \"allowed\" when using QPOptions. The flag controls whether Knative mounts the <code>pod-info</code> volume to the <code>queue-proxy</code> container.</p> <p>Mounting the <code>pod-info</code> volume allows extensions that use QPOptions to access the Service annotations, by reading the <code>/etc/podinfo/annnotations</code> file. See Extending Queue Proxy image with QPOptions for more details.</p> <p>When this feature is <code>enabled</code>, the <code>pod-info</code> volume is always mounted. This is helpful in case where all or most of the cluster Services are required to use extensions that rely on QPOptions.</p> <p>When this feature is <code>allowed</code>, the <code>pod-info</code> volume is not mounted by default. Instead, the volume is mounted only for Services that add the <code>features.knative.dev/queueproxy-podinfo: enabled</code> annotation as shown below:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  annotations: features.knative.dev/queueproxy-podinfo: enabled\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-topology-spread-constraints","title":"Kubernetes Topology Spread Constraints","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-topologyspreadconstraints</code></li> </ul> <p>This flag controls whether <code>topology spread constraints</code> can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      ...\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: node\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            foo: bar\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-dns-policy","title":"Kubernetes DNS Policy","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-dnspolicy</code></li> </ul> <p>This flag controls whether a <code>DNS policy</code> can be specified.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      dnsPolicy: ClusterFirstWithHostNet\n...\n</code></pre>"},{"location":"serving/configuration/feature-flags/#kubernetes-scheduler-name","title":"Kubernetes Scheduler Name","text":"<ul> <li>Type: Extension</li> <li>ConfigMap key: <code>kubernetes.podspec-schedulername</code></li> </ul> <p>This flag controls whether a <code>scheduler name</code> can be specified. <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      ...\n      schedulerName: custom-scheduler-example\n...\n</code></pre></p>"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/","title":"Configuring gradual rollout of traffic to Revisions","text":"<p>If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision.</p> <p>This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP.</p> <p>Knative provides a <code>rollout-duration</code> parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic.</p> <p>Note</p> <p><code>rollout-duration</code> is time-based, and does not interact with the autoscaling subsystem.</p> <p>This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.</p>"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#procedure","title":"Procedure","text":"<p>You can configure the <code>rollout-duration</code> parameter by modifying the <code>config-network</code> ConfigMap, or by using the Operator.</p> ConfigMap configurationOperator configuration <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n name: config-network\n namespace: knative-serving\ndata:\n  rollout-duration: \"380s\"  # Value in seconds.\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    network:\n       rollout-duration: \"380s\"\n</code></pre>"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#route-status-updates","title":"Route status updates","text":"<p>During a rollout, the system updates the Route and Knative Service status conditions. Both the <code>traffic</code> and <code>conditions</code> status parameters are affected.</p> <p>For example, for the following traffic configuration:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 55\n    configurationName: config # Pinned to latest ready Revision\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>Initially 1% of the traffic is rolled out to the Revisions:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 54\n    revisionName: config-00008\n  - percent: 1\n    revisionName: config-00009\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>Then the rest of the traffic is rolled out in increments of 18%:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 36\n    revisionName: config-00008\n  - percent: 19\n    revisionName: config-00009\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>The rollout continues until the target traffic configuration is reached:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\n  traffic:\n  - percent: 55\n    revisionName: config-00009\n  - percent: 45\n    revisionName: config-00005 # Pinned to a specific Revision.\n</code></pre> <p>During the rollout, the Route and Knative Service status conditions are as follows:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n...\nspec:\n...\nstatus:\n  conditions:\n  ...\n  - lastTransitionTime: \"...\"\n    message: A gradual rollout of the latest revision(s) is in progress.\n    reason: RolloutInProgress\n    status: Unknown\n    type: Ready\n</code></pre>"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#multiple-rollouts","title":"Multiple rollouts","text":"<p>If a new revision is created while a rollout is in progress, the system begins to shift traffic immediately to the newest Revision, and drains the incomplete rollouts from newest to oldest.</p>"},{"location":"serving/encryption/cluster-local-domain-tls/","title":"Configure cluster-local domain encryption","text":"<p>Warning</p> <p>The Knative Serving encryption features <code>cluster-local-domain-tls</code> and <code>system-internal-tls</code> are in experimental state. Please use with caution!</p>"},{"location":"serving/encryption/cluster-local-domain-tls/#before-you-begin","title":"Before you begin","text":"<p>You must meet the following requirements to enable secure HTTPS connections:</p> <ul> <li>Knative Serving must be installed. For details about installing the Serving   component, see the Knative installation guides.</li> </ul> <p>Warning</p> <p>This feature is currently only supported with Kourier and Istio as a networking layer.</p>"},{"location":"serving/encryption/cluster-local-domain-tls/#installing-and-configuring-cert-manager-and-integration","title":"Installing and configuring cert-manager and integration","text":"<p>First, you need to install and configure <code>cert-manager</code> and the Knative cert-manager integration. Please refer to Configuring Knative cert-manager integration for details.</p>"},{"location":"serving/encryption/cluster-local-domain-tls/#enabling-cluster-local-domain-tls","title":"Enabling cluster-local-domain-tls","text":"<p>To enable <code>cluster-local-domain-tls</code> update the <code>config-network</code> ConfigMap in the <code>knative-serving</code> namespace:</p> <ol> <li> <p>Run the following command to edit your <code>config-network</code> ConfigMap:</p> <pre><code>kubectl edit configmap config-network -n knative-serving\n</code></pre> </li> <li> <p>Add the <code>cluster-local-domain-tls: Enabled</code> attribute under the <code>data</code> section:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-network\n  namespace: knative-serving\ndata:\n   ...\n   cluster-local-domain-tls: Enabled\n   ...\n</code></pre> </li> <li> <p>Restart the Knative Serving controller to start the Knative cert-manager integration:</p> <pre><code>kubectl rollout restart deploy/controller -n knative-serving\n</code></pre> </li> </ol> <p>Congratulations! Knative is now configured to obtain and renew TLS certificates for cluster-local domains.</p>"},{"location":"serving/encryption/cluster-local-domain-tls/#verification","title":"Verification","text":"<ol> <li> <p>Deploy a Knative Service</p> </li> <li> <p>Check the URL with <code>kubectl get ksvc -n &lt;your-namespace&gt; -o yaml</code></p> </li> <li> <p>The service URL cluster-local domain (https://helloworld.test.svc.cluster.local) should now be https:</p> </li> </ol> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld\n  namespace: test\nspec:\n  # ...\nstatus:\n  address:\n    # cluster-local-domain:\n    url: https://helloworld.test.svc.cluster.local\n  # ...\n  # external domain:\n  url: http://helloworld.first.example.com\n</code></pre>"},{"location":"serving/encryption/cluster-local-domain-tls/#trust","title":"Trust","text":"<p>Note</p> <p>A quick note on trust, all clients that call the cluster-local domain of a Knative Service need to trust the Certificate Authority that signed the certificates. This is out of scope of Knative, but needs to be addressed to ensure a working system. Especially when a Certificate Authority performs a rotation of the CA or the intermediate certificates. Find more information on Configuring Knative cert-manager integration.</p>"},{"location":"serving/encryption/configure-certmanager-integration/","title":"Configuring Knative cert-manager integration","text":"<p>Knative Serving relies on a bridging component to use cert-manager for automated certificate provisioning.  If you intend to use that feature, you need to enable the Knative cert-manager integration.</p>"},{"location":"serving/encryption/configure-certmanager-integration/#prerequisites","title":"Prerequisites","text":"<p>The following must be installed on your Knative cluster:</p> <ul> <li>Knative Serving.</li> <li><code>cert-manager</code> version <code>1.0.0</code> or higher.</li> </ul> <p>Warning</p> <p>Make sure you have installed cert-manager. Otherwise, the Serving controller will not start up correctly.</p>"},{"location":"serving/encryption/configure-certmanager-integration/#issuer-configuration","title":"Issuer configuration","text":"<p>The Knative cert-manager integration defines three references to cert-manager issuers to configure different CAs for the three Knative Serving encryption features:</p> <ul> <li><code>issuerRef</code>: issuer for external-domain certificates used for ingress.</li> <li><code>clusterLocalIssuerRef</code>: issuer for cluster-local-domain certificates used for ingress.</li> <li><code>systemInternalIssuerRef</code>: issuer for certificates for system-internal-tls certificates used by Knative internal components.</li> </ul> <p>The following example uses a self-signed <code>ClusterIssuer</code> and the Knative cert-manager integration references that <code>ClusterIssuer</code> for all three configurations. As this should not be used in production (and does not support rotating the CA without downtime), you should think about which CA should be used for each use case and how trust will be distributed to the clients calling the encrypted services.  For the Knative system components, Knative provides a way to specify a bundle of CAs that should be trusted (more on this below).</p> <p>There is no general answer on how to structure this, here an example on how it could look like:</p> Feature Certificate Authority Trusted via external-domain-tls Let's encrypt Browser clients have the Let's encrypt chain already, all the root CAs will be added in company-wide Docker base image by DevOps team. cluster-local-domain-tls CA provided by cluster operator The CA is managed by the DevOps team and will be added in company-wide Docker base image. system-internal-tls Self-signed Cert-Manager <code>ClusterIssuer</code> The CA will be populated by cert-manager. DevOps team will use trust-manager to distribute the CA to Knative system components."},{"location":"serving/encryption/configure-certmanager-integration/#issuer-selection","title":"Issuer selection","text":"<p>In general, you can refer to the cert-manager documentation. There are examples available for:</p> <ul> <li>CA based on a K8s secret</li> <li>HTTP-01 challenges, e.g. Let's encrypt</li> <li>DNS-01 challenges</li> <li>Self-signed issuers</li> </ul> <p>Important</p> <p>Please note, that not all issuer types work for each Knative feature.</p> <p><code>cluster-local-domain-tls</code> needs to be able to sign certificates for cluster-local domains like <code>myapp.&lt;namespace&gt;</code>, <code>myapp.&lt;namespace&gt;.svc</code> and <code>myapp.&lt;namespace&gt;.svc.cluster.local</code>. The CA is usually outside the cluster, so verification via ACME protocol (DNS01/HTTP01) is impossible. You can use an issuer that allows the creation of these certificates (e.g., a CA issuer). </p> <p><code>system-internal-tls</code> needs to be able to sign specific SANs that Knative validates for. The defined set of SANs is:</p> <ul> <li><code>kn-routing</code></li> <li><code>kn-user-&lt;namespace&gt;</code> ( is each namespace where Knative Services are/will be created) <li><code>data-plane.knative.dev</code></li> <p>As this is also not possible via ACME protocol (DNS01/HTTP01), you need to configure an issuer that allows creating the these certificates (e.g. CA issuer).</p>"},{"location":"serving/encryption/configure-certmanager-integration/#configuring-issuers","title":"Configuring issuers","text":"<p>Warning</p> <p>The self-signed cluster issuer should not be used in production, please see Issuer configuration above for more information.</p> <ol> <li> <p>Create and apply the following self-signed <code>ClusterIssuer</code> to your cluster:</p> <pre><code># this issuer is used by cert-manager to sign all certificates\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: cluster-selfsigned-issuer\nspec:\n  selfSigned: {}\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer # this issuer is specifically for Knative, it will use the CA stored in the secret created by the Certificate below\nmetadata:\n  name: knative-selfsigned-issuer\nspec:\n  ca:\n    secretName: knative-selfsigned-ca\n---\napiVersion: cert-manager.io/v1\nkind: Certificate # this creates a CA certificate, signed by cluster-selfsigned-issuer and stored in the secret knative-selfsigned-ca\nmetadata:\n  name: knative-selfsigned-ca\n  namespace: cert-manager #  If you want to use it as a ClusterIssuer the secret must be in the cert-manager namespace.\nspec:\n  secretName: knative-selfsigned-ca\n  commonName: knative.dev\n  usages:\n    - server auth\n  isCA: true\n  issuerRef:\n    kind: ClusterIssuer\n    name: cluster-selfsigned-issuer\n</code></pre> </li> <li> <p>Ensure that the <code>ClusterIssuer</code> is ready:</p> <p><pre><code>kubectl get clusterissuer cluster-selfsigned-issuer -o yaml\nkubectl get clusterissuer knative-selfsigned-issuer -o yaml\n</code></pre> Result: The <code>Status.Conditions</code> should include <code>Ready=True</code>.</p> </li> <li> <p>Then reference the <code>ClusterIssuer</code> in the <code>config-certmanager</code> ConfigMap:</p> <pre><code>kubectl edit configmap config-certmanager -n knative-serving\n</code></pre> <p>Add the fields within the <code>data</code> section:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-certmanager\n  namespace: knative-serving\n  labels:\n    networking.knative.dev/certificate-provider: cert-manager\ndata:\n  issuerRef: |\n    kind: ClusterIssuer\n    name: knative-selfsigned-issuer\n  clusterLocalIssuerRef: |\n    kind: ClusterIssuer\n    name: knative-selfsigned-issuer\n  systemInternalIssuerRef: |\n    kind: ClusterIssuer\n    name: knative-selfsigned-issuer\n</code></pre> <p>Ensure that the file was updated successfully:</p> <pre><code>kubectl get configmap config-certmanager -n knative-serving -o yaml\n</code></pre> </li> </ol>"},{"location":"serving/encryption/configure-certmanager-integration/#managing-trust-and-rotation-without-downtime","title":"Managing trust and rotation without downtime","text":"<p>As pointed out above, each client that calls a Knative Service using HTTPS needs to trust the CA and/or intermediate chain. If you take a look at the encryption overview, you can see that there are multiple places where  trust needs to be distributed:</p> <ul> <li>Cluster external client (Browser and/or other application): this is considered out of scope of Knative.</li> <li>Cluster internal client (e.g. Knative or Vanilla K8s workload): see below.</li> <li>Knative system components (e.g. Activator, Queue-Proxy, Ingress-Controller): see below.</li> </ul>"},{"location":"serving/encryption/configure-certmanager-integration/#trusting-for-cluster-internal-clients-eg-knative-or-vanilla-k8s-workload","title":"Trusting for cluster internal clients (e.g. Knative or Vanilla K8s workload)","text":"<p>As Knative does not control all workload and the settings are highly dependent on your runtime and/or language, this is out of scope of Knative. But here a few points to consider, as there are several ways on how to provide CAs to your application:</p> <ul> <li>Adding the CA bundle to a Container image on build-time (be aware that this complicates CA rotation, you basically need to rebuild every application)</li> <li>Mounting a CA bundle to the filesystem (e.g. from a <code>Secret</code> or <code>ConfigMap</code>)</li> <li>Reading it from environment variable</li> <li>Accessing it from a <code>Secret</code>/<code>ConfigMap</code> via K8s API</li> </ul> <p>If reloading certificates without downtime is important for your client, the workload must either watch changes on the K8s resource (Secret/ConfigMap) or watch the filesystem. If the workload is watching the filesystem, it is important to note that using <code>ionotify</code> to catch changing Secrets/ConfigMaps is not very reliable on K8s. Tests have shown that it is more reliable to regularly poll and check the certificate on the filesystem for changes.</p> <p>Here are a few examples for golang:</p> <ul> <li>Saving the bundle as a file to a defined path: https://go.dev/src/crypto/x509/root_linux.go (note does not reload without restart)</li> <li>Reloading dynamically via K8s API: https://github.com/knative/serving/blob/main/pkg/activator/certificate/cache.go</li> <li>Reloading from filesystem with a watcher process: https://github.com/knative/serving/blob/main/pkg/queue/certificate/watcher.go</li> </ul>"},{"location":"serving/encryption/configure-certmanager-integration/#trusting-for-knative-system-components","title":"Trusting for Knative system components","text":"<p>Knative system components can be configured to trust one or many CA bundles from <code>ConfigMaps</code>. The cluster operator needs to ensure, to configure them accordingly to avoid any downtimes during a rotation. Knative components look for a <code>ConfigMap</code> in the namespace where the component runs, e.g:</p> <ul> <li>knative-serving</li> <li>istio-system (when using net-istio)</li> <li>kourier-system (when using net-kourier)</li> <li>Each namespace where a Knative Service runs</li> </ul> <p>Knative looks for a <code>ConfigMap</code> with the label <code>networking.knative.dev/trust-bundle: \"true\"</code> and will read all <code>data</code> keys (regardless of the name).  One key can contain one or multiple CAs/Intermediates. If they are valid, they will be added to the trust store of the Knative components.</p> <p>Here is an example of how <code>ConfigMap</code> could look like:</p> <pre><code>apiVersion: v1\ndata:\n  cacerts.pem: |\n    -----BEGIN CERTIFICATE-----\n    MIIDDTCCAfWgAwIBAgIQMQuip05h7NLQq2TB+j9ZmTANBgkqhkiG9w0BAQsFADAW\n    MRQwEgYDVQQDEwtrbmF0aXZlLmRldjAeFw0yMzExMjIwOTAwNDhaFw0yNDAyMjAw\n    OTAwNDhaMBYxFDASBgNVBAMTC2tuYXRpdmUuZGV2MIIBIjANBgkqhkiG9w0BAQEF\n    AAOCAQ8AMIIBCgKCAQEA3clC3CV7sy0TpUKNuTku6QmP9z8JUCbLCPCLACCUc1zG\n    FEokqOva6TakgvAntXLkB3TEsbdCJlNm6qFbbko6DBfX6rEggqZs40x3/T+KH66u\n    4PvMT3fzEtaMJDK/KQOBIvVHrKmPkvccUYK/qWY7rgBjVjjLVSJrCn4dKaEZ2JNr\n    Fd0KNnaaW/dP9/FvviLqVJvHnTMHH5qyRRr1kUGTrc8njRKwpHcnUdauiDoWRKxo\n    Zlyy+MhQfdbbyapX984WsDjCvrDXzkdGgbRNAf+erl6yUm6pHpQhyFFo/zndx6Uq\n    QXA7jYvM2M3qCnXmaFowidoLDsDyhwoxD7WT8zur/QIDAQABo1cwVTAOBgNVHQ8B\n    Af8EBAMCAgQwEwYDVR0lBAwwCgYIKwYBBQUHAwEwDwYDVR0TAQH/BAUwAwEB/zAd\n    BgNVHQ4EFgQU7p4VuECNOcnrP9ulOjc4J37Q2VUwDQYJKoZIhvcNAQELBQADggEB\n    AAv26Vnk+ptQrppouF7yHV8fZbfnehpm07HIZkmnXO2vAP+MZJDNrHjy8JAVzXjt\n    +OlzqAL0cRQLsUptB0btoJuw23eq8RXgJo05OLOPQ2iGNbAATQh2kLwBWd/CMg+V\n    KJ4EIEpF4dmwOohsNR6xa/JoArIYH0D7gh2CwjrdGZr/tq1eMSL+uZcuX5OiE44A\n    2oXF9/jsqerOcH7QUMejSnB8N7X0LmUvH4jAesQgr7jo1JTOBs7GF6wb+U76NzFa\n    8ms2iAWhoplQ+EHR52wffWb0k6trXspq4O6v/J+nq9Ky3vC36so+G1ZFkMhCdTVJ\n    ZmrBsSMWeT2l07qeei2UFRU=\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  labels:\n    networking.knative.dev/trust-bundle: \"true\"\n  name: knative-bundle\n  namespace: knative-serving\n</code></pre>"},{"location":"serving/encryption/configure-certmanager-integration/#using-trust-manager-to-distribute-the-bundle","title":"Using trust-manager to distribute the bundle","text":"<p>As it can be a cumbersome task to distribute the CA bundle to all the namespaces, you can use trust-manager  to automatically distribute the CA bundles. Please refer to their documentation for more information on how to do this.</p>"},{"location":"serving/encryption/configure-certmanager-integration/#trust-during-rotation","title":"Trust during rotation","text":"<p>During a rotation of a CA and/or Intermediate certificates your clients will need to trust the old and the new CA/chain until the rotation is done. Using the trust approach from above, you can do a full chain rotation without downtime:</p> <ol> <li>Make sure your existing setup is up and running.</li> <li>Make sure all Knative Services have the relevant certificates and are not expired.</li> <li>Make sure your CA (and full chain) is not expired.</li> <li>Add the existing and the new CA (and the full chain) to the trust bundle (either manually or via trust-manager).</li> <li>Reconfigure your cert-manager <code>ClusterIssuers</code> or <code>Issuers</code> to use the new CA.</li> <li>Wait until all certificates are expired and are renewed by cert-manager.</li> <li>All certificates are now signed by the new CA.</li> <li>Add some grace period to make sure all components did pick up all the changes.</li> <li>Remove the old CA from the trust bundle.</li> </ol>"},{"location":"serving/encryption/encryption-overview/","title":"Serving Encryption Overview","text":"<p>Warning</p> <p>The Knative Serving encryption features <code>cluster-local-domain-tls</code> and <code>system-internal-tls</code> are in experimental state. Please use with caution!</p> <p>There are three parts to Knative Serving encryption:</p> <ol> <li>HTTPS on the ingress layer external to the cluster (cluster external domain, like <code>myapp-&lt;namespace&gt;.example.com</code>).</li> <li>HTTPS on the ingress layer internal to the cluster (cluster local domains, like <code>myapp.&lt;namespace&gt;.svc.cluster.local</code>).</li> <li>HTTPS between Knative internal components (<code>ingress-controller</code>, <code>activator</code>, <code>queue-proxy</code>).</li> </ol> <p></p> <p>Note</p> <p>Currently, all control-plane traffic (including Kubernetes PreStopHooks and metadata like metrics) is not encrypted.</p>"},{"location":"serving/encryption/encryption-overview/#the-parts-in-detail","title":"The parts in detail","text":"<p>The different parts are independent of each other and (can) use different Certificate Authorities to sign certificates.</p>"},{"location":"serving/encryption/encryption-overview/#external-domain-encryption","title":"External domain encryption","text":"<ul> <li>Certificate CN/SAN contains the external domain of a Knative Service, e.g. <code>myapp-&lt;namespace&gt;.example.com</code>.</li> <li>The certificates are hosted using SNI by the external endpoint of the ingress-controller.</li> <li>The caller has to trust the (external) CA that signed the certificates (this is out of the scope of Knative).</li> <li>These certificates are either provided manually or by enabling automatic certificate provisioning.</li> </ul> <p>See Configure external domain encryption for more information on this feature.</p>"},{"location":"serving/encryption/encryption-overview/#cluster-local-encryption","title":"Cluster-local encryption","text":"<ul> <li>Certificate CN/SAN contains the cluster-local domains of a Knative Service, e.g. <code>myapp.namespace.svc.cluster.local</code>, <code>myapp.namespace.svc</code>, <code>myapp.namespace</code>.</li> <li>The certificates are hosted using SNI by the cluster-local endpoint of the ingress-controller.</li> <li>The caller has to trust the CA that signed the certificates (this is out of the scope of Knative). One option to do this is using trust-manager from cert-manager.</li> <li>To create the certificates, Knative relies on cert-manager and the Knative cert-manager integration. They need to be installed and configured for the feature to work.</li> </ul> <p>See Configure cluster-local domain encryption for more information on this feature.</p>"},{"location":"serving/encryption/encryption-overview/#knative-system-internal-encryption","title":"Knative system-internal encryption","text":"<p>Knative system internal components (Ingress-Controller, Activator, Queue-Proxy) are hosting TLS endpoints when this configuration is enabled.</p> <ul> <li>To create the certificates, Knative relies on cert-manager and the Knative cert-manager integration. They need to be installed and configured for the feature to work.</li> <li>Specific SANs are used to verify each connection. Each component needs to trust the CA (possibly the full chain) that signed the certificates. For this, Knative system components will consume and trust a provided <code>CABundle</code>. The CA bundle needs to be provided by the cluster administrator, possibly using trust-manager from cert-manager.</li> </ul> <p>See Configure Knative system-internal encryption for more information on this feature.</p>"},{"location":"serving/encryption/external-domain-tls/","title":"Configure external domain encryption","text":"<p>Knative allows to use either use custom TLS certificates or to use automatically generated TLS certificates  to enable secure HTTPS connections for your Knative Services for the external domain (like <code>application.example.com</code>).</p>"},{"location":"serving/encryption/external-domain-tls/#before-you-begin","title":"Before you begin","text":"<p>You must meet the following requirements to enable secure HTTPS connections:</p> <ul> <li>Knative Serving must be installed. For details about installing the Serving   component, see the Knative installation guides.</li> <li>You must configure your Knative cluster to use a custom external domain.</li> <li>Your DNS provider must be setup and configured to your domain.</li> <li>A Networking layer such as Kourier, Istio with SDS v1.3 or higher, or Contour v1.1 or higher. See Install a networking layer.</li> </ul> <p>Warning</p> <p>Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve.</p>"},{"location":"serving/encryption/external-domain-tls/#automatically-obtain-and-renew-certificates","title":"Automatically obtain and renew certificates","text":""},{"location":"serving/encryption/external-domain-tls/#installing-and-configuring-cert-manager-and-integration","title":"Installing and configuring cert-manager and integration","text":"<p>Info</p> <p>If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress.  You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider.</p> <p>First, you need to install and configure <code>cert-manager</code> and the Knative cert-manager integration. Please refer to Configuring Knative cert-manager integration for details.</p>"},{"location":"serving/encryption/external-domain-tls/#configuring-knative-serving","title":"Configuring Knative Serving","text":"<p>Automatic certificate provisioning allows to request certificates in two ways:</p> <ul> <li>One certificate for each individual Knative Service</li> <li>One wildcard certificate per namespace</li> </ul> <p>Only one of them can be active at the same time!</p>"},{"location":"serving/encryption/external-domain-tls/#using-a-certificate-for-each-knative-service","title":"Using a certificate for each Knative Service","text":"<p>Update the <code>config-network</code> ConfigMap in the <code>knative-serving</code> namespace to enable <code>external-domain-tls</code>:</p> <ol> <li> <p>Run the following command to edit your <code>config-network</code> ConfigMap:</p> <pre><code>kubectl edit configmap config-network -n knative-serving\n</code></pre> </li> <li> <p>Add the <code>external-domain-tls: Enabled</code> attribute under the <code>data</code> section:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-network\n  namespace: knative-serving\ndata:\n   ...\n   external-domain-tls: Enabled\n   ...\n</code></pre> </li> <li> <p>Restart the Knative Serving controller to start the Knative cert-manager integration:</p> <pre><code>kubectl rollout restart deploy/controller -n knative-serving\n</code></pre> </li> </ol>"},{"location":"serving/encryption/external-domain-tls/#using-one-wildcard-certificate-per-namespace","title":"Using one wildcard certificate per namespace","text":"<p>Warning</p> <p>Provisioning a wildcard Certificate per namespace only works with DNS-01 challenge. This feature cannot be used with HTTP-01 challenge.</p> <p>The per-namespace configuration uses namespace labels to select which namespaces should have a  certificate applied. The selection is configured using the key <code>namespace-wildcard-cert-selector</code> in the <code>config-network</code> ConfigMap. For example, you can use the following configurations:</p> <ul> <li><code>namespace-wildcard-cert-selector</code>: <code>\"\"</code> = Use an empty value to disable the feature (this is the default).</li> <li><code>namespace-wildcard-cert-selector</code>: <code>{}</code> = Use an empty object to enable for all namespaces.</li> </ul> <p>You can also configure the selector to opt-out when a specific label is on the namespace:</p> <p><pre><code>namespace-wildcard-cert-selector: |-\n  matchExpressions:\n  - key: \"networking.knative.dev/disableWildcardCert\"\n    operator: \"NotIn\"\n    values: [\"true\"] \n</code></pre> This selects all namespaces where the label value is not in the set <code>\"true\"</code>.</p> <p>Or use existing kubernetes labels to select namespaces based on their name:</p> <pre><code>namespace-wildcard-cert-selector: |-\n  matchExpressions:\n    - key: \"kubernetes.io/metadata.name\"\n      operator: \"In\"\n      values: [\"my-namespace\", \"my-other-namespace\"] \n</code></pre> <p>To apply the configuration you can use the following command (optionally adapting the label-selector):</p> <pre><code>kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}'\n</code></pre> <p>For more details on namespace selectors, see the Kubernetes documentation.</p> <p>Restart the Knative Serving controller to start the Knative cert-manager integration:</p> <pre><code>kubectl rollout restart deploy/controller -n knative-serving\n</code></pre> <p>Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS Certificate is issued and available on your cluster, your Knative services will be able to handle HTTPS traffic on the external domain.</p>"},{"location":"serving/encryption/external-domain-tls/#manually-obtain-and-renew-certificates","title":"Manually obtain and renew certificates","text":"<p>There are various ways on how to obtain certificates manually. You can either use tools like  Certbot or cert-manager or provide the certificates manually from another source.  In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the procedures later in this topic for details about manually obtaining and configuring certificates.</p>"},{"location":"serving/encryption/external-domain-tls/#obtaining-a-certificate-using-a-tool","title":"Obtaining a certificate using a tool","text":"<p>Please refer to the according documentation of the tool:</p> <ul> <li>Certbot docs</li> <li>cert-manager docs</li> </ul> <p>Knative expects a wildcard certificate signed for the DNS domain of your cluster external domain, like</p> <p><code>*.yourdomain.com</code></p> <p>Once you have obtained the certificate and the private key, create a Kubernetes Secret  for the certificate and key to be used by Knative.</p> <p>Warning</p> <p>Certificates issued by Let's Encrypt are valid for only 90days. Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires.</p>"},{"location":"serving/encryption/external-domain-tls/#create-a-kubernetes-secret","title":"Create a Kubernetes Secret","text":"<p>Use the following steps in the relevant tab to add your certificate to your Knative cluster:</p> ContourIstio <p>To add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin.</p> <ol> <li> <p>Create a Kubernetes secret to hold your TLS certificate, <code>cert.pem</code>, and the    private key, <code>key.pem</code>, by running the command:</p> <pre><code>kubectl create -n contour-external secret tls default-cert \\\n  --key key.pem \\\n  --cert cert.pem\n</code></pre> <p>Note</p> <p>Take note of the namespace and secret name. You will need these in future steps.</p> </li> <li> <p>To use this certificate and private key in different namespaces, you must create a delegation. To do so, create a YAML file using the following template:</p> <pre><code>apiVersion: projectcontour.io/v1\nkind: TLSCertificateDelegation\nmetadata:\n  name: default-delegation\n  namespace: contour-external\nspec:\n  delegations:\n    - secretName: default-cert\n      targetNamespaces:\n      - \"*\"\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> <li> <p>Update the Knative Contour plugin to use the certificate as a fallback    when <code>external-domain-tls</code> is disabled by running the command:</p> <pre><code>kubectl patch configmap config-contour -n knative-serving \\\n  -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}'\n</code></pre> </li> </ol> <p>To add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the <code>knative-ingress-gateway</code>:</p> <ol> <li>Create a Kubernetes secret to hold your TLS certificate, <code>cert.pem</code>, and the    private key, <code>key.pem</code>, by entering the following command:</li> </ol> <pre><code>kubectl create --namespace istio-system secret tls tls-cert \\\n  --key key.pem \\\n  --cert cert.pem\n</code></pre> <ol> <li> <p>Configure Knative to use the new secret that you created for HTTPS    connections:</p> </li> <li> <p>Run the following command to open the Knative shared <code>gateway</code> in edit       mode:</p> <pre><code>kubectl edit gateway knative-ingress-gateway --namespace knative-serving\n</code></pre> </li> <li> <p>Update the <code>gateway</code> to include the following <code>tls:</code> section and       configuration:</p> <pre><code>tls:\n  mode: SIMPLE\n  credentialName: tls-cert\n</code></pre> <p>Example:</p> <p><pre><code># Edit the following object. Lines beginning with a '#' will be ignored.\n# An empty file will abort the edit. If an error occurs while saving this\n# file will be reopened with the relevant failures.\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  # ... skipped ...\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n    - hosts:\n        - \"*\"\n      port:\n        name: http\n        number: 80\n        protocol: HTTP\n    - hosts:\n        - TLS_HOSTS\n      port:\n        name: https\n        number: 443\n        protocol: HTTPS\n      tls:\n        mode: SIMPLE\n        credentialName: tls-cert\n</code></pre>   In this example, <code>TLS_HOSTS</code> represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host.   For detailed instructions, please refer Istio documentation</p> </li> </ol>"},{"location":"serving/encryption/external-domain-tls/#verification","title":"Verification","text":"<ol> <li> <p>Deploy a Knative Service</p> </li> <li> <p>Check the URL with <code>kubectl get ksvc -n &lt;your-namespace&gt;</code></p> </li> <li> <p>The service URL should now be https:</p> <pre><code>NAME           URL                                          LATEST               AGE     CONDITIONS   READY   REASON\nautoscale-go   https://autoscale-go.default.1.example.com   autoscale-go-dd42t   8m17s   3 OK / 3     True\n</code></pre> </li> </ol>"},{"location":"serving/encryption/external-domain-tls/#trust","title":"Trust","text":"<p>Note</p> <p>A quick note on trust, all clients that call the external domain of a Knative Service need to trust the Certificate Authority that signed the certificates. This is out of scope of Knative, but needs to be addressed to ensure a working system. Especially, when a Certificate Authority performs a rotation of the CA or the intermediate certificates. Find more information on Configuring Knative cert-manager integration.</p>"},{"location":"serving/encryption/external-domain-tls/#additional-configuration","title":"Additional configuration","text":""},{"location":"serving/encryption/external-domain-tls/#configuring-http-redirects","title":"Configuring HTTP redirects","text":"<p>Knative Serving allows to automatically redirect HTTP traffic, when HTTPS is enabled on external domains.  To configure this </p> <ol> <li> <p>Configure how HTTP and HTTPS requests are handled with the <code>http-protocol</code> attribute.</p> <p>By default, Knative ingress is configured to serve HTTP traffic (<code>http-protocol: Enabled</code>). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic on external domains, you can specify whether any HTTP traffic is allowed or not.</p> <p>Supported <code>http-protocol</code> values:</p> <ul> <li><code>Enabled</code>: Serve HTTP traffic.</li> <li><code>Redirected</code>: Responds to HTTP request with a <code>302</code> redirect to ask the clients to use HTTPS.</li> </ul> <pre><code>data:\n  http-protocol: Redirected\n</code></pre> <p>Example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-network\n  namespace: knative-serving\ndata:\n  ...\n  external-domain-tls: Enabled\n  http-protocol: Redirected\n  ...\n</code></pre> </li> </ol>"},{"location":"serving/encryption/external-domain-tls/#disable-automatic-tls-certificate-provisioning-per-service-or-route","title":"Disable automatic TLS certificate provisioning per Service or Route","text":"<p>If you have automatic TLS certificate provisioning enabled in your cluster, you can choose to disable the feature for individual Knative Services or Routes by adding the annotation <code>networking.knative.dev/disable-external-domain-tls: true</code>.</p> <p>Using the <code>autoscale-go</code> example:</p> <ol> <li> <p>Edit the service using <code>kubectl edit service.serving.knative.dev/autoscale-go -n default</code> and add the annotation:</p> <pre><code> apiVersion: serving.knative.dev/v1\n kind: Service\n metadata:\n   annotations:\n    ...\n     networking.knative.dev/disable-external-domain-tls: \"true\"\n    ...\n</code></pre> </li> <li> <p>The service URL should now be http, indicating that automatic TLS Certificate provisioning is disabled:</p> <pre><code>NAME           URL                                          LATEST               AGE     CONDITIONS   READY   REASON\nautoscale-go   http://autoscale-go.default.1.example.com    autoscale-go-dd42t   8m17s   3 OK / 3     True\n</code></pre> </li> </ol>"},{"location":"serving/encryption/system-internal-tls/","title":"Configure Knative system-internal encryption","text":"<p>Warning</p> <p>The Knative Serving encryption features <code>cluster-local-domain-tls</code> and <code>system-internal-tls</code> are in experimental state. Please use with caution!</p>"},{"location":"serving/encryption/system-internal-tls/#before-you-begin","title":"Before you begin","text":"<p>You must meet the following requirements to enable secure HTTPS connections:</p> <ul> <li>Knative Serving must be installed. For details about installing the Serving   component, see the Knative installation guides.</li> </ul> <p>Warning</p> <p>This feature is currently only supported with Kourier as a networking layer.</p>"},{"location":"serving/encryption/system-internal-tls/#installing-and-configuring-cert-manager-and-integration","title":"Installing and configuring cert-manager and integration","text":"<p>First, you need to install and configure <code>cert-manager</code> and the Knative cert-manager integration. Please refer to Configuring Knative cert-manager integration for details.</p>"},{"location":"serving/encryption/system-internal-tls/#enabling-system-internal-tls","title":"Enabling system-internal-tls","text":"<p>To enable <code>system-internal-tls</code> update the <code>config-network</code> ConfigMap in the <code>knative-serving</code> namespace:</p> <ol> <li> <p>Run the following command to edit your <code>config-network</code> ConfigMap:</p> <pre><code>kubectl edit configmap config-network -n knative-serving\n</code></pre> </li> <li> <p>Add the <code>system-internal-tls: Enabled</code> attribute under the <code>data</code> section:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-network\n  namespace: knative-serving\ndata:\n   ...\n   system-internal-tls: Enabled\n   ...\n</code></pre> </li> <li> <p>Restart the Knative activator and controller component to start the Knative cert-manager integration:</p> <pre><code>kubectl rollout restart deploy/activator -n knative-serving\nkubectl rollout restart deploy/controller -n knative-serving\n</code></pre> </li> </ol> <p>Congratulations! Knative will now use TLS between its internal system components (Ingress-Controller, Activator and Queue-Proxy).</p>"},{"location":"serving/encryption/system-internal-tls/#verification","title":"Verification","text":"<ol> <li> <p>Deploy a Knative Service</p> </li> <li> <p>Check if certificates are created and ready with <code>kubectl get kcert -n &lt;your-knative-service-namespace&gt;</code></p> </li> <li> <p>Check if the Queue-Proxy container reads the certificate on startup with </p> <pre><code>kubectl logs your-pod -n your-knative-service-namespace -c queue-proxy | grep -E 'certDir|Certificate|tls'\n</code></pre> <p>It should look like this:</p> <pre><code>{\"severity\":\"INFO\",\"timestamp\":\"2024-01-03T07:07:32.892810888Z\",\"logger\":\"queueproxy\",\"caller\":\"certificate/watcher.go:62\",\"message\":\"Starting to watch the following directories for changes{certDir 15 0 /var/lib/knative/certs &lt;nil&gt;} {keyDir 15 0 /var/lib/knative/certs &lt;nil&gt;}\",\"commit\":\"86420f2-dirty\",\"knative.dev/key\":\"first/helloworld-00001\",\"knative.dev/pod\":\"helloworld-00001-deployment-75fbb7d488-qgmxx\"}\n{\"severity\":\"INFO\",\"timestamp\":\"2024-01-03T07:07:32.89397512Z\",\"logger\":\"queueproxy\",\"caller\":\"certificate/watcher.go:131\",\"message\":\"Certificate and/or key have changed on disk and were reloaded.\",\"commit\":\"86420f2-dirty\",\"knative.dev/key\":\"first/helloworld-00001\",\"knative.dev/pod\":\"helloworld-00001-deployment-75fbb7d488-qgmxx\"}\n{\"severity\":\"INFO\",\"timestamp\":\"2024-01-03T07:07:32.894232939Z\",\"logger\":\"queueproxy\",\"caller\":\"sharedmain/main.go:282\",\"message\":\"Starting tls server admin:8022\",\"commit\":\"86420f2-dirty\",\"knative.dev/key\":\"first/helloworld-00001\",\"knative.dev/pod\":\"helloworld-00001-deployment-75fbb7d488-qgmxx\"}\n{\"severity\":\"INFO\",\"timestamp\":\"2024-01-03T07:07:32.894268548Z\",\"logger\":\"queueproxy\",\"caller\":\"sharedmain/main.go:282\",\"message\":\"Starting tls server main:8112\",\"commit\":\"86420f2-dirty\",\"knative.dev/key\":\"first/helloworld-00001\",\"knative.dev/pod\":\"helloworld-00001-deployment-75fbb7d488-qgmxx\"}\n</code></pre> </li> </ol>"},{"location":"serving/encryption/system-internal-tls/#trust","title":"Trust","text":"<p>Warning</p> <p>A quick note on trust, Knative will automatically trust the CA that signed the Certificates, if the cert-manager issuer allows  putting the CA directly in the field <code>ca.crt</code> of the certificates <code>Secret</code>. Regardless of that, Cluster admins should always provide a trust-bundle, as described in  Configuring Knative cert-manager integration. This is also strongly recommended in the cert-manager documentation to avoid issues with rotation.</p>"},{"location":"serving/load-balancing/","title":"Load balancing","text":"<p>You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled.</p>"},{"location":"serving/load-balancing/#activator-pod-selection","title":"Activator pod selection","text":"<p>Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally.</p> <p>Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision.</p> <p>The Activator load balancing algorithm works as follows:</p> <ul> <li>If concurrency is unlimited, the request is sent to the better of two random choices.</li> <li>If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency.</li> </ul> <p>For more information, see the documentation on concurrency.</p>"},{"location":"serving/load-balancing/#configuring-target-burst-capacity","title":"Configuring target burst capacity","text":"<p>Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios.</p> <p>Target burst capacity can be configured using a combination of the following parameters:</p> <ul> <li>Setting the targeted concurrency limits for the revision. See concurrency.</li> <li>Setting the target utilization parameters. See target utilization.</li> <li>Setting the target burst capacity. You can configure target burst capacity using the <code>target-burst-capacity</code> key in the <code>config-autoscaler</code> ConfigMap. See Setting the target burst capacity.</li> <li>Setting the Activator capacity by using the <code>config-autoscaler</code> ConfigMap. See Setting the Activator capacity.</li> </ul>"},{"location":"serving/load-balancing/activator-capacity/","title":"Configuring Activator capacity","text":"<p>If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero.</p> <p>Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula <code>(replicas * target + target-burst-capacity)/activator-capacity</code>. This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity.</p>"},{"location":"serving/load-balancing/activator-capacity/#setting-the-activator-capacity","title":"Setting the Activator capacity","text":"<ul> <li>Global key: <code>activator-capacity</code></li> <li>Possible values: int (at least 1)</li> <li>Default: <code>100</code></li> </ul> <p>Example:</p> Global (ConfigMap)Global (Operator) <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-autoscaler\n  namespace: knative-serving\ndata:\n  activator-capacity: \"200\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      activator-capacity: \"200\"\n</code></pre>"},{"location":"serving/load-balancing/target-burst-capacity/","title":"Configuring target burst capacity","text":"<p>Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing.</p> <p>The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods.</p> <p>Target burst capacity can be configured using a combination of the following parameters:</p> <ul> <li>Setting the targeted concurrency limits for the revision. See concurrency.</li> <li>Setting the target utilization parameters. See target utilization.</li> <li>Setting the target burst capacity. You can configure target burst capacity using the <code>target-burst-capacity</code> annotation key in the <code>config-autoscaler</code> ConfigMap. See Setting the target burst capacity.</li> </ul>"},{"location":"serving/load-balancing/target-burst-capacity/#setting-the-target-burst-capacity","title":"Setting the target burst capacity","text":"<ul> <li>Global key: <code>target-burst-capacity</code></li> <li>Per-revision annotation key: <code>autoscaling.knative.dev/target-burst-capacity</code></li> <li>Possible values: float (<code>0</code> means the Activator is only in path when scaled to 0, <code>-1</code> means the Activator is always in path)</li> <li>Default: <code>200</code></li> </ul> <p>Example:</p> Per RevisionGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  annotations:\n  name: &lt;service_name&gt;\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/target-burst-capacity: \"200\"\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-autoscaler\n  namespace: knative-serving\ndata:\n  target-burst-capacity: \"200\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    autoscaler:\n      target-burst-capacity: \"200\"\n</code></pre> <p>Note</p> <p>Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation.</p> <ul> <li> <p>If <code>autoscaling.knative.dev/target-burst-capacity</code> is set to <code>0</code>, the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied.</p> </li> <li> <p>If <code>autoscaling.knative.dev/target-burst-capacity</code> is set to <code>-1</code>, the Activator is always in the request path, regardless of the revision size.</p> </li> <li> <p>If <code>autoscaling.knative.dev/target-burst-capacity</code> is set to another integer, the Activator may be in the path, depending on the revision scale and load.</p> </li> </ul>"},{"location":"serving/observability/logging/collecting-logs/","title":"Logging","text":"<p>You can use Fluent Bit, a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving, which automatically deletes pods and associated logs when they are no longer needed.</p> <p>Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders.</p>"},{"location":"serving/observability/logging/collecting-logs/#setting-up-logging-components","title":"Setting up logging components","text":"<p>Setting up log collection requires two steps:</p> <ol> <li>Running a log forwarding DaemonSet on each node.</li> <li>Running a collector somewhere in the cluster.</li> </ol> <p>Tip</p> <p>In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath.</p>"},{"location":"serving/observability/logging/collecting-logs/#setting-up-the-collector","title":"Setting up the collector","text":"<p>The <code>fluent-bit-collector.yaml</code> file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called <code>logging</code>.</p> <p>Important</p> <p>Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready.</p> <p></p>"},{"location":"serving/observability/logging/collecting-logs/#procedure","title":"Procedure","text":"<ol> <li> <p>Apply the configuration by entering the command:</p> <p><pre><code>kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml\n</code></pre> The default configuration will classify logs into:</p> <ul> <li>Knative services, or pods with an <code>app=Knative</code> label.</li> <li>Non-Knative apps.</li> </ul> <p>Note</p> <p>Logs default to logging with the pod name; this can be changed by updating the <code>log-collector-config</code> ConfigMap before or after installation.</p> <p>Warning</p> <p>After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it.</p> </li> <li> <p>To access the logs through your web browser, enter the command:</p> <pre><code>kubectl port-forward --namespace logging service/log-collector 8080:80\n</code></pre> </li> <li> <p>Navigate to <code>http://localhost:8080/</code>.</p> </li> <li> <p>Optional: You can open a shell in the <code>nginx</code> pod and search the logs using Unix tools, by entering the command:</p> <pre><code>kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0\n</code></pre> </li> </ol>"},{"location":"serving/observability/logging/collecting-logs/#setting-up-the-forwarders","title":"Setting up the forwarders","text":"<p>See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default.</p> <p>When you create a ConfigMap during the installation steps, you must:</p> <ul> <li>Replace the ElasticSearch configuration with the <code>fluent-bit-configmap.yaml</code>, or</li> <li> <p>Add the following block to the ConfigMap, and update the <code>@INCLUDE output-elasticsearch.conf</code> to be <code>@INCLUDE output-forward.conf</code>:</p> <pre><code>output-forward.conf: |\n  [OUTPUT]\n      Name            forward\n      Host            log-collector.logging\n      Port            24224\n      Require_ack_response  True\n</code></pre> </li> </ul>"},{"location":"serving/observability/logging/collecting-logs/#setting-up-a-local-collector","title":"Setting up a local collector","text":"<p>Warning</p> <p>This procedure describes a development environment setup and is not suitable for production use.</p> <p>If you are using a local Kubernetes cluster for development, you can create a <code>hostPath</code> PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools.</p> <p>The <code>PersistentVolumeClaim</code> will look similar to the following:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: shared-logs\n  labels:\n    app: logs-collector\nspec:\n  accessModes:\n    - \"ReadWriteOnce\"\n  storageClassName: manual\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: logs-log-collector-0\n    namespace: logging\n  capacity:\n    storage: 5Gi\n  hostPath:\n    path: &lt;see below&gt;\n</code></pre> <p>Note</p> <p>The <code>hostPath</code> will vary based on your Kubernetes software and host operating system.</p> <p>You must update the StatefulSet <code>volumeClaimTemplates</code> to reference the <code>shared-logs</code> volume, as shown in the following example:</p> <pre><code>volumeClaimTemplates:\n  metadata:\n    name: logs\n  spec:\n    accessModes: [\"ReadWriteOnce\"]\n    volumeName: shared-logs\n</code></pre>"},{"location":"serving/observability/logging/collecting-logs/#kind","title":"Kind","text":"<p>When creating your cluster, you must use a <code>kind-config.yaml</code> and specify <code>extraMounts</code> for each node, as shown in the following example:</p> <pre><code>apiversion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n  - role: control-plane\n    extraMounts:\n      - hostPath: ./logs\n        containerPath: /shared/logs\n  - role: worker\n    extraMounts:\n      - hostPath: ./logs\n        containerPath: /shared/logs\n</code></pre> <p>You can then use <code>/shared/logs</code> as the <code>spec.hostPath.path</code> in your PersistentVolume. Note that the directory path <code>./logs</code> is relative to the directory that the Kind cluster was created in.</p>"},{"location":"serving/observability/logging/collecting-logs/#docker-desktop","title":"Docker Desktop","text":"<p>Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems:</p> Host OS <code>hostPath</code> Mac OS <code>/Users/${USER}</code> Windows <code>/run/desktop/mnt/host/c/Users/${USER}/</code> Linux <code>/home/${USER}</code>"},{"location":"serving/observability/logging/collecting-logs/#minikube","title":"Minikube","text":"<p>Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes.</p> <p>The following command mounts the <code>logs</code> directory inside the current directory onto <code>/mnt/logs</code> in the VM:</p> <pre><code>minikube mount ./logs:/mnt/logs\n</code></pre> <p>You must also reference <code>/mnt/logs</code> as the <code>hostPath.path</code> in the PersistentVolume.</p>"},{"location":"serving/observability/logging/config-logging/","title":"Configuring Log Settings","text":"<p>Log configuration for all Knative components is managed through the <code>config-logging</code> ConfigMap in the corresponding namespace. For example, Serving components are configured through <code>config-logging</code> in the <code>knative-serving</code> namespace and Eventing components are configured through <code>config-logging</code> in the <code>knative-eventing</code> namespace, etc.</p> <p>Knative components use the zap logging library; options are documented in more detail in that project.</p> <p>In addition to <code>zap-logger-config</code>, which is a general key that applies to all components in that namespace, the <code>config-logging</code> ConfigMap supports overriding the log level for individual components.</p> ConfigMap key Description <code>zap-logger-config</code> A JSON object container for a zap logger configuration. Key fields are highlighted below. <code>zap-logger-config.level</code> The default logging level for components. Messages at or above this severity level will be logged. <code>zap-logger-config.encoding</code> The log encoding format for component logs (defaults to JSON). <code>zap-logger-config.encoderConfig</code> A <code>zap</code> EncoderConfig used to customize record contents. <code>loglevel.&lt;component&gt;</code> Overrides logging level for the given component only. Messages at or above this severity level will be logged. <p>Log levels supported by Zap are:</p> <ul> <li><code>debug</code> - fine-grained debugging</li> <li><code>info</code> - normal logging</li> <li><code>warn</code> - unexpected but non-critical errors</li> <li><code>error</code> - critical errors; unexpected during normal operation</li> <li><code>dpanic</code> - in debug mode, trigger a panic (crash)</li> <li><code>panic</code> - trigger a panic (crash)</li> <li><code>fatal</code> - immediately exit with exit status 1 (failure)</li> </ul>"},{"location":"serving/observability/logging/request-logging/","title":"Configuring Request Log Settings","text":"<p>The request logging for knative serving is managed through the <code>config-observability</code> ConfigMap in <code>knative-serving</code> namespace. The request logs will be printed by the queue-proxy sidecar.</p> <p>Mentioned below are the flags used to configure request logging features.</p> ConfigMap key Description <code>logging.enable-request-log</code> If true, the request logging will be enabled. <code>logging.enable-probe-request-log</code> If true, this enables queue proxy writing request logs for probe requests to stdout. It uses the same template as user requests, i.e. <code>logging.request-log-template</code>. <code>logging.request-log-template</code> The value determines the shape of the request logs and it must be a valid go text/template. It is important to keep this as a single line. Multiple lines are parsed as separate entities by most collection agents and will split the request logs into multiple records."},{"location":"serving/observability/metrics/collecting-metrics/","title":"Collecting Metrics in Knative","text":"<p>Knative supports different popular tools for collecting metrics:</p> <ul> <li>Prometheus</li> <li>OpenTelemetry Collector</li> </ul> <p>Grafana dashboards are available for metrics collected directly with Prometheus.</p> <p>You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry.</p> <p>Warning</p> <p>You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove <code>metrics.backend-destination</code> and <code>metrics.request-metrics-backend-destination</code> keys from the config-observability Configmap to enable Prometheus metrics.</p>"},{"location":"serving/observability/metrics/collecting-metrics/#about-the-prometheus-stack","title":"About the Prometheus Stack","text":"<p>Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used.</p> <p>Grafana is an open-source platform for data analytics and visualization, enabling users to create customizable dashboards for monitoring and analyzing metrics from various data sources.</p> <p>Prometheus Stack is a preconfigured collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules, combined to provide end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. The stack includes by default some Prometheus packages and Grafana.</p>"},{"location":"serving/observability/metrics/collecting-metrics/#setting-up-the-prometheus-stack","title":"Setting up the Prometheus Stack","text":"<ol> <li> <p>Install the Prometheus Stack by using Helm:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml\n# values.yaml contains at minimum the configuration below\n</code></pre> <p>Caution</p> <p>You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. <pre><code>kube-state-metrics:\n  metricLabelsAllowlist:\n    - pods=[*]\n    - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance]\nprometheus:\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n    podMonitorSelectorNilUsesHelmValues: false\n</code></pre></p> </li> <li> <p>Apply the ServiceMonitors/PodMonitors to collect metrics from Knative.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/servicemonitor.yaml\n</code></pre> </li> </ol>"},{"location":"serving/observability/metrics/collecting-metrics/#access-the-prometheus-instance-locally","title":"Access the Prometheus instance locally","text":"<p>By default, the Prometheus instance is only exposed on a private service named <code>prometheus-kube-prometheus-prometheus</code>.</p> <p>To access the console in your web browser:</p> <ol> <li> <p>Enter the command:</p> <pre><code>kubectl port-forward -n default svc/prometheus-kube-prometheus-prometheus 9090:9090\n</code></pre> </li> <li> <p>Access the console in your browser via <code>http://localhost:9090</code>.</p> </li> </ol>"},{"location":"serving/observability/metrics/collecting-metrics/#access-the-grafana-instance-locally","title":"Access the Grafana instance locally","text":"<p>By default, the Grafana instance is only exposed on a private service named <code>prometheus-grafana</code>.</p> <p>To access the dashboards in your web browser:</p> <ol> <li> <p>Enter the command:</p> <pre><code>kubectl port-forward -n default svc/prometheus-grafana 3000:80\n</code></pre> </li> <li> <p>Access the dashboards in your browser via <code>http://localhost:3000</code>.</p> </li> <li> <p>Use the default credentials to login:</p> <pre><code>username: admin\npassword: prom-operator\n</code></pre> </li> </ol>"},{"location":"serving/observability/metrics/collecting-metrics/#import-grafana-dashboards","title":"Import Grafana dashboards","text":"<ol> <li> <p>Grafana dashboards can be imported from the <code>monitoring</code> repository.</p> </li> <li> <p>If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative-extensions/monitoring/main/grafana/dashboards.yaml\n</code></pre> <p>Caution</p> <p>You will need to ensure that the helm chart has following values configured, otherwise the dashboards loading will not work. <pre><code>grafana:\n  sidecar:\n    dashboards:\n      enabled: true\n      searchNamespace: ALL\n</code></pre> If you have an existing configmap and the dashboards loading doesn't work, add the <code>labelValue: true</code> attribute to the helm chart after the <code>searchNamespace: ALL</code> declaration.</p> </li> </ol>"},{"location":"serving/observability/metrics/collecting-metrics/#about-opentelemetry","title":"About OpenTelemetry","text":"<p>OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs.</p> <p>You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components.</p> <p>OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.</p>"},{"location":"serving/observability/metrics/collecting-metrics/#understanding-the-collector","title":"Understanding the collector","text":"<p>The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service.</p> <p>In the following example, you can configure a single collector instance using a ConfigMap and a Deployment.</p> <p>Tip</p> <p>For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator.</p> <p>Caution</p> <p>The Grafana dashboards at https://github.com/knative-extensions/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector.</p> <p></p>"},{"location":"serving/observability/metrics/collecting-metrics/#set-up-the-collector","title":"Set up the collector","text":"<ol> <li> <p>Create a namespace for the collector to run in, by entering the following command:</p> <p><pre><code>kubectl create namespace metrics\n</code></pre> The next step uses the <code>metrics</code> namespace for creating the collector.</p> </li> <li> <p>Create a Deployment, Service, and ConfigMap for the collector by entering the following command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml\n</code></pre> </li> <li> <p>Update the <code>config-observability</code> ConfigMaps in the Knative Serving and    Eventing namespaces, by entering the follow command:</p> <pre><code>kubectl patch --namespace knative-serving configmap/config-observability \\\n  --type merge \\\n  --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'\nkubectl patch --namespace knative-eventing configmap/config-observability \\\n  --type merge \\\n  --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'\n</code></pre> </li> </ol>"},{"location":"serving/observability/metrics/collecting-metrics/#verify-the-collector-setup","title":"Verify the collector setup","text":"<ol> <li> <p>You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command:</p> <pre><code>kubectl port-forward --namespace metrics deployment/otel-collector 8889\n</code></pre> </li> <li> <p>Fetch <code>http://localhost:8889/metrics</code> to see the exported metrics.</p> </li> </ol>"},{"location":"serving/observability/metrics/serving-metrics/","title":"Knative Serving metrics","text":"<p>Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next.</p>"},{"location":"serving/observability/metrics/serving-metrics/#activator","title":"Activator","text":"<p>The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled.</p> Metric Name Description Type Tags Unit Status <code>request_concurrency</code> Concurrent requests that are routed to ActivatorThese are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge <code>configuration_name</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>request_count</code> The number of requests that are routed to Activator.These are requests that have been fulfilled from the activator handler. Counter <code>configuration_name</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>response_code</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>request_latencies</code> The response time in millisecond for the fulfilled routed requests Histogram <code>configuration_name</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>response_code</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Milliseconds Stable"},{"location":"serving/observability/metrics/serving-metrics/#autoscaler","title":"Autoscaler","text":"<p>Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA).</p> Metric Name Description Type Tags Unit Status <code>desired_pods</code> Number of pods autoscaler wants to allocate Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>excess_burst_capacity</code> Excess burst capacity overserved over the stable window Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>stable_request_concurrency</code> Average of requests count per observed pod over the stable window Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>panic_request_concurrency</code> Average of requests count per observed pod over the panic window Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>target_concurrency_per_pod</code> The desired number of concurrent requests for each pod Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>stable_requests_per_second</code> Average requests-per-second per observed pod over the stable window Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>panic_requests_per_second</code> Average requests-per-second per observed pod over the panic window Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>target_requests_per_second</code> The desired requests-per-second for each pod Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>panic_mode</code> 1 if autoscaler is in panic mode, 0 otherwise Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>requested_pods</code> Number of pods autoscaler requested from Kubernetes Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>actual_pods</code> Number of pods that are allocated currently in ready state Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>not_ready_pods</code> Number of pods that are not ready currently Gauge <code>configuration_name=</code><code>namespace_name=</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>pending_pods</code> Number of pods that are pending currently Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>terminating_pods</code> Number of pods that are terminating currently Gauge <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name&lt;br&gt;</code> Dimensionless Stable <code>scrape_time</code> Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram <code>configuration_name</code><code>namespace_name</code><code>revision_name</code><code>service_name</code> Milliseconds Stable"},{"location":"serving/observability/metrics/serving-metrics/#controller","title":"Controller","text":"<p>The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued.</p> Metric Name Description Type Tags Unit Status <code>work_queue_depth</code> Depth of the work queue Gauge <code>reconciler</code> Dimensionless Stable <code>reconcile_count</code> Number of reconcile operations Counter <code>reconciler</code><code>success</code> Dimensionless Stable <code>reconcile_latency</code> Latency of reconcile operations Histogram <code>reconciler</code><code>success</code> Milliseconds Stable <code>workqueue_adds_total</code> Total number of adds handled by workqueue Counter <code>name</code> Dimensionless Stable <code>workqueue_depth</code> Current depth of workqueue Gauge <code>reconciler</code> Dimensionless Stable <code>workqueue_queue_latency_seconds</code> How long in seconds an item stays in workqueue before being requested Histogram <code>name</code> Seconds Stable <code>workqueue_retries_total</code> Total number of retries handled by workqueue Counter <code>name</code> Dimensionless Stable <code>workqueue_work_duration_seconds</code> How long in seconds processing an item from a workqueue takes. Histogram <code>name</code> Seconds Stable <code>workqueue_unfinished_work_seconds</code> How long in seconds the outstanding workqueue items have been in flight (total). Histogram <code>name</code> Seconds Stable <code>workqueue_longest_running_processor_seconds</code> How long in seconds the longest outstanding workqueue item has been in flight Histogram <code>name</code> Seconds Stable"},{"location":"serving/observability/metrics/serving-metrics/#webhook","title":"Webhook","text":"<p>Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource.</p> Metric Name Description Type Tags Unit Status <code>request_count</code> The number of requests that are routed to webhook Counter <code>admission_allowed</code><code>kind_group</code><code>kind_kind</code><code>kind_version</code><code>request_operation</code><code>resource_group</code><code>resource_namespace</code><code>resource_resource</code><code>resource_version</code> Dimensionless Stable <code>request_latencies</code> The response time in milliseconds Histogram <code>admission_allowed</code><code>kind_group</code><code>kind_kind</code><code>kind_version</code><code>request_operation</code><code>resource_group</code><code>resource_namespace</code><code>resource_resource</code><code>resource_version</code> Milliseconds Stable"},{"location":"serving/observability/metrics/serving-metrics/#go-runtime-memstats","title":"Go Runtime - memstats","text":"<p>Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purposes, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc).</p> Metric Name Description Type Tags Unit Status <code>go_alloc</code> The number of bytes of allocated heap objects (same as heap_alloc) Gauge <code>name</code> Dimensionless Stable <code>go_total_alloc</code> The cumulative bytes allocated for heap objects Gauge <code>name</code> Dimensionless Stable <code>go_sys</code> The total bytes of memory obtained from the OS Gauge <code>name</code> Dimensionless Stable <code>go_lookups</code> The number of pointer lookups performed by the runtime Gauge <code>name</code> Dimensionless Stable <code>go_mallocs</code> The cumulative count of heap objects allocated Gauge <code>name</code> Dimensionless Stable <code>go_frees</code> The cumulative count of heap objects freed Gauge <code>name</code> Dimensionless Stable <code>go_heap_alloc</code> The number of bytes of allocated heap objects Gauge <code>name</code> Dimensionless Stable <code>go_heap_sys</code> The number of bytes of heap memory obtained from the OS Gauge <code>name</code> Dimensionless Stable <code>go_heap_idle</code> The number of bytes in idle (unused) spans Gauge <code>name</code> Dimensionless Stable <code>go_heap_in_use</code> The number of bytes in in-use spans Gauge <code>name</code> Dimensionless Stable <code>go_heap_released</code> The number of bytes of physical memory returned to the OS Gauge <code>name</code> Dimensionless Stable <code>go_heap_objects</code> The number of allocated heap objects Gauge <code>name</code> Dimensionless Stable <code>go_stack_in_use</code> The number of bytes in stack spans Gauge <code>name</code> Dimensionless Stable <code>go_stack_sys</code> The number of bytes of stack memory obtained from the OS Gauge <code>name</code> Dimensionless Stable <code>go_mspan_in_use</code> The number of bytes of allocated mspan structures Gauge <code>name</code> Dimensionless Stable <code>go_mspan_sys</code> The number of bytes of memory obtained from the OS for mspan structures Gauge <code>name</code> Dimensionless Stable <code>go_mcache_in_use</code> The number of bytes of allocated mcache structures Gauge <code>name</code> Dimensionless Stable <code>go_mcache_sys</code> The number of bytes of memory obtained from the OS for mcache structures Gauge <code>name</code> Dimensionless Stable <code>go_bucket_hash_sys</code> The number of bytes of memory in profiling bucket hash tables. Gauge <code>name</code> Dimensionless Stable <code>go_gc_sys</code> The number of bytes of memory in garbage collection metadata Gauge <code>name</code> Dimensionless Stable <code>go_other_sys</code> The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge <code>name</code> Dimensionless Stable <code>go_next_gc</code> The target heap size of the next GC cycle Gauge <code>name</code> Dimensionless Stable <code>go_last_gc</code> The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge <code>name</code> Nanoseconds Stable <code>go_total_gc_pause_ns</code> The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge <code>name</code> Nanoseconds Stable <code>go_num_gc</code> The number of completed GC cycles. Gauge <code>name</code> Dimensionless Stable <code>go_num_forced_gc</code> The number of GC cycles that were forced by the application calling the GC function. Gauge <code>name</code> Dimensionless Stable <code>go_gc_cpu_fraction</code> The fraction of this program's available CPU time used by the GC since the program started Gauge <code>name</code> Dimensionless Stable <p>Note</p> <p>The name tag is empty.</p>"},{"location":"serving/reference/serving-api/","title":"Serving API","text":"<p>This file is updated to the correct version from the serving repo (docs/serving-api.md) during the build.</p>"},{"location":"serving/revisions/","title":"About Revisions","text":"<p>Revisions are Knative Serving resources that contain point-in-time snapshots of the application code and configuration for each change made to a Knative Service.</p> <p>You cannot create Revisions or update a Revision spec directly; Revisions are always created in response to updates to a Configuration spec. However, you can force the deletion of Revisions, to handle leaked resources as well as for removal of known bad Revisions to avoid future errors when managing a Knative Service.</p> <p>Revisions are generally immutable, except where they may reference mutable core Kubernetes resources such as ConfigMaps and Secrets. Revisions can also be mutated by changes in Revision defaults. Changes to defaults that mutate Revisions are generally syntactic and not semantic.</p>"},{"location":"serving/revisions/#additional-resources","title":"Additional resources","text":"<ul> <li>Revisions concept documentation</li> </ul>"},{"location":"serving/revisions/revision-admin-config-options/","title":"Administrator configuration options","text":"<p>If you have cluster administrator permissions for your Knative installation, you can modify ConfigMaps to change the global default configuration options for Revisions of Knative Services on the cluster.</p>"},{"location":"serving/revisions/revision-admin-config-options/#garbage-collection","title":"Garbage collection","text":"<p>When Revisions of a Knative Service are inactive, they are automatically cleaned up and cluster resources are reclaimed after a set time period. This is known as garbage collection.</p> <p>You can configure garbage collection parameters a specific Revision if you are a developer. You can also configure default, cluster-wide garbage collection parameters for all the Revisions of all the Services on a cluster if you have cluster administrator permissions.</p> <p>You can set cluster-wide garbage collection configurations for your cluster by modifying the <code>config-gc</code> ConfigMap.</p> <p>The following garbage collection settings can be modified:</p> Name Description <code>retain-since-create-time</code> The time that must have elapsed since a Revision was created before a Revision is considered for garbage collection. <code>retain-since-last-active-time</code> The time that must have elapsed since a Revision was last active before a Revision is considered for garbage collection. <code>min-non-active-revisions</code> The minimum number of inactive Revisions to retain. <code>max-non-active-revisions</code> The maximum number of inactive Revisions to retain. <p>Revisions are always retained if they belong to any one of the following categories:</p> <ul> <li>The Revision is active and is being reference by a Route.</li> <li>The Revision was created within the time specified by the <code>retain-since-create-time</code> setting.</li> <li>The Revision was last referenced by a Route within the time specified by the <code>retain-since-last-active-time</code> setting.</li> <li>There are fewer existing Revisions than the number specified by the <code>min-non-active-revisions</code> setting.</li> </ul>"},{"location":"serving/revisions/revision-admin-config-options/#examples","title":"Examples","text":"<ul> <li> <p>Immediately clean up any inactive Revisions:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-gc\n  namespace: knative-serving\ndata:\n  min-non-active-revisions: \"0\"\n  max-non-active-revisions: \"0\"\n  retain-since-create-time: \"disabled\"\n  retain-since-last-active-time: \"disabled\"\n...\n</code></pre> </li> <li> <p>Retain the last ten inactive revisions:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-gc\n  namespace: knative-serving\ndata:\n  retain-since-create-time: \"disabled\"\n  retain-since-last-active-time: \"disabled\"\n  max-non-active-revisions: \"10\"\n...\n</code></pre> </li> <li> <p>Disable garbage collection on the cluster:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-gc\n  namespace: knative-serving\ndata:\n  retain-since-create-time: \"disabled\"\n  retain-since-last-active-time: \"disabled\"\n  max-non-active-revisions: \"disabled\"\n...\n</code></pre> </li> </ul>"},{"location":"serving/revisions/revision-developer-config-options/","title":"Developer configuration options","text":"<p>While Revisions cannot be created manually without modifying the Configuration of a Knative Service, you can modify the spec of an existing Revision to change its behavior.</p>"},{"location":"serving/revisions/revision-developer-config-options/#garbage-collection","title":"Garbage collection","text":"<p>When Revisions of a Knative Service are inactive, they are automatically cleaned up and cluster resources are reclaimed after a set time period. This is known as garbage collection.</p> <p>You can configure garbage collection parameters a specific Revision if you are a developer. You can also configure default, cluster-wide garbage collection parameters for all the Revisions of all the Services on a cluster if you have cluster administrator permissions.</p>"},{"location":"serving/revisions/revision-developer-config-options/#disabling-garbage-collection-for-a-revision","title":"Disabling garbage collection for a Revision","text":"<p>You can configure a Revision so that it is never garbage collected by adding the <code>serving.knative.dev/no-gc: \"true\"</code> annotation:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Revision\nmetadata:\n  annotations:\n    serving.knative.dev/no-gc: \"true\"\n...\n</code></pre>"},{"location":"serving/services/","title":"About Knative Services","text":"<p>Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured.</p> <p>Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service.</p> <p>Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic.</p>"},{"location":"serving/services/#modifying-knative-services","title":"Modifying Knative services","text":"<p>Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The <code>serving.knative.dev/service</code> label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed.</p> <p>The Service updates its <code>status</code> fields based on the corresponding <code>status</code> value for the owned Route and Configuration. The Service must include conditions of <code>RoutesReady</code> and <code>ConfigurationsReady</code> in addition to the generic <code>Ready</code> condition. Other conditions can also be present.</p>"},{"location":"serving/services/#additional-resources","title":"Additional resources","text":"<ul> <li>For more information about the Knative Service object, see the Resource Types documentation.</li> </ul>"},{"location":"serving/services/certificate-class/","title":"Configuring a custom certificate class for a Service","text":"<p>When <code>external-domain-tls</code> is enabled and Knative Services are created, a certificate class (<code>certificate-class</code>) is automatically chosen based on the value in the <code>config-network</code> ConfigMap located inside the <code>knative-serving</code> namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to <code>cert-manager.certificate.networking.knative.dev</code>. After <code>certificate-class</code> is configured, it is used for all Knative Services unless it is overridden with a <code>certificate-class</code> annotation.</p>"},{"location":"serving/services/certificate-class/#using-the-certificate-class-annotation","title":"Using the certificate class annotation","text":"<p>Generally it is recommended for Knative Services to use the default <code>certificate-class</code>. However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service.</p> <p>You can configure each Service to use a different certificate class by specifying the <code>networking.knative.dev/certificate-class</code> annotation.</p> <p>To add a certificate class annotation to a Service, run the following command: <pre><code>kubectl annotate kservice &lt;service-name&gt; networking.knative.dev/certifcate-class=&lt;certificate-provider&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;service-name&gt;</code> is the name of the Service that you are applying the annotation to.</li> <li><code>&lt;certificate-provider&gt;</code> is the type of certificate provider that is used as the certificate class for the Service.</li> </ul>"},{"location":"serving/services/configure-probing/","title":"Configuring Probing","text":""},{"location":"serving/services/configure-probing/#general-understanding-of-knative-probing","title":"General understanding of Knative Probing","text":"<p>It is important to note that Knative probing is different from Kubernetes probing.  One reason for this is that Knative tries to minimize cold-start time and thus is probing in a vastly higher interval than Kubernetes does. </p> <p>The general probing architecture looks like this:</p> <p></p> <ul> <li>Users can optionally define Readiness and/or Liveness probes in the <code>KnativeService</code> CR.</li> <li>The Liveness probes are directly executed by the Kubelet against the according container.</li> <li>Readiness probes, on the other hand, are rewritten by Knative to be executed by the Queue-Proxy container.</li> <li>Knative does probing from in places (e.g. Activator, net-* controller, and from Queue-Proxy), to make sure the whole network stack is configured and ready. Compared to vanilla Kubernetes, Knative uses faster (called aggressive probing) probing interval to shorten the cold-start times when a Pod is already up and running while Kubernetes itself has not yet reflected that readiness.</li> <li>Knative will define a default Readiness probe for the primary user container when no probe is defined by the user. It will check for a TCP socket on the traffic port of the Knative Service.</li> <li>Knative will also define a Readiness probe for the Queue-Proxy container itself. Queue-Proxy's health endpoint aggregates all results from it's rewritten Readiness probes for all user containers (primary + sidecars). For the aggregated status, Queue-Proxy will call each container's Readiness probe in parallel, wait for their response (or timeout) and report an aggregated result back to Kubernetes. </li> </ul> <p>Knative will see a Pod as healthy and ready to serve traffic once the Queue-Proxy probe returns a success response and once the Knative networking layer reconfiguration has finished.</p> <p>Note</p> <p>Keep in mind, that Knative could see your Pod as healthy and ready while Kubernetes still thinks it is not or vice versa. The <code>Deployment</code> and <code>Pod</code> statuses do not reflect the status in <code>Knative</code>. To fully check the status that Knative sees, you have to check  all the conditions on the object hierarchy of Knative (e.g. <code>Service</code>, <code>Configuration</code>, <code>Revision</code>, <code>PodAutoscaler</code>, <code>ServerlessService</code>, <code>Route</code>, <code>Ingress</code>).</p>"},{"location":"serving/services/configure-probing/#configuring-custom-probes","title":"Configuring custom probes","text":"<p>Note</p> <p>If you are using multiple containers in your Knative Service, make sure to enable multi-container probing.</p> <p>You can define Readiness and Liveness probes in your Knative Service the same way you would in Kubernetes:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: runtime\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n\n      - name: first-container\n        image: &lt;your-image&gt;\n        ports:\n          - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            port: 8080 # you can also check on a different port than the containerPort (traffic-port)\n            path: \"/health\"\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n\n      - name: second-container\n        image: &lt;your-image&gt;\n        readinessProbe:\n          httpGet:\n            port: 8089\n            path: \"/health\"\n        livenessProbe:\n          tcpSocket:\n            port: 8089\n</code></pre> <p>Supported probe types are:</p> <ul> <li>httpGet</li> <li>tcpSocket</li> <li>exec</li> <li>grpc</li> </ul> <p>Note</p> <p>Be aware that Knative also does some defaulting (checking readiness on the traffic port using an HTTP check) and additional validation to make aggressive probing work.</p> <p>Warning</p> <p>As the Queue-Proxy container does not rewrite or check defined Liveness probes, it is important to know that Kubernetes can and will restart specific containers once a Liveness probe fails. Make sure to also include the same check that you define as a Liveness probe as a Readiness probe to make sure Knative is aware of the failing container in the Pod. Otherwise, you may see, it is possible that you see connection errors during the restart of a container caused by the Liveness probe failure.</p>"},{"location":"serving/services/configure-requests-limits-services/","title":"Configure resource requests and limits","text":"<p>You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services.</p> <p>The following example shows how you can set the <code>requests</code> and <code>limits</code> fields for a service:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: docker.io/user/example-app\n          resources:\n            requests:\n              cpu: 100m\n              memory: 640M\n            limits:\n              cpu: 1\n</code></pre>"},{"location":"serving/services/configure-requests-limits-services/#configure-queue-proxy-resources","title":"Configure Queue Proxy resources","text":"<p>In order to set the Queue Proxy resource requests and limits you can either set them globally in the deployment config map or you can set them at the service level using the corresponding annotations targeting cpu, memory and ephemeral-storage resource types. The above example becomes:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n  template:\n    metadata:\n      annotations:\n        queue.sidecar.serving.knative.dev/cpu-resource-request: \"1\"\n        queue.sidecar.serving.knative.dev/cpu-resource-limit: \"2\"\n        queue.sidecar.serving.knative.dev/memory-resource-request: \"1Gi\"\n        queue.sidecar.serving.knative.dev/memory-resource-limit: \"2Gi\"\n        queue.sidecar.serving.knative.dev/ephemeral-storage-resource-request: \"400Mi\"\n        queue.sidecar.serving.knative.dev/ephemeral-storage-resource-limit: \"450Mi\"\n    spec:\n...\n</code></pre> <p>Alternatively, you could use a special annotation <code>queue.sidecar.serving.knative.dev/resource-percentage</code> that calculates the Queue Proxy resources as a percentage of the application's container. In this case there are min, max boundaries applied to the cpu and memory resource requirements:</p> Resource Requirements Min Max Cpu  Request 25m 100m Cpu Limit 40m 500m Memory  Request 50Mi 200Mi Memory Limit 200Mi 500Mi <p>Note</p> <p>If the user simultaneously sets a percentage annotation and a specific resource value via the corresponding resource annotation then the latter takes precedence.</p> <p>Warning</p> <p>The <code>queue.sidecar.serving.knative.dev/resource-percentage</code> annotation is now deprecated and will be removed in future versions.</p>"},{"location":"serving/services/configure-requests-limits-services/#additional-resources","title":"Additional resources","text":"<ul> <li>For more information requests and limits for Kubernetes resources, see Managing Resources for Containers.</li> </ul>"},{"location":"serving/services/creating-services/","title":"Creating a Service","text":"<p>You can create a Knative service by applying a YAML file or using the <code>kn service create</code> CLI command.</p>"},{"location":"serving/services/creating-services/#prerequisites","title":"Prerequisites","text":"<p>To create a Knative service, you will need:</p> <ul> <li>A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving.</li> <li>Optional: To use the <code>kn service create</code> command, you must install the <code>kn</code> CLI.</li> </ul>"},{"location":"serving/services/creating-services/#procedure","title":"Procedure","text":"<p>Tip</p> <p>The following commands create a <code>helloworld-go</code> sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service.</p> <p>Create a sample service:</p> Apply YAMLkn CLI <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          env:\n            - name: TARGET\n              value: \"Go Sample v1\"\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <pre><code>kn service create helloworld-go --image ghcr.io/knative/helloworld-go:latest\n</code></pre> <p>After the service has been created, Knative performs the following tasks:</p> <ul> <li>Creates a new immutable revision for this version of the app.</li> <li>Performs network programming to create a route, ingress, service, and load balancer for your app.</li> <li>Automatically scales your pods up and down based on traffic, including to zero active pods.</li> </ul>"},{"location":"serving/services/custom-domains/","title":"Configuring custom domains","text":"Feature Availability: beta since Knative v0.24 <ul> <li>beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways.</li> </ul> <p>Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping.</p> <p>You can create a <code>DomainMapping</code> object to map a single, non-wildcard domain to a specific Knative Service.</p> <p>For example, if you own the domain name <code>example.org</code>, and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain.</p> <p>Note</p> <p>If you create a domain mapping to map to a private Knative Service, the private Knative Service is accessible from public internet with the custom domain of the domain mapping.</p> <p>Tip</p> <p>This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain.</p>"},{"location":"serving/services/custom-domains/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation.</li> <li>You must have the domain mapping feature enabled on your cluster.</li> <li>You must have access to a Knative service that you can map a domain to.</li> <li>You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar.</li> </ul>"},{"location":"serving/services/custom-domains/#procedure","title":"Procedure","text":"<p>To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name.</p> <ol> <li> <p>Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims:</p> <ul> <li> <p>To create a ClusterDomainClaim manually:</p> <ol> <li> <p>Create a YAML file using the following template:</p> <pre><code>apiVersion: networking.internal.knative.dev/v1alpha1\nkind: ClusterDomainClaim\nmetadata:\n  name: &lt;domain-name&gt;\nspec:\n  namespace: &lt;namespace&gt;\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> </li> <li> <p>To create ClusterDomainClaims automatically: set the <code>autocreate-cluster-domain-claims</code> property to <code>true</code> in the <code>config-network</code> ConfigMap in the <code>knative-serving</code> namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. </p> </li> </ul> </li> <li> <p>Create a DomainMapping object:</p> YAMLkn <ol> <li> <p>Create a YAML file using the following template:</p> <p><pre><code>apiVersion: serving.knative.dev/v1beta1\nkind: DomainMapping\nmetadata:\n  name: &lt;domain-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  ref:\n    name: &lt;service-name&gt;\n    kind: Service\n    apiVersion: serving.knative.dev/v1\n  tls:\n    secretName: &lt;cert-secret&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;domain-name&gt;</code> is the domain name that you want to map a Service to.</li> <li><code>&lt;namespace&gt;</code> is the namespace that contains both the <code>DomainMapping</code> and <code>Service</code> objects.</li> <li><code>&lt;service-name&gt;</code> is the name of the Service that is mapped to the domain.</li> <li><code>&lt;cert-secret&gt;</code> is the name of a Secret that holds the server certificate for TLS communication. If this optional <code>tls:</code> section is provided, the protocol is switched from HTTP to HTTPS.</li> </ul> <p>Tip</p> <p>You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <code>&lt;name&gt;.&lt;namespace&gt;.&lt;clusterdomain&gt;</code>, where <code>&lt;name&gt;</code> and <code>&lt;namespace&gt;</code> are the name and namespace of a Kubernetes Service, and <code>&lt;clusterdomain&gt;</code>is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services.</p> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <p>Run the command:</p> <pre><code>kn domain create &lt;domain-name&gt; --ref &lt;target&gt; --tls &lt;tls-secret&gt; --namespace &lt;namespace&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;domain-name&gt;</code> is the domain name that you want to map a Service or Route to.</li> <li><code>&lt;target&gt;</code> is the name of the Service or Route that is mapped to the domain. You can use the prefix <code>ksvc:</code> or <code>kroute:</code> to specify whether to map the domain to a Knative Service or Route. If no prefix is given, <code>ksvc:</code> is assumed. Additionally, you can use a <code>:namespace</code> suffix to point to a Service or Route in a different namespace. Examples:<ul> <li><code>mysvc</code> maps to a Service <code>mysvc</code> in the same namespace as this mapping.</li> <li><code>kroute:myroute:othernamespace</code> maps to a Route <code>myroute</code> in namespace <code>othernamespace</code>.</li> </ul> </li> <li><code>&lt;tls-secret&gt;</code> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate.</li> <li><code>&lt;namespace&gt;</code> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace.</li> </ul> <p>Note</p> <p>In addition to creating DomainMappings, you can use the <code>kn domain</code> command to list, describe, update, and delete existing DomainMappings. For more information about the command, run <code>kn domain --help</code>.</p> </li> <li> <p>Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.</p> </li> </ol>"},{"location":"serving/services/custom-tls-certificate-domain-mapping/","title":"Using a custom TLS certificate for DomainMapping","text":"Feature Availability: beta since Knative v0.24 <ul> <li>beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways.</li> </ul> <p>By providing the reference to an existing TLS Certificate you can instruct a <code>DomainMapping</code> to use that certificate to secure the mapped service.  Please note that for Services using this feature, the automatic certificate creation using  external-domain-tls is skipped.</p>"},{"location":"serving/services/custom-tls-certificate-domain-mapping/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have followed the steps from Configuring custom domains and now have a working <code>DomainMapping</code>.</li> <li>You must have a TLS certificate from your Certificate Authority provider or a self-signed certificate.</li> </ul>"},{"location":"serving/services/custom-tls-certificate-domain-mapping/#procedure","title":"Procedure","text":"<ol> <li> <p>Assuming you have obtained the <code>cert</code> and <code>key</code> files from your Certificate Authority provider or have self-signed certificate, create a plain Kubernetes TLS Secret by running the command:</p> <p>Use kubectl to create the secret: <pre><code>kubectl create secret tls &lt;tls-secret-name&gt; --cert=path/to/cert/file --key=path/to/key/file\n</code></pre> Where <code>&lt;tls-secret-name&gt;</code> is the name of the secret object being created.</p> </li> <li> <p>Update your <code>DomainMapping</code> YAML file to use the newly created secret as follows:</p> <p><pre><code>apiVersion: serving.knative.dev/v1beta1\nkind: DomainMapping\nmetadata:\n  name: &lt;domain-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  ref:\n    name: &lt;service-name&gt;\n    kind: Service\n    apiVersion: serving.knative.dev/v1\n# tls block specifies the secret to be used\n  tls:\n    secretName: &lt;tls-secret-name&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;tls-secret-name&gt;</code> is the name of the TLS secret created in the previous step.</li> <li><code>&lt;domain-name&gt;</code> is the domain name that you want to map a Service to.</li> <li><code>&lt;namespace&gt;</code> is the namespace that contains both the <code>DomainMapping</code> and <code>Service</code> objects.</li> <li><code>&lt;service-name&gt;</code> is the name of the Service that will be mapped to the domain.</li> </ul> </li> <li> <p>Verify the <code>DomainMapping</code> status:</p> <ol> <li>Check the status by running the command: <pre><code>kubectl get domainmapping &lt;domain-name&gt;\n</code></pre> The <code>URL</code> column of the status should show the mapped domain with the scheme updated to <code>https</code>: <pre><code>NAME                      URL                               READY   REASON\n&lt;domain-name&gt;             https://&lt;domain-name&gt;             True\n</code></pre></li> <li>If the Service is exposed publicly, verify that it is available by running: <pre><code>curl https://&lt;domain-name&gt;\n</code></pre> If the certificate is self-signed skip verification by adding the <code>-k</code> flag to the curl command.</li> </ol> </li> </ol>"},{"location":"serving/services/http-protocol/","title":"Configuring HTTP","text":""},{"location":"serving/services/http-protocol/#https-redirection","title":"HTTPS redirection","text":"<p>Operators can force HTTPS redirection for all Services. See the <code>http-protocol</code> mentioned in Configure external domain encryption page for more details.</p>"},{"location":"serving/services/http-protocol/#overriding-the-default-http-behavior","title":"Overriding the default HTTP behavior","text":"<p>You can override the default behavior for each Service or global configuration.</p> <ul> <li>Global key: <code>http-protocol</code></li> <li>Per-revision annotation key: <code>networking.knative.dev/http-protocol</code></li> <li>Possible values:<ul> <li><code>enabled</code> \u2014 Services accept HTTP traffic.</li> <li><code>redirected</code> \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead.</li> </ul> </li> <li>Default: <code>enabled</code></li> </ul> <p>Example:</p> Per ServiceGlobal (ConfigMap)Global (Operator) <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example\n  namespace: default\n  annotations:\n    networking.knative.dev/http-protocol: \"redirected\"\nspec:\n  ...\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: config-network\n  namespace: knative-serving\ndata:\n  http-protocol: \"redirected\"\n</code></pre> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\nspec:\n  config:\n    network:\n      http-protocol: \"redirected\"\n</code></pre>"},{"location":"serving/services/http-protocol/#http1-full-duplex-support-per-workload","title":"HTTP/1 Full Duplex support per workload","text":"<p>Knative services can turn on the support for HTTP/1 full duplex end-to-end on the data path. This should be used in scenarios where the related Golang issue is hit eg. the application server writes back to QP's reverse proxy before the latter has consumed the whole request. For more details on why the issue appears see here.</p>"},{"location":"serving/services/http-protocol/#configure-http1-full-duplex-support","title":"Configure HTTP/1 Full Duplex support","text":"<p>In order to enable the HTTP/1 full duplex support you can set the corresponding annotation at the revision spec level as follows:</p> <p>Warning</p> <p>Test with your http clients before enabling, as older clients may not provide support for HTTP/1 full duplex.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: default\nspec:\n  template:\n    spec:\n      annotations:\n        features.knative.dev/http-full-duplex: \"Enabled\"\n...\n</code></pre>"},{"location":"serving/services/ingress-class/","title":"Configuring Services custom ingress class","text":"<p>When a Knative Service is created an ingress class (<code>ingress-class</code>) is automatically assigned to it, based on the value in the <code>config-network</code> ConfigMap located inside the <code>knative-serving</code> namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to <code>istio.ingress.networking.knative.dev</code>. Once configured the <code>ingress-class</code> is used for all Knative Services unless it is overridden with an <code>ingress-class</code> annotation.</p> <p>Warning</p> <p>Changing the ingress class in <code>config-network</code> ConfigMap will only affect newly created Services</p>"},{"location":"serving/services/ingress-class/#using-the-ingress-class-annotation","title":"Using the ingress class annotation","text":"<p>Generally it is recommended for Knative Services to use the default <code>ingress-class</code>. However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service.</p> <p>You can configure each Service to use a different ingress class by specifying the <code>networking.knative.dev/ingress-class</code> annotation.</p> <p>To add an ingress class annotation to a Service, run the following command: <pre><code>kubectl annotate kservice &lt;service-name&gt; networking.knative.dev/ingress-class=&lt;ingress-type&gt;\n</code></pre> Where:</p> <ul> <li><code>&lt;service-name&gt;</code> is the name of the Service that you are applying the annotation to.</li> <li><code>&lt;ingress-type&gt;</code> is the type of ingress that is used as the ingress class for the Service.</li> </ul> <p>Note</p> <p>This annotation overrides the <code>ingress-class</code> value specified in the <code>config-network</code> ConfigMap.</p>"},{"location":"serving/services/private-services/","title":"Configuring private Services","text":"<p>By default, Services deployed through Knative use the <code>.svc.cluster.local</code> domain, meaning they are private and thus do not have a public IP address or a public URL.</p> <p>In order to make Knative Services public (with a public IP address and public URL) by default, configure a domain name for the Service. This can be done for a single Service or for all Services on a cluster.</p>"},{"location":"serving/services/private-services/#making-individual-services-private","title":"Making individual services private","text":"<p>To make an individual Service private, the Service or Route can be labelled with <code>networking.knative.dev/visibility=cluster-local</code> so that it is not published to the external gateway.</p> <ul> <li> <p>To label a Knative Service:</p> <pre><code>kubectl label kservice ${KSVC_NAME} networking.knative.dev/visibility=cluster-local\n</code></pre> <p>By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes.</p> </li> <li> <p>To label a Route when the Route is used directly without a Knative Service:</p> <pre><code>kubectl label route ${ROUTE_NAME} networking.knative.dev/visibility=cluster-local\n</code></pre> </li> <li> <p>To label a Kubernetes Service:</p> <pre><code>kubectl label service ${SERVICE_NAME} networking.knative.dev/visibility=cluster-local\n</code></pre> </li> </ul>"},{"location":"serving/services/private-services/#example","title":"Example","text":"<p>You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service:</p> <pre><code>kubectl label kservice helloworld-go networking.knative.dev/visibility=cluster-local\n</code></pre> <p>You can then verify that the change has been made by verifying the URL for the <code>helloworld-go</code> Service:</p> <pre><code>kubectl get kservice helloworld-go\n\nNAME            URL                                              LATESTCREATED         LATESTREADY           READY   REASON\nhelloworld-go   http://helloworld-go.default.svc.cluster.local   helloworld-go-2bz5l   helloworld-go-2bz5l   True\n</code></pre> <p>The Service returns the a URL with the <code>svc.cluster.local</code> domain, indicating the Service is only available in the cluster-local network.</p>"},{"location":"serving/services/service-metrics/","title":"Service metrics","text":"<p>Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance.</p> <p>Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side.</p>"},{"location":"serving/services/service-metrics/#queue-proxy-metrics","title":"Queue proxy metrics","text":"<p>Requests endpoint.</p> Metric Name Description Type Tags Unit Status <code>revision_request_count</code> The number of requests that are routed to queue-proxy Counter <code>configuration_name</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>response_code</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>revision_request_latencies</code> The response time in millisecond Histogram <code>configuration_name</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>response_code</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Milliseconds Stable <code>revision_app_request_count</code> The number of requests that are routed to user-container Counter <code>configuration_name</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>response_code</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <code>revision_app_request_latencies</code> The response time in millisecond Histogram <code>configuration_name</code><code>namespace_name</code><code>pod_name</code><code>response_code</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Milliseconds Stable <code>revision_queue_depth</code> The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge <code>configuration_name</code><code>event-display</code><code>container_name</code><code>namespace_name</code><code>pod_name</code><code>response_code_class</code><code>revision_name</code><code>service_name</code> Dimensionless Stable <p>Note</p> <p>The <code>revision_queue_depth</code> metric will be exported only if the revision concurrency hard limit is set to a value greater than 1.</p>"},{"location":"serving/services/service-metrics/#exposing-queue-proxy-metrics","title":"Exposing Queue proxy metrics","text":"<p>Queue proxy exports metrics for the requests endpoint on port 9091. The metrics can be scraped by Prometheus when <code>metrics.request-metrics-backend-destination</code> is set to <code>prometheus</code> (default) in the configmap <code>observability</code>. The backend can be changed to <code>opencensus</code> which uses a push model and requires a destination address which can be set in the same configmap via <code>metrics.opencensus-address</code>. User can control the reporting period for both backends with <code>metrics.request-metrics-reporting-period-seconds</code>. If <code>metrics.request-metrics-reporting-period-seconds</code> is not set at all then the reporting period depends on the value of the global reporting period, <code>metrics.reporting-period-seconds</code>, that affects both control and data planes. If both properties are not available then the reporting period defaults to 5s for the Prometheus backend and 60s for the Opencensus one.</p> <p>Here is a sample configuration for the observability configmap in order to connect to the OpenTelemetry collector:</p> <pre><code>metrics.request-metrics-backend-destination: \"opencensus\"\nmetrics.opencensus-address: \"otel-collector.metrics:55678\"\nmetrics.request-metrics-reporting-period-seconds: \"1\"\n</code></pre> <p>Note</p> <p>The reporting period is to 1s so that we can push metrics as soon as possible but this could be overwhelming for the targeted metrics backend. Setting a value of zero or a negative value defaults to 10s (does not mean no delay) which is the default reporting period defined by the Opencensus metrics client library. The latter is used by Knative Serving for exporting metrics.</p>"},{"location":"serving/services/storage/","title":"Volume Support for Knative services","text":"<p>You can provide data storage for Knative Services by configuring different volumes types. Serving supports mounting the volume types: <code>emptyDir</code>, <code>secret</code>, <code>configMap</code> and <code>projected</code>. PersistentVolumes are supported but require a feature flag to be enabled.</p> <p>Warning</p> <p>Mounting large volumes may add considerable overhead to the application's start up time.</p> <p>Bellow there is an example of using a persistent volume claim with a Knative Service.</p>"},{"location":"serving/services/storage/#prerequisites","title":"Prerequisites","text":"<p>Before you can configure PVCs for a Service, this feature must be enabled in the <code>config-features</code> ConfigMap as follows:</p> <pre><code>kubectl patch --namespace knative-serving configmap/config-features \\\n --type merge \\\n --patch '{\"data\":{\"kubernetes.podspec-persistent-volume-claim\": \"enabled\", \"kubernetes.podspec-persistent-volume-write\": \"enabled\"}}'\n</code></pre> <ul> <li>The <code>kubernetes.podspec-persistent-volume-claim</code> extension controls whether persistent volumes (PVs) can be used with Knative Serving.</li> <li>The <code>kubernetes.podspec-persistent-volume-write</code> extension controls whether PVs are available to Knative Serving with the write access.</li> </ul> <p>Note</p> <p>If you have installed Serving via the Knative operator then you need to set the above feature flags only at the corresponding Serving CR.</p>"},{"location":"serving/services/storage/#procedure","title":"Procedure","text":"<ul> <li>Modify the PVC configuration for your Service:</li> </ul> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\n...\nspec:\n  template:\n    spec:\n      containers:\n        ...\n        volumeMounts:\n            - mountPath: /data\n              name: mydata\n              readOnly: false\n      volumes:\n        - name: mydata\n          persistentVolumeClaim:\n            claimName: knative-pv-claim\n            readOnly: false\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: knative-pv-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"serving/services/using-queue-extensions/","title":"Using extensions enabled by QPOptions","text":"<p>QPOptions is a Queue Proxy feature that enables extending Queue Proxy with additional Go packages. For example, the security-guard repository extends Queue Proxy by adding runtime security features to protect user services.</p> <p>Once your cluster is setup with extensions enabled by QPOptions, a Service can decide which extensions it wish to use and how to configure such extensions. Activating and configuring extensions is described here.</p>"},{"location":"serving/services/using-queue-extensions/#overview","title":"Overview","text":"<p>A Service can activate and configure extensions by adding <code>qpoption.knative.dev/*</code> annotations under the: <code>spec.template.metadata</code> of the Service Custom Resource Definition (CRD).</p> <p>Setting a value of: <code>qpoption.knative.dev/&lt;ExtensionName&gt;-activate: \"enable\"</code> activates the extension.</p> <p>Setting a value of: <code>qpoption.knative.dev/&lt;extension-name&gt;-config-&lt;key&gt;: \"&lt;value&gt;\"</code> adds a configuration of <code>key: value</code> to the extension.</p> <p>In addition, the Service must ensure that the Pod Info volume is mounted by adding the <code>features.knative.dev/queueproxy-podinfo: enabled</code> annotation under the: <code>spec.template.metadata</code> of the Service CRD.</p> <p>You can create a Knative Service by applying a YAML file or by using the <code>kn service create</code> CLI command.</p>"},{"location":"serving/services/using-queue-extensions/#prerequisites","title":"Prerequisites","text":"<p>Before you can use extensions enabled by QPOptions, you must:</p> <ul> <li>Prepare your cluster:</li> <li>Make sure you are using a Queue Proxy image that was built with the extensions that you wish to use - See Extending Queue Proxy image with QPOptions.</li> <li>Make sure that the cluster config-features is set with <code>queueproxy.mount-podinfo: allowed</code>. See Enabling Queue Proxy Pod Info for more details.</li> <li>Meet the prerequisites in Creating a Service</li> </ul>"},{"location":"serving/services/using-queue-extensions/#procedure","title":"Procedure","text":"<p>Tip</p> <p>The following commands create a <code>helloworld-go</code> sample Service while activating and configuring the <code>test-gate</code> extension for this Service. You can modify these commands, including the extension(s) to be activated and the extension configuration.</p> <p>Create a sample Service:</p> Apply YAMLkn CLI <ol> <li> <p>Create a YAML file using the following example:</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  template:\n    metadata:\n        annotations:\n          features.knative.dev/queueproxy-podinfo: enabled\n          qpoption.knative.dev/testgate-activate: enable\n          qpoption.knative.dev/testgate-config-response: CU\n          qpoption.knative.dev/testgate-config-sender: Joe\n    spec:\n      containers:\n        - image: ghcr.io/knative/helloworld-go:latest\n          env:\n            - name: TARGET\n              value: \"World\"\n</code></pre> </li> <li> <p>Apply the YAML file by running the command:</p> <p><pre><code>kubectl apply -f &lt;filename&gt;.yaml\n</code></pre> Where <code>&lt;filename&gt;</code> is the name of the file you created in the previous step.</p> </li> </ol> <pre><code>kn service create helloworld-go \\\n    --image ghcr.io/knative/helloworld-go:latest \\\n    --env TARGET=World \\\n    --annotation features.knative.dev/queueproxy-podinfo=enabled \\\n    --annotation qpoption.knative.dev/testgate-activate=enable \\\n    --annotation qpoption.knative.dev/testgate-config-response=Goodbye \\\n    --annotation qpoption.knative.dev/testgate-config-sender=Joe\n</code></pre> <p>After the Service has been created, Knative propagates the annotations to the podSpec of the Service deployment. When a Service pod is created, the Queue Proxy sidecar will mount a volume that contains the pod annotations and activate the <code>testgate</code> extension. This occurs if the <code>testgate</code> extension is available in the Queue Proxy image. The <code>testgate</code> extension will then be configured with the configuration: <code>{ sender: \"Joe\", response: \"CU\"}</code>.</p>"},{"location":"serving/troubleshooting/debugging-application-issues/","title":"Debugging application issues","text":"<p>If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application.</p>"},{"location":"serving/troubleshooting/debugging-application-issues/#check-terminal-output","title":"Check terminal output","text":"<p>Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed.</p> <p>This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100:</p> <pre><code>Error from server (InternalError): error when applying patch:\n{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}}\nto:\n&amp;{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false}\nfor: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100.\nERROR: Non-zero return code '1' from command: Process exited with status 1\n</code></pre>"},{"location":"serving/troubleshooting/debugging-application-issues/#check-route-status","title":"Check Route status","text":"<p>Run the following command to get the <code>status</code> of the <code>Route</code> object with which you deployed your application:</p> <pre><code>kubectl get route &lt;route-name&gt; --output yaml\n</code></pre> <p>The <code>conditions</code> in <code>status</code> provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting.</p>"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingressistio-routing","title":"Check Ingress/Istio routing","text":"<p>To list all Ingress resources and their corresponding labels, run the following command:</p> <pre><code>kubectl get ingresses.networking.internal.knative.dev -o=custom-columns='NAME:.metadata.name,LABELS:.metadata.labels'\nNAME            LABELS\nhelloworld-go   map[serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go]\n</code></pre> <p>The labels <code>serving.knative.dev/route</code> and <code>serving.knative.dev/routeNamespace</code> indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status.</p> <p>Otherwise, run the following command to look at the ClusterIngress created for your Route</p> <pre><code>kubectl get ingresses.networking.internal.knative.dev &lt;INGRESS_NAME&gt; --output yaml\n</code></pre> <p>particularly, look at the <code>status:</code> section. If the Ingress is working correctly, we should see the condition with <code>type=Ready</code> to have <code>status=True</code>. Otherwise, there will be error messages.</p> <p>Now, if Ingress shows status <code>Ready</code>, there must be a corresponding VirtualService. Run the following command:</p> <pre><code>kubectl get virtualservice -l networking.internal.knative.dev/ingress=&lt;INGRESS_NAME&gt; -n &lt;INGRESS_NAMESPACE&gt; --output yaml\n</code></pre> <p>the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate.</p> <p>If you are familiar with Istio and <code>istioctl</code>, you may try using <code>istioctl</code> to look deeper using Istio guide.</p>"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingress-status","title":"Check Ingress status","text":"<p>Knative uses a LoadBalancer service called <code>istio-ingressgateway</code> Service.</p> <p>To check the IP address of your Ingress, use</p> <pre><code>kubectl get svc -n istio-system istio-ingressgateway\n</code></pre> <p>If there is no external IP address, use</p> <pre><code>kubectl describe svc istio-ingressgateway -n istio-system\n</code></pre> <p>to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue.</p>"},{"location":"serving/troubleshooting/debugging-application-issues/#check-revision-status","title":"Check Revision status","text":"<p>If you configure your <code>Route</code> with <code>Configuration</code>, run the following command to get the name of the <code>Revision</code> created for you deployment (look up the configuration name in the <code>Route</code> .yaml file):</p> <pre><code>kubectl get configuration &lt;configuration-name&gt; --output jsonpath=\"{.status.latestCreatedRevisionName}\"\n</code></pre> <p>If you configure your <code>Route</code> with <code>Revision</code> directly, look up the revision name in the <code>Route</code> yaml file.</p> <p>Then run the following command:</p> <pre><code>kubectl get revision &lt;revision-name&gt; --output yaml\n</code></pre> <p>A ready <code>Revision</code> should have the following condition in <code>status</code>:</p> <pre><code>conditions:\n  - reason: ServiceReady\n    status: \"True\"\n    type: Ready\n</code></pre> <p>If you see this condition, check the following to continue debugging:</p> <ul> <li>Check Pod status</li> <li>Check Istio routing</li> </ul> <p>Tip</p> <p>If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting. An alternative is to check Pod status.</p>"},{"location":"serving/troubleshooting/debugging-application-issues/#check-pod-status","title":"Check Pod status","text":"<p>To get the <code>Pod</code>s for all your deployments:</p> <pre><code>kubectl get pods\n</code></pre> <p>This command should list all <code>Pod</code>s with brief status. For example:</p> <pre><code>NAME                                                      READY     STATUS             RESTARTS   AGE\nconfiguration-example-00001-deployment-659747ff99-9bvr4   2/2       Running            0          3h\nconfiguration-example-00002-deployment-5f475b7849-gxcht   1/2       CrashLoopBackOff   2          36s\n</code></pre> <p>Choose one and use the following command to see detailed information for its <code>status</code>. Some useful fields are <code>conditions</code> and <code>containerStatuses</code>:</p> <pre><code>kubectl get pod &lt;pod-name&gt; --output yaml\n</code></pre>"}]}